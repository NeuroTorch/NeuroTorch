{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning Tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This tutorial was tested with the version `0.0.1-beta0` of NeuroTorch.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Warning**\n",
    "\n",
    "The reinforcement pipeline is currently under development and there are several issues to fix at this time. If you change the environment to one with continuous action, you may notice an error where the actions result in a bunch of NaN. If you think you know what cause it, please communicate with us. In addition, with discrete actions, the PPO algorithm doesn't seem to converge with good cumulative rewards every time and the test cumulative rewards don't seem to match the train one. Again, if you think you know what cause this instability, please communicate with us. We are sorry for this inconvenient, and thank you for your patience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train an agent in a [gym](https://www.gymlibrary.dev/content/basic_usage/) environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now install the dependencies by running the following commands:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r rl_requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "import neurotorch as nt\n",
    "from neurotorch.rl.agent import Agent\n",
    "from neurotorch.rl.rl_academy import RLAcademy\n",
    "from neurotorch.rl.utils import TrajectoryRenderer, space_to_continuous_shape\n",
    "from neurotorch.transforms.spikes_encoders import SpikesEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs_file_setup(\"rl_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env_id = \"LunarLander-v2\"\n",
    "env = gym.vector.make(env_id, num_envs=10, render_mode=\"rgb_array\")\n",
    "use_spiking_policy = True  # Type of the policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_spiking_policy:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_snn-policy\"\n",
    "else:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_default-policy\"\n",
    "checkpoint_manager = nt.CheckpointManager(\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    save_freq=10,\n",
    "    metric=RLAcademy.CUM_REWARDS_METRIC_KEY,\n",
    "    minimise_metric=False,\n",
    "    save_best_only=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we are initializing the learning algorithm that will be used to train the agent. For now, this learning algorithm it's the popular [Proximal Policy Optimisation](https://arxiv.org/pdf/1707.06347.pdf) from OpenAI."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_la = nt.rl.PPO(\n",
    "    tau=0.0,\n",
    "    critic_weight=0.5,\n",
    "    entropy_weight=0.01,\n",
    "    gae_lambda=1.0,\n",
    "    default_critic_lr=1e-3,\n",
    "    default_policy_lr=5e-4,\n",
    "    critic_criterion=torch.nn.SmoothL1Loss(),\n",
    "    clip_ratio=0.2,\n",
    "    critic_clip=0.2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is now the time to define our policy. For short, the policy is the model that will be used to take the actions in the environment. The critic is the model used to estimate the rewards-to-go of the states that the agent will encounter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_spiking_policy:\n",
    "    policy = nt.SequentialRNN(\n",
    "        input_transform=[\n",
    "            SpikesEncoder(\n",
    "                n_steps=8,\n",
    "                n_units=space_to_continuous_shape(env.single_observation_space)[0],\n",
    "                spikes_layer_type=nt.SpyLIFLayer,\n",
    "            )\n",
    "        ],\n",
    "        layers=[\n",
    "            nt.SpyLIFLayer(\n",
    "                space_to_continuous_shape(env.single_observation_space)[0], 128, use_recurrent_connection=False\n",
    "            ),\n",
    "            nt.SpyLILayer(128, space_to_continuous_shape(env.single_action_space)[0]),\n",
    "        ],\n",
    "        output_transform=[nt.transforms.ReduceMax(dim=1)],\n",
    "    ).build()\n",
    "else:\n",
    "    policy = nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(space_to_continuous_shape(env.single_observation_space)[0], 128),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(128, space_to_continuous_shape(env.single_action_space)[0]),\n",
    "        ]\n",
    "    ).build()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we're defining the agent using the policy and the critic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env=env,\n",
    "    behavior_name=env_id,\n",
    "    policy=policy,\n",
    "    critic=nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(space_to_continuous_shape(env.single_observation_space)[0], 128),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        ]\n",
    "    ).build(),\n",
    "    checkpoint_folder=checkpoint_manager.checkpoint_folder,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the RLAcademy. This is a special type of Trainer used to train the agent in a reinforcement learning pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "academy = RLAcademy(\n",
    "    agent=agent,\n",
    "    callbacks=[checkpoint_manager, ppo_la],\n",
    "    normalize_rewards=False,\n",
    "    init_epsilon=0.00,\n",
    "    use_priority_buffer=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training time!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we will start the actual training with the following parameter:\n",
    "\n",
    "    - `n_iterations`: The number of time the trainer will generate trajectories and will do an optimisation pass.\n",
    "    - `n_epochs`: The number of time the trainer will pass through the buffer of episodes for an optimisation pass.\n",
    "    - `n_batches`: The number of batch to do at each epoch.\n",
    "    - `n_new_trajectories`: The number of new trajectories to generate at each iteration.\n",
    "    - `batch_size`: The number of episodes for a single batch.\n",
    "    - `buffer_size`: The size of the buffer.\n",
    "    - `clear_buffer`: Wheater to clear or the the buffer before each iteration.\n",
    "    - `last_k_rewards`: The number of k previous rewards to show in the metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = academy.train(\n",
    "    env,\n",
    "    n_iterations=500,\n",
    "    n_epochs=30,\n",
    "    n_batches=-1,\n",
    "    n_new_trajectories=10,\n",
    "    batch_size=4096,\n",
    "    buffer_size=np.inf,\n",
    "    clear_buffer=True,\n",
    "    randomize_buffer=True,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.LAST_ITR,\n",
    "    force_overwrite=False,\n",
    "    verbose=True,\n",
    "    render=False,\n",
    "    last_k_rewards=10,\n",
    ")\n",
    "if not env.closed:\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history.plot(show=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Phase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we will generate new trajectories of the agent just to see how it will perform."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.load_checkpoint(\n",
    "    checkpoints_meta_path=checkpoint_manager.checkpoints_meta_path,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.BEST_ITR\n",
    ")\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "agent.eval()\n",
    "gen_trajectories_out = academy.generate_trajectories(\n",
    "    n_trajectories=10, epsilon=0.0, verbose=True, env=env, render=True, re_trajectories=True,\n",
    ")\n",
    "best_trajectory_idx = np.argmax([t.cumulative_reward for t in gen_trajectories_out.trajectories])\n",
    "trajectory_renderer = TrajectoryRenderer(trajectory=gen_trajectories_out.trajectories[best_trajectory_idx], env=env)\n",
    "trajectory_renderer.render()\n",
    "trajectory_renderer.to_mp4(f\"figures/trajectory_{best_trajectory_idx}.mp4\")\n",
    "cumulative_rewards = gen_trajectories_out.cumulative_rewards\n",
    "print(f\"Buffer: {gen_trajectories_out.buffer}\")\n",
    "print(f\"Cumulative rewards: {np.nanmean(cumulative_rewards):.3f} +/- {np.nanstd(cumulative_rewards):.3f}\")\n",
    "n_terminated = sum([int(e.terminal) for e in gen_trajectories_out.buffer])\n",
    "print(f\"{n_terminated = }\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
