{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train a neural network to recognize handwritten digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now install the dependencies by running the following commands:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r mnist_requirements.txt\n",
    "!pip install norse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "import norse\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "from tutorials.mnist.dataset import get_dataloaders, DatasetId\n",
    "import neurotorch as nt\n",
    "from neurotorch import Dimension, DimensionProperty\n",
    "from neurotorch.callbacks import CheckpointManager, LoadCheckpointMode\n",
    "from neurotorch.metrics import ClassificationMetrics\n",
    "from neurotorch.trainers import ClassificationTrainer\n",
    "from neurotorch.transforms.spikes_encoders import SpyLIFEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs_file_setup(\"mnist_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's the time to defined our training and networks parameters. In the following,\n",
    "\n",
    "- the number of iteration is the number of time the trainer will pass through the entire training dataset;\n",
    "- the batch size is the number of samples that will be loaded at the same time for each forward and backward pass;\n",
    "- the learning rate is the learning rate used by the optimizer;\n",
    "- the number of steps is the number of euler integration steps performed by the model during each forward and backward pass;\n",
    "- the number of hidden neurons is the number of neurons that will be used in the hidden layer of the model;\n",
    "- dt is the time step of the euler integration;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_id = DatasetId.MNIST\n",
    "\n",
    "# Training parameters\n",
    "n_iterations = 30\n",
    "batch_size = 1024\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Network parameters\n",
    "n_steps = 8\n",
    "n_hidden_neurons = 128\n",
    "dt = 1e-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_folder = f\"./checkpoints/{dataset_id.name}\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_folder, save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NeuroTorch is a library that allows to build bio-like networks, that is, they are in general recurrent neural networks. In this task we are classifying images, so we must transform the input images into times series of values. In our case, we are using spiking neural network to perform the classification, so we will transform the input images into spikes trains. The used transform in this tutorial is the ConstantCurrentLIFEncoder from the [norse](https://github.com/norse/norse) library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# input_transform = Compose([torch.nn.Flatten(start_dim=1), norse.torch.ConstantCurrentLIFEncoder(seq_length=n_steps, dt=dt), Lambda(lambda x: x.permute(1, 0, 2))])\n",
    "input_transform = Compose([torch.nn.Flatten(start_dim=2), SpyLIFEncoder(n_steps=n_steps, n_units=28*28)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is time to load the dataloaders. The dataloader is an object used to the data from a given dataset. In our case we will load the MNIST or the FASHION-MNIST dataset. To change the dataset, just change the value of the variable `dataset_id` for `DatasetId.MNIST` or `DatasetId.FASHION_MNIST` in the cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Here is an exemple of the MNIST dataset:\n",
    "<p align=\"center\"> <img width=\"1200\" height=\"500\" src=\"../../images/mnist/MnistExamples.png\"> </p>\n",
    "\n",
    "And an exemple of the Fashion-MNIST dataset:\n",
    "<p align=\"center\"> <img width=\"1200\" height=\"500\" src=\"../../images/mnist/fashion-mnist-sprite.png\"> </p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(\n",
    "\tdataset_id=dataset_id,\n",
    "\tbatch_size=batch_size,\n",
    "\ttrain_val_split_ratio=0.95,\n",
    "\tnb_workers=max(0, min(2, psutil.cpu_count(logical=False))),\n",
    "\tpin_memory=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can finally define our model. The sequential model is a neural network that is composed of a list of layers. The list of layers is ordered and the first layer is the input layer. The last layer is the output layer. The sequential model can be found in the subpackage `neurotorch.modules`. Note that we can specify the input transform that will be applied to the input data before the forward pass through the list of layers. If the transformation is computationally expensive, it is recommended to put it in the dataloaders instead of the model for the training. Usually the input transform in the model is used when the model is used for inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = nt.SequentialRNN(\n",
    "\tinput_transform=input_transform,\n",
    "\tlayers=[\n",
    "\t\tnt.LIFLayer(\n",
    "\t\t\tinput_size=[Dimension(None, DimensionProperty.TIME), Dimension(28*28, DimensionProperty.NONE)],\n",
    "\t\t\tuse_recurrent_connection=False,\n",
    "\t\t\toutput_size=n_hidden_neurons,\n",
    "\t\t\tdt=dt,\n",
    "\t\t),\n",
    "\t\tnt.SpyLILayer(dt=dt, output_size=10),\n",
    "\t],\n",
    "\tname=f\"{dataset_id.name}_network\",\n",
    "\tcheckpoint_folder=checkpoint_folder,\n",
    "\thh_memory_size=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the instantiation of the model, we have to build it, so it can infer the missing sizes of the layers in the model and create the tensors that will be used during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.build()\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we create the trainer that will be used to trained our network. They are several types of trainers that can be found in the subpackage `neurotorch.trainers` like the `ClassificationTrainer`, the `RegressionTrainer` or the `PPOTrainer`. In this tutorial, we will use the `ClassificationTrainer` because we are trying to classify some images. Note that the callbacks are some object that will be called during the training. They are found in the subpackage `neurotorch.callbacks` like the `CheckpointManager`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = ClassificationTrainer(\n",
    "\tmodel=network,\n",
    "\toptimizer=torch.optim.Adam(network.parameters(), lr=learning_rate),\n",
    "\tcallbacks=checkpoint_manager,\n",
    "\tverbose=True,\n",
    ")\n",
    "print(trainer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training will start! Let's see how our simple network will perform on this task.\n",
    "\n",
    "Note: the argument `force_overwrite` is used to overwrite the existing checkpoint if it exists and the argument `load_checkpoint_mode` if the model will be loaded from the last checkpoint (`LoadCheckpointMode.LAST_ITR`) or from the best checkpoint (`LoadCheckpointMode.BEST_ITR`). The loading mode is really useful when you want to stop the training and restart it later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_history = trainer.train(\n",
    "\tdataloaders[\"train\"],\n",
    "\tdataloaders[\"val\"],\n",
    "\tn_iterations=n_iterations,\n",
    "\tload_checkpoint_mode=LoadCheckpointMode.LAST_ITR,\n",
    "\t# force_overwrite=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training is finished! Let's see how our network learned over the iterations.\n",
    "\n",
    "Note: the next graphs are really useful to see if the model overfit or underfit the data over the iterations. It's why you can use the callback `TrainingHistoryVisualizationCallback` to see those graphs in real time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_history.plot(show=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can load the model at the best or the last iteration for the testing phase."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.load_checkpoint(checkpoint_manager.checkpoints_meta_path, LoadCheckpointMode.BEST_ITR, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how our network perform on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies={\n",
    "\tk: ClassificationMetrics.accuracy(network, dataloaders[k], verbose=True, desc=f\"{k}_accuracy\")\n",
    "\tfor k in dataloaders\n",
    "}\n",
    "pprint.pprint(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the results on other popular classification metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precisions={\n",
    "\tk: ClassificationMetrics.precision(network, dataloaders[k], verbose=True, desc=f\"{k}_precision\")\n",
    "\tfor k in dataloaders\n",
    "},\n",
    "recalls={\n",
    "\tk: ClassificationMetrics.recall(network, dataloaders[k], verbose=True, desc=f\"{k}_recall\")\n",
    "\tfor k in dataloaders\n",
    "},\n",
    "f1s={\n",
    "\tk: ClassificationMetrics.f1(network, dataloaders[k], verbose=True, desc=f\"{k}_f1\")\n",
    "\tfor k in dataloaders\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = OrderedDict(dict(\n",
    "\tnetwork=network,\n",
    "\taccuracies=accuracies,\n",
    "\tprecisions=precisions,\n",
    "\trecalls=recalls,\n",
    "\tf1s=f1s,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pprint.pprint(results, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the end of the tutorial!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "NeuroTorch is a library that allows you to easily build neural networks and train them. Moreover, it allows you to do image classification using dynamics from neurosciences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
