{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial was tested with the version `0.0.1-beta4` of NeuroTorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"nt-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://NeuroTorch.github.io/NeuroTorch/\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/neurotorch_logo_32px.png?raw=true\" width=32px height=32px  />Documentation</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/colab_logo_32px.png?raw=true\" width=32px height=32px  />Run in Google Colab</a>\n",
    "</td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32px height=32px />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/download_logo_32px.png?raw=true\" width=32px height=32px />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train an agent in a [gym](https://www.gymlibrary.dev/content/basic_usage/) environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now install the dependencies by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Install dependencies {display-mode: \"form\"}\n",
    "\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "\n",
    "!pip install git+https://github.com/NeuroTorch/NeuroTorch.git@119-rl-ppo\n",
    "!pip install pythonbasictools\n",
    "!pip install gym[box2d]==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "import neurotorch as nt\n",
    "from neurotorch.callbacks.early_stopping import EarlyStoppingThreshold\n",
    "from neurotorch.rl.agent import Agent\n",
    "from neurotorch.rl.rl_academy import RLAcademy\n",
    "from neurotorch.rl.utils import TrajectoryRenderer, space_to_continuous_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_file_setup(\"rl_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env parameters\n",
    "env_id = \"LunarLander-v2\"\n",
    "continuous_action = True\n",
    "\n",
    "# Network parameters\n",
    "use_spiking_policy = True  # Type of the policy\n",
    "n_hidden_units = 128\n",
    "n_critic_hidden_units = 128\n",
    "n_encoder_steps = 8\n",
    "\n",
    "# Trainer parameters\n",
    "n_iterations = 600\n",
    "n_epochs = 30\n",
    "n_new_trajectories = 1\n",
    "last_k_rewards = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id, render_mode=\"rgb_array\", continuous=continuous_action)\n",
    "continuous_obs_shape = space_to_continuous_shape(getattr(env, \"single_observation_space\", env.observation_space))\n",
    "continuous_action_shape = space_to_continuous_shape(getattr(env, \"single_action_space\", env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_spiking_policy:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_snn-policy\"\n",
    "else:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_classical-policy\"\n",
    "checkpoint_manager = nt.CheckpointManager(\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    save_freq=int(0.1*n_iterations),\n",
    "    metric=RLAcademy.CUM_REWARDS_METRIC_KEY,\n",
    "    minimise_metric=False,\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are initializing the learning algorithm that will be used to train the agent. For now, this learning algorithm it's the popular [Proximal Policy Optimisation](https://arxiv.org/pdf/1707.06347.pdf) from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_la = nt.rl.PPO(\n",
    "    critic_criterion=torch.nn.SmoothL1Loss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now the time to define our policy. For short, the policy is the model that will be used to take the actions in the environment. The critic is the model used to estimate the rewards-to-go of the states that the agent will encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_spiking_policy:\n",
    "    policy = nt.SequentialRNN(\n",
    "        input_transform=[\n",
    "            nt.transforms.ConstantValuesTransform(n_steps=n_encoder_steps)\n",
    "        ],\n",
    "        layers=[\n",
    "            nt.SpyLIFLayerLPF(\n",
    "                continuous_obs_shape[0], n_hidden_units, use_recurrent_connection=False\n",
    "            ),\n",
    "            nt.SpyLILayer(n_hidden_units, continuous_action_shape[0]),\n",
    "        ],\n",
    "        output_transform=[\n",
    "            (\n",
    "                nt.transforms.ReduceFuncTanh(nt.transforms.ReduceMean(dim=1))\n",
    "                if continuous_action else\n",
    "                nt.transforms.ReduceMax(dim=1)\n",
    "            )\n",
    "        ],\n",
    "    ).build()\n",
    "else:\n",
    "    policy = nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(continuous_obs_shape[0], n_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_hidden_units, n_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_hidden_units, continuous_action_shape[0]),\n",
    "            (torch.nn.Tanh() if continuous_action else torch.nn.Identity())\n",
    "        ]\n",
    "    ).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're defining the agent using the policy and the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env=env,\n",
    "    behavior_name=env_id,\n",
    "    policy=policy,\n",
    "    critic=nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(continuous_obs_shape[0], n_critic_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_critic_hidden_units, n_critic_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_critic_hidden_units, 1),\n",
    "        ]\n",
    "    ).build(),\n",
    "    checkpoint_folder=checkpoint_manager.checkpoint_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an early stopping callback that will stop the training if the mean of the last k cumulative rewards is better or equal than 230 (at 200 the environnement is considered as solved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingThreshold(\n",
    "    metric=f\"mean_last_{last_k_rewards}_rewards\",\n",
    "    threshold=230.0,\n",
    "    minimize_metric=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the RLAcademy. This is a special type of Trainer used to train the agent in a reinforcement learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "academy = RLAcademy(\n",
    "    agent=agent,\n",
    "    callbacks=[checkpoint_manager, ppo_la, early_stopping],\n",
    ")\n",
    "print(f\"Academy:\\n{academy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will start the actual training with the following parameter:\n",
    "\n",
    "    - `n_iterations`: The number of time the trainer will generate trajectories and will do an optimisation pass.\n",
    "    - `n_epochs`: The number of time the trainer will pass through the buffer of episodes for an optimisation pass.\n",
    "    - `n_batches`: The number of batch to do at each epoch.\n",
    "    - `n_new_trajectories`: The number of new trajectories to generate at each iteration.\n",
    "    - `batch_size`: The number of episodes for a single batch.\n",
    "    - `buffer_size`: The size of the buffer.\n",
    "    - `clear_buffer`: Wheater to clear or the the buffer before each iteration.\n",
    "    - `last_k_rewards`: The number of k previous rewards to show in the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = academy.train(\n",
    "    env,\n",
    "    n_iterations=n_iterations,\n",
    "    n_epochs=n_epochs,\n",
    "    n_batches=-1,\n",
    "    n_new_trajectories=n_new_trajectories,\n",
    "    batch_size=4096,\n",
    "    buffer_size=np.inf,\n",
    "    clear_buffer=True,\n",
    "    randomize_buffer=True,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.LAST_ITR,\n",
    "    force_overwrite=False,\n",
    "    verbose=True,\n",
    "    render=False,\n",
    "    last_k_rewards=last_k_rewards,\n",
    ")\n",
    "if not getattr(env, \"closed\", False):\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will generate new trajectories of the agent just to see how it will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_checkpoint(\n",
    "    checkpoints_meta_path=checkpoint_manager.checkpoints_meta_path,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.BEST_ITR\n",
    ")\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\", continuous=continuous_action)\n",
    "agent.eval()\n",
    "gen_trajectories_out = academy.generate_trajectories(\n",
    "    n_trajectories=10, epsilon=0.0, verbose=True, env=env, render=True, re_trajectories=True,\n",
    ")\n",
    "best_trajectory_idx = np.argmax([t.cumulative_reward for t in gen_trajectories_out.trajectories])\n",
    "trajectory_renderer = TrajectoryRenderer(trajectory=gen_trajectories_out.trajectories[best_trajectory_idx], env=env)\n",
    "\n",
    "cumulative_rewards = gen_trajectories_out.cumulative_rewards\n",
    "print(f\"Buffer: {gen_trajectories_out.buffer}\")\n",
    "print(f\"Cumulative rewards: {np.nanmean(cumulative_rewards):.3f} +/- {np.nanstd(cumulative_rewards):.3f}\")\n",
    "best_cum_reward_fmt = f\"{cumulative_rewards[best_trajectory_idx]:.3f}\"\n",
    "print(f\"Best trajectory: {best_trajectory_idx}, cumulative reward: {best_cum_reward_fmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the best trajectory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, anim = trajectory_renderer.render(\n",
    "    filename=(\n",
    "        f\"{agent.checkpoint_folder}/figures/trajectory_{best_trajectory_idx}-\"\n",
    "        f\"CR{best_cum_reward_fmt.replace('.', '_')}\"\n",
    "    ),\n",
    "    file_extension=\"gif\",\n",
    "    show=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
