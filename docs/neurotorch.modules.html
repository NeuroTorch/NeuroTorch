

<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>neurotorch.modules package &mdash; NeuroTorch  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/css/theme.min.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="neurotorch.modules.layers package" href="neurotorch.modules.layers.html" />
    <link rel="prev" title="neurotorch.metrics package" href="neurotorch.metrics.html" /> 

</head>

<body>
    <header>
        <div class="container">
            <a class="site-nav-toggle hidden-lg-up"><i class="icon-menu"></i></a>
            <a class="site-title" href="index.html">
                NeuroTorch
            </a>
        </div>
    </header>


<div class="breadcrumbs-outer hidden-xs-down">
    <div class="container">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="breadcrumbs">
    
      <li><a href="index.html">Docs</a></li>
        
          <li><a href="neurotorch.html">neurotorch package</a></li>
        
      <li>neurotorch.modules package</li>
    
    
      <li class="breadcrumbs-aside">
        
            
            <a href="sources/neurotorch.modules.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
</div>
    </div>
</div>
    <div class="main-outer">
        <div class="container">
            <div class="row">
                <div class="col-12 col-lg-3 site-nav">
                    
<div role="search">
    <form class="search" action="search.html" method="get">
        <div class="icon-input">
            <input type="text" name="q" placeholder="Search" />
            <span class="icon-search"></span>
        </div>
        <input type="submit" value="Go" class="d-hidden" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div>
                    <div class="site-nav-tree">
                        
                            
                            
                                <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">1. Description</a><ul>
<li class="toctree-l2"><a class="reference internal" href="readme.html#current-version">Current Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="readme.html#next-versions">Next Versions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#installation">2. Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="readme.html#last-unstable-version">2.1 Last unstable version</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#tutorials-applications">3. Tutorials / Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#quick-usage-preview">4. Quick usage preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#why-neurotorch">5. Why NeuroTorch?</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#similar-work">6. Similar work</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#about">7. About</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#important-links">8. Important Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#found-a-bug-or-have-a-feature-request">9. Found a bug or have a feature request?</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#thanks">10. Thanks</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#license">11. License</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#citation">12. Citation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modules:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="neurotorch.html">neurotorch package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="neurotorch.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#module-neurotorch.dimension">neurotorch.dimension module</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#module-neurotorch">Module contents</a></li>
</ul>
</li>
</ul>

                            
                        
                    </div>
                </div>
                <div class="col-12 col-lg-9">
                    <div class="document">
                        
                            
  <section id="neurotorch-modules-package">
<h1>neurotorch.modules package<a class="headerlink" href="#neurotorch-modules-package" title="Permalink to this heading">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="neurotorch.modules.layers.html">neurotorch.modules.layers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.base">neurotorch.modules.layers.base module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer"><code class="docutils literal notranslate"><span class="pre">BaseLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.__call__"><code class="docutils literal notranslate"><span class="pre">BaseLayer.__call__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.__init__"><code class="docutils literal notranslate"><span class="pre">BaseLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.build"><code class="docutils literal notranslate"><span class="pre">BaseLayer.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">BaseLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.device"><code class="docutils literal notranslate"><span class="pre">BaseLayer.device</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.forward"><code class="docutils literal notranslate"><span class="pre">BaseLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.freeze_weights"><code class="docutils literal notranslate"><span class="pre">BaseLayer.freeze_weights</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.get_and_reset_regularization_loss"><code class="docutils literal notranslate"><span class="pre">BaseLayer.get_and_reset_regularization_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.get_regularization_loss"><code class="docutils literal notranslate"><span class="pre">BaseLayer.get_regularization_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.infer_sizes_from_inputs"><code class="docutils literal notranslate"><span class="pre">BaseLayer.infer_sizes_from_inputs()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">BaseLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.is_built"><code class="docutils literal notranslate"><span class="pre">BaseLayer.is_built</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.is_ready_to_build"><code class="docutils literal notranslate"><span class="pre">BaseLayer.is_ready_to_build</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.requires_grad"><code class="docutils literal notranslate"><span class="pre">BaseLayer.requires_grad</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.reset_regularization_loss"><code class="docutils literal notranslate"><span class="pre">BaseLayer.reset_regularization_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.to"><code class="docutils literal notranslate"><span class="pre">BaseLayer.to()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer.update_regularization_loss"><code class="docutils literal notranslate"><span class="pre">BaseLayer.update_regularization_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.__init__"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.build"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.connectivity_convention"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.connectivity_convention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.force_dale_law"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.force_dale_law</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.forward"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_sign"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.forward_sign</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.forward_weights</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_sign_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_forward_sign_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_data"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_forward_weights_data()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_forward_weights_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_sign_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_recurrent_sign_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_data"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_recurrent_weights_data()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_recurrent_weights_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_sign_parameters"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_sign_parameters()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.get_weights_parameters"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.get_weights_parameters()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_sign"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.recurrent_sign</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.recurrent_weights</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_sign_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_forward_sign_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_data"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_forward_weights_data()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_forward_weights_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_sign_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_recurrent_sign_parameter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_data"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_recurrent_weights_data()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_parameter"><code class="docutils literal notranslate"><span class="pre">BaseNeuronsLayer.set_recurrent_weights_parameter()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.classical">neurotorch.modules.layers.classical module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.__init__"><code class="docutils literal notranslate"><span class="pre">Linear.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.build"><code class="docutils literal notranslate"><span class="pre">Linear.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.create_empty_state"><code class="docutils literal notranslate"><span class="pre">Linear.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.extra_repr"><code class="docutils literal notranslate"><span class="pre">Linear.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.Linear.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">Linear.initialize_weights_()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN"><code class="docutils literal notranslate"><span class="pre">LinearRNN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.__init__"><code class="docutils literal notranslate"><span class="pre">LinearRNN.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.build"><code class="docutils literal notranslate"><span class="pre">LinearRNN.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.create_empty_state"><code class="docutils literal notranslate"><span class="pre">LinearRNN.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.extra_repr"><code class="docutils literal notranslate"><span class="pre">LinearRNN.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.forward"><code class="docutils literal notranslate"><span class="pre">LinearRNN.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.classical.LinearRNN.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">LinearRNN.initialize_weights_()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.leaky_integrate">neurotorch.modules.layers.leaky_integrate module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer"><code class="docutils literal notranslate"><span class="pre">LILayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.__init__"><code class="docutils literal notranslate"><span class="pre">LILayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.build"><code class="docutils literal notranslate"><span class="pre">LILayer.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">LILayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.extra_repr"><code class="docutils literal notranslate"><span class="pre">LILayer.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.forward"><code class="docutils literal notranslate"><span class="pre">LILayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.LILayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">LILayer.initialize_weights_()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer"><code class="docutils literal notranslate"><span class="pre">SpyLILayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.__init__"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.build"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.build()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.extra_repr"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.forward"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.leaky_integrate.SpyLILayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">SpyLILayer.initialize_weights_()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.spiking">neurotorch.modules.layers.spiking module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.ALIFLayer"><code class="docutils literal notranslate"><span class="pre">ALIFLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.ALIFLayer.__init__"><code class="docutils literal notranslate"><span class="pre">ALIFLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.ALIFLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">ALIFLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.ALIFLayer.forward"><code class="docutils literal notranslate"><span class="pre">ALIFLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.ALIFLayer.update_regularization_loss"><code class="docutils literal notranslate"><span class="pre">ALIFLayer.update_regularization_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.BellecLIFLayer"><code class="docutils literal notranslate"><span class="pre">BellecLIFLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.BellecLIFLayer.__init__"><code class="docutils literal notranslate"><span class="pre">BellecLIFLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.BellecLIFLayer.forward"><code class="docutils literal notranslate"><span class="pre">BellecLIFLayer.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.IzhikevichLayer"><code class="docutils literal notranslate"><span class="pre">IzhikevichLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.IzhikevichLayer.__init__"><code class="docutils literal notranslate"><span class="pre">IzhikevichLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.IzhikevichLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">IzhikevichLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.IzhikevichLayer.forward"><code class="docutils literal notranslate"><span class="pre">IzhikevichLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.IzhikevichLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">IzhikevichLayer.initialize_weights_()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer"><code class="docutils literal notranslate"><span class="pre">LIFLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer.__init__"><code class="docutils literal notranslate"><span class="pre">LIFLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">LIFLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer.forward"><code class="docutils literal notranslate"><span class="pre">LIFLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">LIFLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.LIFLayer.update_regularization_loss"><code class="docutils literal notranslate"><span class="pre">LIFLayer.update_regularization_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.__init__"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.forward"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.reset_regularization_loss"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.reset_regularization_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyALIFLayer.update_regularization_loss"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayer.update_regularization_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.__init__"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.forward"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.reset_regularization_loss"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.reset_regularization_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking.SpyLIFLayer.update_regularization_loss"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayer.update_regularization_loss()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.spiking_lpf">neurotorch.modules.layers.spiking_lpf module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF"><code class="docutils literal notranslate"><span class="pre">ALIFLayerLPF</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.__init__"><code class="docutils literal notranslate"><span class="pre">ALIFLayerLPF.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.create_empty_state"><code class="docutils literal notranslate"><span class="pre">ALIFLayerLPF.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.extra_repr"><code class="docutils literal notranslate"><span class="pre">ALIFLayerLPF.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.forward"><code class="docutils literal notranslate"><span class="pre">ALIFLayerLPF.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF"><code class="docutils literal notranslate"><span class="pre">LIFLayerLPF</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.__init__"><code class="docutils literal notranslate"><span class="pre">LIFLayerLPF.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.create_empty_state"><code class="docutils literal notranslate"><span class="pre">LIFLayerLPF.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.extra_repr"><code class="docutils literal notranslate"><span class="pre">LIFLayerLPF.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.forward"><code class="docutils literal notranslate"><span class="pre">LIFLayerLPF.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayerLPF</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.__init__"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayerLPF.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.create_empty_state"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayerLPF.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.extra_repr"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayerLPF.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.forward"><code class="docutils literal notranslate"><span class="pre">SpyALIFLayerLPF.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayerLPF</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.__init__"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayerLPF.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.create_empty_state"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayerLPF.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.extra_repr"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayerLPF.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.forward"><code class="docutils literal notranslate"><span class="pre">SpyLIFLayerLPF.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers.wilson_cowan">neurotorch.modules.layers.wilson_cowan module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer"><code class="docutils literal notranslate"><span class="pre">WilsonCowanCURBDLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.__init__"><code class="docutils literal notranslate"><span class="pre">WilsonCowanCURBDLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">WilsonCowanCURBDLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.forward"><code class="docutils literal notranslate"><span class="pre">WilsonCowanCURBDLayer.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.__init__"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.create_empty_state"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.create_empty_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.forward"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.initialize_weights_"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.initialize_weights_()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.r"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.r</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.tau"><code class="docutils literal notranslate"><span class="pre">WilsonCowanLayer.tau</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.modules.layers.html#module-neurotorch.modules.layers">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType"><code class="docutils literal notranslate"><span class="pre">LayerType</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.ALIF"><code class="docutils literal notranslate"><span class="pre">LayerType.ALIF</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.Izhikevich"><code class="docutils literal notranslate"><span class="pre">LayerType.Izhikevich</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.LI"><code class="docutils literal notranslate"><span class="pre">LayerType.LI</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.LIF"><code class="docutils literal notranslate"><span class="pre">LayerType.LIF</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.SpyALIF"><code class="docutils literal notranslate"><span class="pre">LayerType.SpyALIF</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.SpyLI"><code class="docutils literal notranslate"><span class="pre">LayerType.SpyLI</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.SpyLIF"><code class="docutils literal notranslate"><span class="pre">LayerType.SpyLIF</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.LayerType.from_str"><code class="docutils literal notranslate"><span class="pre">LayerType.from_str()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-neurotorch.modules.base">
<span id="neurotorch-modules-base-module"></span><h2>neurotorch.modules.base module<a class="headerlink" href="#module-neurotorch.modules.base" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.base.</span></span><span class="sig-name descname"><span class="pre">BaseModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BaseModel'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.base.NamedModule" title="neurotorch.modules.base.NamedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">NamedModule</span></code></a></p>
<p>This class is the base class of all models.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>input_sizes: The input sizes of the model.</p></li>
<li><p>input_transform (torch.nn.ModuleDict): The transforms to apply to the inputs.</p></li>
<li><p>output_sizes: The output size of the model.</p></li>
<li><p>output_transform (torch.nn.ModuleDict): The transforms to apply to the outputs.</p></li>
<li><p>name: The name of the model.</p></li>
<li><p>checkpoint_folder: The folder where the checkpoints are saved.</p></li>
<li><p>kwargs: Additional arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Call self as a function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BaseModel'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor of the BaseModel class. This class is the base class of all models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_sizes</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>DimensionLike</em><em>]</em><em>, </em><em>SizeTypes</em><em>]</em>) – The input sizes of the model.</p></li>
<li><p><strong>output_size</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>DimensionLike</em><em>]</em><em>, </em><em>SizeTypes</em><em>]</em>) – The output size of the model.</p></li>
<li><p><strong>name</strong> (<em>str</em>) – The name of the model.</p></li>
<li><p><strong>checkpoint_folder</strong> (<em>str</em>) – The folder where the checkpoints are saved.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device of the model. If None, the default device is used.</p></li>
<li><p><strong>input_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transforms to apply to the inputs. The input_transform must work batch-wise.</p></li>
<li><p><strong>output_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transforms to apply to the outputs. The output_transform must work batch-wise.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>kwargs</strong> – Additional arguments.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.apply_input_transform">
<span class="sig-name descname"><span class="pre">apply_input_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.apply_input_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the input transform to the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – dict of inputs of shape (batch_size, <a href="#id1"><span class="problematic" id="id2">*</span></a>input_size)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The input of the network with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.apply_output_transform">
<span class="sig-name descname"><span class="pre">apply_output_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.apply_output_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the output transform to the outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>outputs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – dict of outputs of shape (batch_size, <a href="#id3"><span class="problematic" id="id4">*</span></a>output_size).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the network transformed.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.base.BaseModel" title="neurotorch.modules.base.BaseModel"><span class="pre">BaseModel</span></a></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Not used.</p></li>
<li><p><strong>kwargs</strong> – Not used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The network.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#neurotorch.modules.base.BaseModel" title="neurotorch.modules.base.BaseModel">BaseModel</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.checkpoints_meta_path">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">checkpoints_meta_path</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#neurotorch.modules.base.BaseModel.checkpoints_meta_path" title="Permalink to this definition">¶</a></dt>
<dd><p>The path to the checkpoints meta file.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The path to the checkpoints meta file.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">device</span></em><a class="headerlink" href="#neurotorch.modules.base.BaseModel.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The device of the model.
:rtype: torch.device</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_default_input_transform">
<span class="sig-name descname"><span class="pre">get_default_input_transform</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_default_input_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the default input transform. The default input transform is a to tensor transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The default input transform.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[str, nn.Module]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_default_output_transform">
<span class="sig-name descname"><span class="pre">get_default_output_transform</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_default_output_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the default output transform. The default output transform is an identity transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The default output transform.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[str, nn.Module]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_prediction_log_proba">
<span class="sig-name descname"><span class="pre">get_prediction_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_prediction_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction log probabilities of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The inputs of the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – If True, the outputs trace will be returned.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – If True, the hidden states will be returned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The prediction log probabilities.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_prediction_proba">
<span class="sig-name descname"><span class="pre">get_prediction_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_prediction_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction probabilities of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The inputs of the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – If True, the outputs trace will be returned.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – If True, the hidden states will be returned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The prediction probabilities.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_prediction_trace">
<span class="sig-name descname"><span class="pre">get_prediction_trace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_prediction_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction trace of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The inputs of the network.</p></li>
<li><p><strong>kwargs</strong> – Additional arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The prediction trace.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Dict[str, torch.Tensor], torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.get_raw_prediction">
<span class="sig-name descname"><span class="pre">get_raw_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.get_raw_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>The raw prediction of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The inputs of the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – If True, the outputs trace will be returned.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – If True, the hidden states will be returned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The raw prediction.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.hard_update">
<span class="sig-name descname"><span class="pre">hard_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#neurotorch.modules.base.BaseModel" title="neurotorch.modules.base.BaseModel"><span class="pre">BaseModel</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.hard_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the weights from the other network to this network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<em>'BaseModel'</em>) – The other network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.infer_sizes_from_inputs">
<span class="sig-name descname"><span class="pre">infer_sizes_from_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.infer_sizes_from_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer the input and output sizes from the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The inputs of the network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.input_sizes">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">input_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#neurotorch.modules.base.BaseModel.input_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.is_built">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_built</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.base.BaseModel.is_built" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.load_checkpoint">
<span class="sig-name descname"><span class="pre">load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoints_meta_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_checkpoint_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="neurotorch.callbacks.html#neurotorch.callbacks.checkpoints_manager.LoadCheckpointMode" title="neurotorch.callbacks.checkpoints_manager.LoadCheckpointMode"><span class="pre">LoadCheckpointMode</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">LoadCheckpointMode.BEST_ITR</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the checkpoint from the checkpoints_meta_path. If the checkpoints_meta_path is None, the default
checkpoints_meta_path is used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoints_meta_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The path to the checkpoints meta file.</p></li>
<li><p><strong>load_checkpoint_mode</strong> (<a class="reference internal" href="neurotorch.callbacks.html#neurotorch.callbacks.checkpoints_manager.LoadCheckpointMode" title="neurotorch.callbacks.checkpoints_manager.LoadCheckpointMode"><em>LoadCheckpointMode</em></a>) – The mode to use when loading the checkpoint.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – Whether to print the loaded checkpoint information.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The loaded checkpoint information.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.output_sizes">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">output_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#neurotorch.modules.base.BaseModel.output_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.soft_update">
<span class="sig-name descname"><span class="pre">soft_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#neurotorch.modules.base.BaseModel" title="neurotorch.modules.base.BaseModel"><span class="pre">BaseModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.soft_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the weights from the other network to this network with a factor of tau.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>other</strong> (<em>'BaseModel'</em>) – The other network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – The factor of the copy.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Move and/or cast the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.channels_last</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point or complex <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>s. In addition, this method will
only cast the floating point or complex parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#neurotorch.modules.base.BaseModel.device" title="neurotorch.modules.base.BaseModel.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point or complex dtype of
the parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
<li><p><strong>memory_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>) – the desired memory
format for 4D parameters and buffers in this module (keyword
only argument)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="go">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">))</span>
<span class="go">tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.BaseModel.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_viz</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.BaseModel.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an ONNX model from the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>in_viz</strong> (<em>Any</em>) – The input to visualize.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The ONNX model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.base.NamedModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.base.</span></span><span class="sig-name descname"><span class="pre">NamedModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.NamedModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.NamedModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.NamedModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.NamedModule.name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#neurotorch.modules.base.NamedModule.name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the name of the module. If the name is not set, it will be set to the class name.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The name of the module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.NamedModule.name_is_set">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name_is_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.base.NamedModule.name_is_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns whether the name of the module has been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether the name of the module has been set.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.base.SizedModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.base.</span></span><span class="sig-name descname"><span class="pre">SizedModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.SizedModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.base.NamedModule" title="neurotorch.modules.base.NamedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">NamedModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.base.SizedModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.base.SizedModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.SizedModule.input_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">input_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#neurotorch.modules.base.SizedModule.input_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.base.SizedModule.output_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">output_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#neurotorch.modules.base.SizedModule.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.functions">
<span id="neurotorch-modules-functions-module"></span><h2>neurotorch.modules.functions module<a class="headerlink" href="#module-neurotorch.modules.functions" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.functions.PSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.functions.</span></span><span class="sig-name descname"><span class="pre">PSigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.PSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies the Pseudo-Sigmoid function element-wise.</p>
<p>Pseudo-Sigmoid is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\\text{PSigmoid}(x) = \\frac{1}{1 + \\exp(- p \\odot x)}\end{split}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.functions.PSigmoid.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.PSigmoid.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.functions.PSigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.PSigmoid.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.functions.PSigmoid.p">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">p</span></span><a class="headerlink" href="#neurotorch.modules.functions.PSigmoid.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.functions.WeirdTanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.functions.</span></span><span class="sig-name descname"><span class="pre">WeirdTanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.WeirdTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies the Hyperbolic Tangent (Tanh) function element-wise.</p>
<p>Tanh is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Tanh}(x) = \tanh(x) = \frac{a\exp(\alpha x) - b\exp(-\beta x)} {c\exp(\gamma x) + d\exp(-\delta x)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.functions.WeirdTanh.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.WeirdTanh.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.functions.WeirdTanh.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.functions.WeirdTanh.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.sequential">
<span id="neurotorch-modules-sequential-module"></span><h2>neurotorch.modules.sequential module<a class="headerlink" href="#module-neurotorch.modules.sequential" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.sequential.</span></span><span class="sig-name descname"><span class="pre">Sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'Sequential'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.base.BaseModel" title="neurotorch.modules.base.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModel</span></code></a></p>
<p>The Sequential is a neural network that is constructed by stacking layers.</p>
<a class="reference internal image-reference" href="../../images/modules/Sequential_model_schm.png"><img alt="../../images/modules/Sequential_model_schm.png" class="align-center" src="../../images/modules/Sequential_model_schm.png" style="width: 300px;" /></a>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_layers</span></code> (torch.nn.ModuleDict): The input layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">hidden_layers</span></code> (torch.nn.ModuleList): The hidden layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_layers</span></code> (torch.nn.ModuleDict): The output layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">foresight_time_steps</span></code> (int): The number of time steps that the model will forecast.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'Sequential'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>The Sequential is a neural network that is constructed by stacking layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>layers</strong> – The layers to be used in the model. One of the following structure is expected:</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="o">*</span><span class="n">inputs_layers</span><span class="p">,</span> <span class="p">],</span>
    <span class="o">*</span><span class="n">hidden_layers</span><span class="p">,</span>
    <span class="p">[</span><span class="o">*</span><span class="n">output_layers</span><span class="p">,</span> <span class="p">]</span>
<span class="p">]</span>
<span class="ow">or</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">input_layer</span><span class="p">,</span>
    <span class="o">*</span><span class="n">hidden_layers</span><span class="p">,</span>
    <span class="n">output_layer</span>
<span class="p">]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the model.</p></li>
<li><p><strong>checkpoint_folder</strong> (<em>str</em>) – The folder where the checkpoints are saved.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use.</p></li>
<li><p><strong>input_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transform to apply to the input. The input_transform must work on a single datum.</p></li>
<li><p><strong>output_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transform to apply to the output trace. The output_transform must work batch-wise.</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.sequential.Sequential" title="neurotorch.modules.sequential.Sequential"><span class="pre">Sequential</span></a></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network and all its layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The network.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.sequential.Sequential" title="neurotorch.modules.sequential.Sequential">Sequential</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.build_layers">
<span class="sig-name descname"><span class="pre">build_layers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.build_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layers of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">device</span></em><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The device of the model.
:rtype: torch.device</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The inputs to the model where the dimensions are {input_name: (batch_size, input_size)}.</p></li>
<li><p><strong>kwargs</strong> – Additional arguments for the forward pass.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary of outputs where the values are the names of the layers and the values are the outputs
of the layers.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_all_layers">
<span class="sig-name descname"><span class="pre">get_all_layers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_all_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all the layers of the model as a list. The order of the layers is the same as the order of the layers in the
model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of all the layers of the model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[nn.Module]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_all_layers_names">
<span class="sig-name descname"><span class="pre">get_all_layers_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_all_layers_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all the names of the layers of the model. The order of the layers is the same as the order of the layers in
the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of all the names of the layers of the model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_and_reset_regularization_loss">
<span class="sig-name descname"><span class="pre">get_and_reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_and_reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the regularization loss as a sum of all the regularization losses of the layers. Then reset the
regularization losses.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the regularization loss.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_dict_of_layers">
<span class="sig-name descname"><span class="pre">get_dict_of_layers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_dict_of_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all the layers of the model as a dictionary. The order of the layers is the same as the order of the layers
in the model. The keys of the dictionary are the names of the layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary of all the layers of the model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[str, nn.Module]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_layer">
<span class="sig-name descname"><span class="pre">get_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a layer of the model. If the name is None, the first layer is returned which is useful when the model has
only one layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em>) – The name of the layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The layer with the given name. If the name is None, the first layer is returned.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_layers">
<span class="sig-name descname"><span class="pre">get_layers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the layers with the specified names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>layer_names</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The names of the layers to get.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The layers with the specified names.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[nn.Module]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_prediction_log_proba">
<span class="sig-name descname"><span class="pre">get_prediction_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_prediction_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction log probability of the model which is the log softmax of the output of the forward pass.
The log softmax is performed on the last dimension. This method is generally used for training in classification
task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the prediction log probability of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[tuple[Tensor, Any, Any], tuple[Tensor, Any], Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_prediction_proba">
<span class="sig-name descname"><span class="pre">get_prediction_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_prediction_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction probability of the model which is the softmax of the output of the forward pass.
The softmax is performed on the last dimension. This method is generally used for classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the prediction probability of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.get_raw_prediction">
<span class="sig-name descname"><span class="pre">get_raw_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.get_raw_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the raw prediction of the model which is the output of the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the raw prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.infer_sizes_from_inputs">
<span class="sig-name descname"><span class="pre">infer_sizes_from_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.infer_sizes_from_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer the sizes of the inputs layers from the inputs of the network. The sizes of the inputs layers are set to
the size of the inputs without the batch dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The inputs of the network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential.Sequential.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential.Sequential.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layers of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.sequential_rnn">
<span id="neurotorch-modules-sequential-rnn-module"></span><h2>neurotorch.modules.sequential_rnn module<a class="headerlink" href="#module-neurotorch.modules.sequential_rnn" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.sequential_rnn.</span></span><span class="sig-name descname"><span class="pre">SequentialRNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><span class="pre">BaseLayer</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><span class="pre">BaseLayer</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">foresight_time_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'SequentialRNN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.sequential.Sequential" title="neurotorch.modules.sequential.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>The SequentialRNN is a neural network that is constructed by stacking layers.</p>
<a class="reference internal image-reference" href="../../images/modules/Sequential_model_schm.png"><img alt="../../images/modules/Sequential_model_schm.png" class="align-center" src="../../images/modules/Sequential_model_schm.png" style="width: 300px;" /></a>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_layers</span></code> (torch.nn.ModuleDict): The input layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">hidden_layers</span></code> (torch.nn.ModuleList): The hidden layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_layers</span></code> (torch.nn.ModuleDict): The output layers of the model.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">foresight_time_steps</span></code> (int): The number of time steps that the model will forecast.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><span class="pre">BaseLayer</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.modules.layers.html#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><span class="pre">BaseLayer</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">foresight_time_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'SequentialRNN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_transform</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>The SequentialModel is a neural network that is constructed by stacking layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>layers</strong> – The layers to be used in the model. One of the following structure is expected:</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="o">*</span><span class="n">inputs_layers</span><span class="p">,</span> <span class="p">],</span>
    <span class="o">*</span><span class="n">hidden_layers</span><span class="p">,</span>
    <span class="p">[</span><span class="o">*</span><span class="n">output_layers</span><span class="p">,</span> <span class="p">]</span>
<span class="p">]</span>
<span class="ow">or</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">input_layer</span><span class="p">,</span>
    <span class="o">*</span><span class="n">hidden_layers</span><span class="p">,</span>
    <span class="n">output_layer</span>
<span class="p">]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>foresight_time_steps</strong> (<em>int</em>) – The number of time steps to predict in the future. When multiple inputs or outputs
are given, the outputs of the network are given to the inputs in the same order as they were specified in
the construction of the network. In other words, the first output is given to the first input, the second
output is given to the second input, and so on. If there are fewer outputs than inputs, the last inputs are
not considered as recurrent inputs, so they are not fed.</p></li>
<li><p><strong>name</strong> (<em>str</em>) – The name of the model.</p></li>
<li><p><strong>checkpoint_folder</strong> (<em>str</em>) – The folder where the checkpoints are saved.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use.</p></li>
<li><p><strong>input_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transform to apply to the input. The input_transform must work on a single datum.</p></li>
<li><p><strong>output_transform</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>List</em><em>[</em><em>Callable</em><em>]</em><em>]</em>) – The transform to apply to the output trace. The output_transform must work batch-wise.</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>out_memory_size</strong> (<em>int</em>) – The size of the memory buffer for the output trace. The output of each layer is
stored in the memory buffer. If the memory_size is not specified, the memory_size is set to
foresight_time_steps if specified, otherwise to is set to infinity.
Reduce this number to 1 if you want to use less memory and if you
don’t need the intermediate outputs. Default is foresight_time_steps if specified, otherwise inf.</p></li>
<li><p><strong>hh_memory_size</strong> (<em>int</em>) – The size of the memory buffer for the hidden state. The hidden state of each layer
is stored in the memory buffer. If the memory_size is not specified, the memory_size is set to
foresight_time_steps if specified, otherwise to is set
infinity. Reduce this number to 1 if you want to use less memory and if you don’t need the
intermediate hidden states. Default is foresight_time_steps if specified, otherwise inf.</p></li>
<li><p><strong>memory_device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device to use for the memory buffer. If not specified,
the memory_device is set to the device of the model. To use less cuda memory, you can set the
memory_device to cpu. However, this will slow down the computation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.sequential_rnn.SequentialRNN" title="neurotorch.modules.sequential_rnn.SequentialRNN"><span class="pre">SequentialRNN</span></a></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network and all its layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The network.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.sequential_rnn.SequentialRNN" title="neurotorch.modules.sequential_rnn.SequentialRNN">SequentialRNN</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the model.</p>
<dl>
<dt>When it comes to integrate a time series:</dt><dd><ul class="simple">
<li><p>We integrate the initial conditions &lt;time_step&gt; times.</p></li>
<li><p>We predict the remaining &lt;forward_sight_time_steps - 1&gt; time steps from the initial conditions</p></li>
<li><p>Please note that the last output of the integration of the initial conditions is the input for</p></li>
</ul>
<p>the integration of the remaining time steps AND also the first prediction.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Example</span><span class="p">:</span>
    <span class="n">time_series</span> <span class="o">=</span> <span class="p">[</span><span class="n">t_0</span><span class="p">,</span> <span class="n">t_1</span> <span class="o">...</span> <span class="n">t_N</span><span class="p">]</span> <span class="k">if</span><span class="p">:</span>
    <span class="p">[</span><span class="n">t_0</span><span class="p">,</span> <span class="n">t_1</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">Initial</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">then</span> <span class="n">t_1</span> <span class="n">generate</span> <span class="n">the</span> <span class="n">first</span> <span class="n">prediction</span> <span class="p">(</span><span class="n">t_2</span><span class="p">)</span> <span class="p">:</span>
    <span class="p">[</span><span class="n">t_2</span><span class="p">,</span> <span class="n">t_3</span> <span class="o">...</span> <span class="n">t_N</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">The</span> <span class="n">remaining</span> <span class="n">time</span> <span class="n">steps</span> <span class="n">are</span> <span class="n">predicted</span> <span class="kn">from</span><span class="w"> </span><span class="nn">the</span> <span class="n">initial</span> <span class="n">conditions</span><span class="o">.</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The inputs to the model where the dimensions are
{input_name: (batch_size, time_steps, input_size)}. If the inputs have the shape
(batch_size, input_size), then the time_steps is 1. All the inputs must have the same
time_steps otherwise the inputs with lower time_steps will be padded with zeros.</p></li>
<li><p><strong>init_hidden_states</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>...</em><em>]</em><em>]</em><em>]</em>) – The initial hidden states of the model. The dimensions are
{layer_name: (h_0[batch_size, hidden_size_0], …, h_K[batch_size, hidden_size_K])}
where K is the number of hidden states of the layer. If the initial hidden states are not
specified, the initial hidden states are set to None and will be initialized by the layer.</p></li>
<li><p><strong>kwargs</strong> – Additional arguments for the forward pass.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>foresight_time_steps</strong> (<em>int</em>) – The number of time steps to forecast. Default: The value of the
attribute :<code class="xref py py-attr docutils literal notranslate"><span class="pre">foresight_time_steps</span></code>.</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple of two dictionaries. The first dictionary contains the outputs of the model and the second
dictionary contains the hidden states of the model. The keys of the dictionaries are the
names of the layers. The values of the dictionaries are lists of tensors. The length of the
lists is the number of time steps.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple[Dict[str, torch.Tensor], Dict[str, Tuple[torch.Tensor, …]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_and_reset_regularization_loss">
<span class="sig-name descname"><span class="pre">get_and_reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_and_reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the regularization loss as a sum of all the regularization losses of the layers. Then reset the
regularization losses.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the regularization loss.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_fmt_prediction">
<span class="sig-name descname"><span class="pre">get_fmt_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">inputs:</span> <span class="pre">~torch.Tensor,</span> <span class="pre">lambda_func:</span> <span class="pre">~typing.Callable[[~torch.Tensor],</span> <span class="pre">~torch.Tensor]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">SequentialRNN.&lt;lambda&gt;&gt;,</span> <span class="pre">re_outputs_trace:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">re_hidden_states:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_fmt_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction of the model which is the output of the forward pass and apply the max operation on the
time dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>lambda_func</strong> – the function to apply on the output trace. Default is get the last item on the time.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the max prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_last_prediction">
<span class="sig-name descname"><span class="pre">get_last_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_last_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction of the model which is the output of the forward pass and get the last item on the
time dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the last prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_max_prediction">
<span class="sig-name descname"><span class="pre">get_max_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_max_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction of the model which is the output of the forward pass and apply the max operation on the
time dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the max prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_mean_prediction">
<span class="sig-name descname"><span class="pre">get_mean_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_mean_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction of the model which is the output of the forward pass and apply the mean operation on the
time dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the mean prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_log_proba">
<span class="sig-name descname"><span class="pre">get_prediction_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction log probability of the model which is the log softmax of the output of the forward pass.
The log softmax is performed on the time dimension. This method is generally used for training in classification
task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the prediction log probability of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[tuple[Tensor, Any, Any], tuple[Tensor, Any], Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_proba">
<span class="sig-name descname"><span class="pre">get_prediction_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prediction probability of the model which is the softmax of the output of the forward pass.
The softmax is performed on the time dimension. This method is generally used for classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the prediction probability of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any, Any], Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_trace">
<span class="sig-name descname"><span class="pre">get_prediction_trace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_prediction_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the prediction trace for the given inputs. Method used for time series prediction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – inputs to the network.</p></li>
<li><p><strong>kwargs</strong> – kwargs to be passed to the forward method.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>foresight_time_steps</strong> (<em>int</em>) – <p>number of time steps to predict. Default is self.foresight_time_steps.</p>
<dl class="simple">
<dt>:: Note: If the value of foresight_time_steps is specified, make sure that the values of the attributes</dt><dd><p><a class="reference internal" href="#neurotorch.modules.sequential_rnn.SequentialRNN.out_memory_size" title="neurotorch.modules.sequential_rnn.SequentialRNN.out_memory_size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">out_memory_size</span></code></a> and <a class="reference internal" href="#neurotorch.modules.sequential_rnn.SequentialRNN.hh_memory_size" title="neurotorch.modules.sequential_rnn.SequentialRNN.hh_memory_size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hh_memory_size</span></code></a> are correctly set.</p>
</dd>
</dl>
</p></li>
<li><p><strong>return_hidden_states</strong> (<em>bool</em>) – if True, returns the hidden states of the model. Default is False.</p></li>
<li><p><strong>trunc_time_steps</strong> (<em>int</em>) – number of time steps to truncate the prediction trace. Default is None.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the prediction trace.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Union[Dict[str, torch.Tensor], torch.Tensor, Tuple[torch.Tensor, …]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.get_raw_prediction">
<span class="sig-name descname"><span class="pre">get_raw_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_outputs_trace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.get_raw_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the raw prediction of the model which is the output of the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – inputs to the network.</p></li>
<li><p><strong>re_outputs_trace</strong> (<em>bool</em>) – Whether to return the outputs trace. Default is True.</p></li>
<li><p><strong>re_hidden_states</strong> (<em>bool</em>) – Whether to return the hidden states. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the raw prediction of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[Any, Any], Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.hh_memory_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hh_memory_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.hh_memory_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the size of the hidden state memory buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The size of the hidden state memory buffer.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.sequential_rnn.SequentialRNN.out_memory_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">out_memory_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#neurotorch.modules.sequential_rnn.SequentialRNN.out_memory_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the size of the output memory buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The size of the output memory buffer.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.spike_funcs">
<span id="neurotorch-modules-spike-funcs-module"></span><h2>neurotorch.modules.spike_funcs module<a class="headerlink" href="#module-neurotorch.modules.spike_funcs" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisidePhiApprox">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.spike_funcs.</span></span><span class="sig-name descname"><span class="pre">HeavisidePhiApprox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisidePhiApprox" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.spike_funcs.SpikeFunction" title="neurotorch.modules.spike_funcs.SpikeFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpikeFunction</span></code></a></p>
<p>Implementation of the spike function. The spike function is a differentiable version of the Heaviside function.
The Heaviside function is defined in the doc of <a class="reference internal" href="#neurotorch.modules.spike_funcs.SpikeFunction" title="neurotorch.modules.spike_funcs.SpikeFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpikeFunction</span></code></a>. The backward pass of this function is the
approximation of the heaviside used in <span id="id5">Bellec <em>et al.</em> [<a class="reference internal" href="#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>. This approximation is defined in equation
<a class="reference internal" href="#equation-heaviside-phi-approx">(1)</a>.</p>
<div class="math notranslate nohighlight" id="equation-heaviside-phi-approx">
<span class="eqno">(1)<a class="headerlink" href="#equation-heaviside-phi-approx" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \psi_j^t = \frac{\gamma_\text{pd}}{v_{\text{th}}}
    \text{max}\left(0, 1 - \left\vert\frac{v_j^t - A_j^t}{v_\text{th}}\right\vert\right)
\end{equation}\]</div>
<div class="docutils container" id="id6">
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ABMIZA<span class="fn-bracket">]</span></span>
<p><strong>missing journal in al-batah_modified_2010</strong></p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">BSS+20</a><span class="fn-bracket">]</span></span>
<p>Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. <em>Nature Communications</em>, 11(1):3625, 2020. URL: <a class="reference external" href="https://www.nature.com/articles/s41467-020-17236-y">https://www.nature.com/articles/s41467-020-17236-y</a> (visited on 2021-12-18), <a class="reference external" href="https://doi.org/10.1038/s41467-020-17236-y">doi:10.1038/s41467-020-17236-y</a>.</p>
</div>
<div class="citation" id="id47" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gro88<span class="fn-bracket">]</span></span>
<p>Stephen Grossberg. Nonlinear neural networks: principles, mechanisms, and architectures. <em>Neural Networks</em>, 1(1):17–61, 1988. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/0893608088900214">https://www.sciencedirect.com/science/article/pii/0893608088900214</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/0893-6080(88)90021-4">doi:https://doi.org/10.1016/0893-6080(88)90021-4</a>.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Izh07<span class="fn-bracket">]</span></span>
<p>Eugene M. Izhikevich. <em>Dynamical Systems in Neuroscience</em>. MIT Press, 2007. ISBN 978-0-262-09043-8.</p>
</div>
<div class="citation" id="id7" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NMZ19<span class="fn-bracket">]</span></span>
<p>Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. <em>IEEE Signal Processing Magazine</em>, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. <a class="reference external" href="https://doi.org/10.1109/MSP.2019.2931595">doi:10.1109/MSP.2019.2931595</a>.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PDD22<span class="fn-bracket">]</span></span>
<p>Vincent Painchaud, Nicolas Doyon, and Patrick Desrosiers. Beyond wilson-cowan dynamics: oscillations and chaos without inhibition. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2204.00583">https://arxiv.org/abs/2204.00583</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.2204.00583">doi:10.48550/ARXIV.2204.00583</a>.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PAS+<span class="fn-bracket">]</span></span>
<p>Matthew G. Perich, Charlotte Arlt, Sofia Soares, Megan E. Young, Clayton P. Mosher, Juri Minxha, Eugene Carter, Ueli Rutishauser, Peter H. Rudebeck, Christopher D. Harvey, and Kanaka Rajan. Inferring brain-wide interactions using data-constrained recurrent neural network models. Pages: 2020.12.18.423348 Section: New Results. URL: <a class="reference external" href="https://www.biorxiv.org/content/10.1101/2020.12.18.423348v2">https://www.biorxiv.org/content/10.1101/2020.12.18.423348v2</a> (visited on 2022-11-06), <a class="reference external" href="https://doi.org/10.1101/2020.12.18.423348">doi:10.1101/2020.12.18.423348</a>.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VRA05<span class="fn-bracket">]</span></span>
<p>Tim P. Vogels, Kanaka Rajan, and L.F. Abbott. Neural network dynamics. <em>Annual Review of Neuroscience</em>, 28(1):357–376, 2005. PMID: 16022600. URL: <a class="reference external" href="https://doi.org/10.1146/annurev.neuro.28.061604.135637">https://doi.org/10.1146/annurev.neuro.28.061604.135637</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1146/annurev.neuro.28.061604.135637">arXiv:https://doi.org/10.1146/annurev.neuro.28.061604.135637</a>, <a class="reference external" href="https://doi.org/10.1146/annurev.neuro.28.061604.135637">doi:10.1146/annurev.neuro.28.061604.135637</a>.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WC72<span class="fn-bracket">]</span></span>
<p>Hugh R Wilson and Jack D Cowan. Excitatory and inhibitory interactions in localized populations of model neurons. <em>Biophysical journal</em>, 12(1):1–24, 1972.</p>
</div>
<div class="citation" id="id49" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZSZ+<span class="fn-bracket">]</span></span>
<p>Chunyuan Zhang, Qi Song, Hui Zhou, Yigui Ou, Hongyao Deng, and Laurence Tianruo Yang. Revisiting recursive least squares for training deep neural networks. URL: <a class="reference external" href="http://arxiv.org/abs/2109.03220">http://arxiv.org/abs/2109.03220</a> (visited on 2022-11-06), <a class="reference external" href="https://arxiv.org/abs/2109.03220 [cs]">arXiv:2109.03220 [cs]</a>.</p>
</div>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisidePhiApprox.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FunctionCtx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisidePhiApprox.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>The implementation of the equation <a class="reference internal" href="#equation-heaviside-phi-approx">(1)</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>torch.autograd.function.FunctionCtx</em>) – The context of the function. It is used to retrieve information from the forward pass.</p></li>
<li><p><strong>grad_outputs</strong> (<em>torch.Tensor</em>) – The gradient of the loss with respect to the output of the forward pass.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The gradient of the loss with respect to the input of the forward pass.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisidePhiApprox.epsilon">
<span class="sig-name descname"><span class="pre">epsilon</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1e-05</span></em><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisidePhiApprox.epsilon" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisidePhiApprox.pseudo_derivative">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">pseudo_derivative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisidePhiApprox.pseudo_derivative" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisideSigmoidApprox">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.spike_funcs.</span></span><span class="sig-name descname"><span class="pre">HeavisideSigmoidApprox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisideSigmoidApprox" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.spike_funcs.SpikeFunction" title="neurotorch.modules.spike_funcs.SpikeFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpikeFunction</span></code></a></p>
<p>Implementation of the spike function. The spike function is a differentiable version of the Heaviside function.
The Heaviside function is defined in the doc of <a class="reference internal" href="#neurotorch.modules.spike_funcs.SpikeFunction" title="neurotorch.modules.spike_funcs.SpikeFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpikeFunction</span></code></a>. The backward pass of this function is the
first derivative of the fast sigmoid function defined in equation <a class="reference internal" href="#equation-fast-sigmoid">(2)</a>. The derivative
is shown in equation <a class="reference internal" href="#equation-fast-sigmoid-derivative">(3)</a> used in Zenke &amp; Ganguli (2018).</p>
<div class="math notranslate nohighlight" id="equation-fast-sigmoid">
<span class="eqno">(2)<a class="headerlink" href="#equation-fast-sigmoid" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    S(x) = \frac{1}{1 + e^{-x}}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-fast-sigmoid-derivative">
<span class="eqno">(3)<a class="headerlink" href="#equation-fast-sigmoid-derivative" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    S'(x) \approx \frac{x}{\left(1 + \gamma\vert{x - thr}\vert\right)^2}
\end{equation}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.HeavisideSigmoidApprox.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FunctionCtx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#neurotorch.modules.spike_funcs.HeavisideSigmoidApprox.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>The implementation of the equation <a class="reference internal" href="#equation-fast-sigmoid-derivative">(3)</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>torch.autograd.function.FunctionCtx</em>) – The context of the function. It is used to retrieve information from the forward pass.</p></li>
<li><p><strong>grad_outputs</strong> (<em>torch.Tensor</em>) – The gradient of the loss with respect to the output of the forward pass.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The gradient of the loss with respect to the input of the forward pass.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFuncType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.spike_funcs.</span></span><span class="sig-name descname"><span class="pre">SpikeFuncType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFuncType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFuncType.FastSigmoid">
<span class="sig-name descname"><span class="pre">FastSigmoid</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFuncType.FastSigmoid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFuncType.Phi">
<span class="sig-name descname"><span class="pre">Phi</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFuncType.Phi" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.spike_funcs.</span></span><span class="sig-name descname"><span class="pre">SpikeFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>Implementation of the spike function. The spike function is a differentiable version of the Heaviside function.
The Heaviside function is defined as the equation <a class="reference internal" href="#equation-heaviside">(4)</a>. The backward pass of this function has to be
an approximation of the derivative of the Heaviside function.</p>
<div class="math notranslate nohighlight" id="equation-heaviside">
<span class="eqno">(4)<a class="headerlink" href="#equation-heaviside" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{equation}
    H(x, thr) = \left\{
    \begin{matrix}
        1 &amp; \text{ if } x &gt; thr; \\
        0 &amp; \text{ else}.
    \end{matrix}
    \right.
\end{equation}\end{split}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FunctionCtx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor we need to compute the
surrogate gradient of the loss with respect to the input.
Here we use the normalized negative part of a fast sigmoid
as this was done in Zenke &amp; Ganguli (2018).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.spike_funcs.SpikeFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FunctionCtx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">tensor(1.)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">tensor(1.)</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.spike_funcs.SpikeFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The forward pass of the spike function is the Heaviside function. See the heaviside equation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>torch.autograd.function.FunctionCtx</em>) – The context of the function. It is used to store information for the backward pass. Use the method
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> to store information.</p></li>
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>threshold</strong> (<em>torch.Tensor</em>) – The threshold of the spike function.</p></li>
<li><p><strong>gamma</strong> (<em>torch.Tensor</em>) – The gamma parameter of the spike function. This parameter is used in the backward pass to
increase the gradient of the spike function. See child classes for more information.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the spike function.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.utils">
<span id="neurotorch-modules-utils-module"></span><h2>neurotorch.modules.utils module<a class="headerlink" href="#module-neurotorch.modules.utils" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.utils.DimensionsCat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.utils.</span></span><span class="sig-name descname"><span class="pre">DimensionsCat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.utils.DimensionsCat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.utils.DimensionsCat.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.utils.DimensionsCat.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Call self as a function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.utils.DimensionsCat.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.utils.DimensionsCat.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.utils.DimensionsCat.axes">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">axes</span></span><a class="headerlink" href="#neurotorch.modules.utils.DimensionsCat.axes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.wrappers">
<span id="neurotorch-modules-wrappers-module"></span><h2>neurotorch.modules.wrappers module<a class="headerlink" href="#module-neurotorch.modules.wrappers" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.NamedModuleWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">NamedModuleWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.NamedModuleWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.base.NamedModule" title="neurotorch.modules.base.NamedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">NamedModule</span></code></a></p>
<p>Wrapper for a module that does not inherit from NamedModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.NamedModuleWrapper.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.NamedModuleWrapper.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.NamedModuleWrapper.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.NamedModuleWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.SizedModuleWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">SizedModuleWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.SizedModuleWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.base.SizedModule" title="neurotorch.modules.base.SizedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SizedModule</span></code></a></p>
<p>Wrapper for a module that does not inherit from SizedModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.SizedModuleWrapper.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.SizedModuleWrapper.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.SizedModuleWrapper.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.SizedModuleWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.SizedModuleWrapper.infer_input_size">
<span class="sig-name descname"><span class="pre">infer_input_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.SizedModuleWrapper.infer_input_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.wrappers.SizedModuleWrapper.infer_output_size">
<span class="sig-name descname"><span class="pre">infer_output_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.wrappers.SizedModuleWrapper.infer_output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-neurotorch.modules" title="Permalink to this heading">¶</a></h2>
</section>
</section>


                        
                    </div>
                </div>
            </div>
        </div>
    </div>    


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
    <script type="text/javascript" src="static/documentation_options.js"></script>
    <script type="text/javascript" src="static/doctools.js"></script>
    <script type="text/javascript" src="static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="static/js/theme.js"></script>
  
    <div class="footer" role="contentinfo">
        <div class="container">
            &#169; Copyright 2022, Jérémie Gince.
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1.
        </div>
    </div>  

</body>
</html>