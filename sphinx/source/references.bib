@article{neftci_surrogate_2019,
	title = {Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks},
	volume = {36},
	issn = {1558-0792},
	doi = {10.1109/MSP.2019.2931595},
	shorttitle = {Surrogate Gradient Learning in Spiking Neural Networks},
	abstract = {Spiking neural networks ({SNNs}) are nature's versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking {NN} processors have attempted to emulate biological {NNs}. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional {NNs}, {SNNs} can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training {SNNs} and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient ({SG}) methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
	pages = {51--63},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
	date = {2019-11},
	note = {Conference Name: {IEEE} Signal Processing Magazine},
	keywords = {Neural networks, Biological system modeling, Energy efficiency, Fault tolerance},
	year = {2019}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}



@online{noauthor_mnist_nodate,
	title = {{MNIST} handwritten digit database, Yann {LeCun}, Corinna Cortes and Chris Burges},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2022-04-12},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:C\:\\Users\\gince\\Zotero\\storage\\98TBHR6P\\mnist.html:text/html},
}



@book{izhikevich_dynamical_2007,
	title = {Dynamical Systems in Neuroscience},
	isbn = {978-0-262-09043-8},
	abstract = {In order to model neuronal behavior or to interpret the results of modeling studies, neuroscientists must call upon methods of nonlinear dynamics. This book offers an introduction to nonlinear dynamical systems theory for researchers and graduate students in neuroscience. It also provides an overview of neuroscience for mathematicians who want to learn the basic facts of electrophysiology. Dynamical Systems in Neuroscience presents a systematic study of the relationship of electrophysiology, nonlinear dynamics, and computational properties of neurons. It emphasizes that information processing in the brain depends not only on the electrophysiological properties of neurons but also on their dynamical properties. The book introduces dynamical systems, starting with one- and two-dimensional Hodgkin-Huxley-type models and continuing to a description of bursting systems. Each chapter proceeds from the simple to the complex, and provides sample problems at the end. The book explains all necessary mathematical concepts using geometrical intuition; it includes many figures and few equations, making it especially suitable for non-mathematicians. Each concept is presented in terms of both neuroscience and mathematics, providing a link between the two disciplines. Nonlinear dynamical systems theory is at the core of computational neuroscience research, but it is not a standard part of the graduate neuroscience curriculum—or taught by math or physics department in a way that is suitable for students of biology. This book offers neuroscience students and researchers a comprehensive account of concepts and methods increasingly used in computational neuroscience.An additional chapter on synchronization, with more advanced material, can be found at the author's website, www.izhikevich.com.},
	pagetotal = {522},
	publisher = {{MIT} Press},
	author = {Izhikevich, Eugene M.},
	date = {2007},
	year = {2007},
	langid = {english},
	keywords = {Medical / Neuroscience},
	annotation = {Ce livre explore les dynamiques utilisé en neurosciences et explique leur provenance en biologie. Il passe en revue les concepts fondamentales des neurosciences et des systèmes dynamiques dans l'objectif de donner au lecteur les connaissance requise pour s'initier à ces domaines complexes. Axé sur les mathématique on y voit un ensemble complet d'outils permettant de résoudre un grand nombre de problèmes partant des équation différentiel simple à l'analyse complexe de système dynamique.},
}


@article{bellec_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17236-y},
	doi = {10.1038/s41467-020-17236-y},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time ({BPTT}), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	pages = {3625},
	number = {1},
	journal = {Nature Communications},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	urldate = {2021-12-18},
	date = {2020-07-17},
	year = {2020},
	langid = {english},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models, Neuroscience, Synaptic plasticity},
}

@article{10.1162/neco_a_01086,
    author = {Zenke, Friedemann and Ganguli, Surya},
    title = "{SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks}",
    journal = {Neural Computation},
    volume = {30},
    number = {6},
    pages = {1514-1541},
    year = {2018},
    month = {06},
    abstract = "{A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01086},
    url = {https://doi.org/10.1162/neco\_a\_01086},
    eprint = {https://direct.mit.edu/neco/article-pdf/30/6/1514/1039264/neco\_a\_01086.pdf},
}



@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2022-04-13},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	annotation = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\gince\\Zotero\\storage\\8FDB5URI\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\gince\\Zotero\\storage\\J3XTYVIP\\1412.html:text/html},
}

@article{Gince_LamontagneCaron_SNNImgClassification_2022,
  title={SNN Image Classification},
  author={Gince, Jérémie and Lamontagne-Caron, Rémi},
  year={2022},
  publisher={Université Laval},
  url={https://github.com/JeremieGince/SNNImageClassification},
}

@misc{python.org,
    title={Python.org},
    url={https://www.python.org/},
    journal={Python.org}
}

@misc{pytorch,
    title={PyTorch},
    url={https://pytorch.org/},
    journal={PyTorch}
}

@misc{tensorflow,
    title={Tensorflow},
    url={https://www.tensorflow.org/?hl=en},
    journal={TensorFlow}
}


@article{davies_loihi_2018,
	title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
	volume = {38},
	issn = {1937-4143},
	doi = {10.1109/MM.2018.112130359},
	shorttitle = {Loihi},
	abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve {LASSO} optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a {CPU} iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
	pages = {82--99},
	number = {1},
	journaltitle = {{IEEE} Micro},
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and {McCoy}, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	date = {2018-01},
	note = {Conference Name: {IEEE} Micro},
	keywords = {Biological neural networks, machine learning, Computer architecture, Neuromorphics, Neurons, artificial intelligence, Computational modeling, Algorithm design and analysis, neuromorphic computing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\gince\\Zotero\\storage\\LLDTCEAV\\8259423.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\UVK56WKR\\Davies et al. - 2018 - Loihi A Neuromorphic Manycore Processor with On-C.pdf:application/pdf},
}



@book{ermentrout_mathematical_2010,
	title = {Mathematical foundations of neuroscience},
	volume = {35},
	publisher = {Springer},
	author = {Ermentrout, Bard and Terman, David H},
	date = {2010},
	annotation = {Cet oeuvre passe en revue les concepts mathématiques fondamentaux en neurosciences. Tout en gardant un aspect plus qualitatif, il montre les différents modèle de neurones et les différentes façons pour modéliser son activité. Complémentaire au livre de M. Izhikevich {\textbackslash}cite\{izhikevich\_dynamical\_2007\}, son objectif principal est d'offrir au lecteurs la compréhension nécessaire afin de commencer de débuter dans le domaine des neurosciences.},
}






@article{dewolf_spiking_2021,
	title = {Spiking neural networks take control},
	volume = {6},
	url = {http://www.science.org/doi/full/10.1126/scirobotics.abk3268},
	doi = {10.1126/scirobotics.abk3268},
	pages = {eabk3268},
	number = {58},
	journaltitle = {Science Robotics},
	author = {{DeWolf}, Travis},
	urldate = {2022-02-09},
	date = {2021-09-08},
	note = {Publisher: American Association for the Advancement of Science},
	annotation = {L'article met en valeur le fait que les réseaux d'apprentissage à impulsions ({SNN}) deviennent de plus en plus populaire et prometteur.},
	file = {DeWolf - 2021 - Spiking neural networks take control.pdf:C\:\\Users\\gince\\Zotero\\storage\\I6HQKG7T\\DeWolf - 2021 - Spiking neural networks take control.pdf:application/pdf},
}



@book{goodfellow_deep_2016,
	title = {Deep Learning},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	pagetotal = {801},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016-11-10},
	langid = {english},
	note = {Google-Books-{ID}: {omivDQAAQBAJ}},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science},
	annotation = {Ce livre passe en revue les concepts de base en apprentissage automatique et en apprentissage profond. De l'algèbre linéaire de base aux modèle génératif, on y voit un ensemble complets de méthodes et d'outils mathématiques afin de commencer dans le domaine de l'apprentissage profond. Il contient des aspects important du domaine comme les algorithme d'optimisations de paramètres, les modèles populaires comme les réseaux de perceptrons, les modèles à convolutions et les réseaux récurrents.},
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - Fashion-{MNIST} Benchmark (Image Classification)},
	url = {https://paperswithcode.com/sota/image-classification-on-fashion-mnist},
	abstract = {The current state-of-the-art on Fashion-{MNIST} is Fine-Tuning {DARTS}. See a full comparison of 11 papers with code.},
	urldate = {2022-05-01},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Remi\\Zotero\\storage\\M5N9QQML\\image-classification-on-fashion-mnist.html:text/html},
}

@article{atiya_new_2000,
	title = {New results on recurrent network training: unifying the algorithms and accelerating convergence},
	volume = {11},
	issn = {1045-9227},
	doi = {10.1109/72.846741},
	shorttitle = {New results on recurrent network training},
	abstract = {How to efficiently train recurrent networks remains a challenging and active research topic. Most of the proposed training approaches are based on computational ways to efficiently obtain the gradient of the error function, and can be generally grouped into five major groups. In this study we present a derivation that unifies these approaches. We demonstrate that the approaches are only five different ways of solving a particular matrix equation. The second goal of this paper is develop a new algorithm based on the insights gained from the novel formulation. The new algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems. In addition, it reaches the error minimum in a much smaller number of iterations. A desirable characteristic of recurrent network training algorithms is to be able to update the weights in an on-line fashion. We have also developed an on-line version of the proposed algorithm, that is based on updating the error gradient approximation in a recursive manner.},
	pages = {697--709},
	number = {3},
	journaltitle = {{IEEE} transactions on neural networks},
	shortjournal = {{IEEE} Trans Neural Netw},
	author = {Atiya, A. F. and Parlos, A. G.},
	date = {2000},
	pmid = {18249797},
}

@article{cramer_heidelberg_2022,
	title = {The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9311226/},
	doi = {10.1109/TNNLS.2020.3044364},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Cramer, Benjamin and Stradmann, Yannik and Schemmel, Johannes and Zenke, Friedemann},
	urldate = {2022-06-29},
	date = {2022},
	file = {Full Text:C\:\\Users\\gince\\Zotero\\storage\\3IEQ2ZHC\\Cramer et al. - 2022 - The Heidelberg Spiking Data Sets for the Systemati.pdf:application/pdf},
}


@article{london2005dendritic,
  title={Dendritic computation},
  author={London, Michael and H{\"a}usser, Michael},
  journal={Annu. Rev. Neurosci.},
  volume={28},
  pages={503--532},
  year={2005},
  publisher={Annual Reviews}
}


@article{bellec_eligibility_2019,
	title = {Eligibility traces provide a data-inspired alternative to backpropagation through time},
	url = {https://openreview.net/forum?id=SkxJ4QKIIS},
	abstract = {We present eligibility propagation an alternative to {BPTT} that is compatible with experimental data on synaptic plasticity and competes with {BPTT} on machine learning benchmarks.},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
	urldate = {2022-03-14},
	date = {2019-09-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\SKY7UZ9R\\Bellec et al. - 2019 - Eligibility traces provide a data-inspired alterna.pdf:application/pdf;Snapshot:C\:\\Users\\gince\\Zotero\\storage\\K3YNWDQ5\\forum.html:text/html},
}



@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	rights = {2016 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms13276},
	doi = {10.1038/ncomms13276},
	abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron’s axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
	pages = {13276},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	urldate = {2022-04-05},
	date = {2016-11-08},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms},
	file = {Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\6IFKXRIH\\Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf:application/pdf;Snapshot:C\:\\Users\\gince\\Zotero\\storage\\TXPTFBIM\\ncomms13276.html:text/html},
}


@article{bassett2017network,
  title={Network neuroscience},
  author={Bassett, Danielle S and Sporns, Olaf},
  journal={Nature neuroscience},
  volume={20},
  number={3},
  pages={353--364},
  year={2017},
  publisher={Nature Publishing Group}
}


@article{scheffer2020connectome,
  title={A Connectome and Analysis of the Adult Drosophila Central Brain},
  author={Scheffer, Louis K and Xu, C Shan and Januszewski, Michal and Lu, Zhiyuan and Takemura, Shin-ya and Hayworth, Kenneth J and Huang, Gary and Shinomiya, Kazunori and Maitlin-Shepard, Jeremy and Berg, Stuart and others},
  journal={eLife},
  volume ={2020},
  number={9},
  pages={e57443},
  year={2020},
  publisher={}
}

@article{bressler2011wiener,
  title={Wiener--Granger causality: a well established methodology},
  author={Bressler, Steven L and Seth, Anil K},
  journal={Neuroimage},
  volume={58},
  number={2},
  pages={323--329},
  year={2011},
  publisher={Elsevier}
}


@ARTICLE {raulvicentemichaelwibralmichaellindneretgordonpipa2011,
    author  = "Raul Vicente, Michael Wibral, Michael Lindner et Gordon Pipa",
    title   = "Transfer entropy— a model-free measure of effective connectivity for the neurosciences.",
    journal = "Journal of computational neuroscience",
    year    = "2011",
    volume  = "30(1)",
    pages   = "45–67"
}


@article{runge2019detecting,
  title={Detecting and quantifying causal associations in large nonlinear time series datasets},
  author={Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
  journal={Science Advances},
  volume={5},
  number={11},
  pages={eaau4996},
  year={2019},
  publisher={American Association for the Advancement of Science}
}


@article{vanwalleghem2018integrative,
  title={Integrative whole-brain neuroscience in larval zebrafish},
  author={Vanwalleghem, Gilles C and Ahrens, Misha B and Scott, Ethan K},
  journal={Current opinion in neurobiology},
  volume={50},
  pages={136--145},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{pavlidis_spiking_2005,
	title = {Spiking neural network training using evolutionary algorithms},
	volume = {4},
	doi = {10.1109/IJCNN.2005.1556240},
	abstract = {Networks of spiking neurons can perform complex non-linear computations in fast temporal coding just as well as rate coded networks. These networks differ from previous models in that spiking neurons communicate information by the timing, rather than the rate, of spikes. To apply spiking neural networks on particular tasks, a learning process is required. Most existing training algorithms are based on unsupervised Hebbian learning. In this paper, we investigate the performance of the parallel differential evolution algorithm, as a supervised training algorithm for spiking neural networks. The approach was successfully tested on well-known and widely used classification problems.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {2190--2194 vol. 4},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Pavlidis, N.G. and Tasoulis, O.K. and Plagianakos, V.P. and Nikiforidis, G. and Vrahatis, M.N.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Artificial neural networks, Biological neural networks, Biological system modeling, Neural networks, Neurons, Computational modeling, Computer networks, Evolutionary computation, Signal processing algorithms, Timing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\gince\\Zotero\\storage\\865WYZV8\\1556240.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\4UKI2GLP\\Pavlidis et al. - 2005 - Spiking neural network training using evolutionary.pdf:application/pdf},
}


@inproceedings{capuozzo_compact_2011,
	title = {A compact evolutionary algorithm for integer spiking neural network robot controllers},
	doi = {10.1109/SECON.2011.5752941},
	abstract = {In order to facilitate online training of a robot controller composed of a spiking neural network, we propose the creation of a method dubbed a `compact evolutionary algorithm'. The compact evolutionary algorithm, derived from the compact genetic algorithm, greatly reduces the memory requirements for evolutionary optimization and also obviates the need for floating-point arithmetic capabilities allowing its efficient implementation by microcontrollers. The compact evolutionary algorithm is compared to the traditional evolutionary algorithm for solving three cyclic functions that are of use in a walking robot.},
	eventtitle = {2011 Proceedings of {IEEE} Southeastcon},
	pages = {237--242},
	booktitle = {2011 Proceedings of {IEEE} Southeastcon},
	author = {Capuozzo, Mario D. and Livingston, David L.},
	date = {2011-03},
	note = {{ISSN}: 1558-058X},
	keywords = {Biological neural networks, Evolutionary computation, Genetic algorithms, Legged locomotion, Microcontrollers, Neurons},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\55MIRTQT\\Capuozzo and Livingston - 2011 - A compact evolutionary algorithm for integer spiki.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\gince\\Zotero\\storage\\HW7SW926\\5752941.html:text/html},
}


@incollection{eiben_what_2015,
	location = {Berlin, Heidelberg},
	title = {What Is an Evolutionary Algorithm?},
	isbn = {978-3-662-44874-8},
	url = {https://doi.org/10.1007/978-3-662-44874-8_3},
	series = {Natural Computing Series},
	abstract = {The most important aim of this chapter is to describe what an evolutionary algorithm ({EA}) is. In order to give a unifying view we present a general scheme that forms the common basis for all the different variants of evolutionary algorithms. The main components of {EAs} are discussed, explaining their role and related issues of terminology. This is immediately followed by two example applications to make things more concrete. We then go on to discuss general issues concerning the operation of {EAs}, to place them in a broader context and explain their relationship with other global optimisation techniques.},
	pages = {25--48},
	booktitle = {Introduction to Evolutionary Computing},
	publisher = {Springer},
	author = {Eiben, A. E. and Smith, J. E.},
	editor = {Eiben, A.E. and Smith, J.E.},
	urldate = {2022-02-18},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-662-44874-8_3},
	keywords = {Candidate Solution, Evolutionary Algorithm, Knapsack Problem, Local Search Algorithm, Mutation Operator},
	file = {Springer Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\5VDAHRSS\\Eiben and Smith - 2015 - What Is an Evolutionary Algorithm.pdf:application/pdf},
}

@inproceedings{rounds_evolutionary_2016,
	location = {Cham},
	title = {An Evolutionary Framework for Replicating Neurophysiological Data with Spiking Neural Networks},
	isbn = {978-3-319-45823-6},
	doi = {10.1007/978-3-319-45823-6_50},
	series = {Lecture Notes in Computer Science},
	abstract = {Here we present a framework for the automatic tuning of spiking neural networks ({SNNs}) that utilizes an evolutionary algorithm featuring indirect encoding to achieve a drastic reduction in the dimensionality of the parameter space, combined with a {GPU}-accelerated {SNN} simulator that results in a considerable decrease in the time needed for fitness evaluation, despite the need for both a training and a testing phase. We tuned the parameters governing a learning rule called spike-timing-dependent plasticity ({STDP}), which was used to alter the synaptic weights of the network. We validated this framework by applying it to a case study in which synthetic neuronal firing rates were matched to electrophysiologically recorded neuronal firing rates in order to evolve network functionality. Our framework was not only able to match their firing rates, but also captured functional and behavioral aspects of the biological neuronal population, in roughly 50 generations.},
	pages = {537--547},
	booktitle = {Parallel Problem Solving from Nature – {PPSN} {XIV}},
	publisher = {Springer International Publishing},
	author = {Rounds, Emily L. and Scott, Eric O. and Alexander, Andrew S. and De Jong, Kenneth A. and Nitz, Douglas A. and Krichmar, Jeffrey L.},
	editor = {Handl, Julia and Hart, Emma and Lewis, Peter R. and López-Ibáñez, Manuel and Ochoa, Gabriela and Paechter, Ben},
	date = {2016},
	langid = {english},
	keywords = {Spiking neural networks, Data matching, Evolutionary algorithms, Indirect encoding, Neurophysiological recordings, Parallel computing, Plasticity},
	file = {Springer Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\ZXS7C9BG\\Rounds et al. - 2016 - An Evolutionary Framework for Replicating Neurophy.pdf:application/pdf},
}



@article{kim_learning_2018,
	title = {Learning recurrent dynamics in spiking networks},
	volume = {7},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.37124},
	doi = {10.7554/eLife.37124},
	abstract = {Spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. While the output of recurrent network models can learn to perform various tasks, the possible range of recurrent dynamics that emerge after learning remains unknown. Here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity. We apply the training method to learn arbitrary firing patterns, stabilize irregular spiking activity in a network of excitatory and inhibitory neurons respecting Dale’s law, and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement. We identify sufficient conditions for successful learning, characterize two types of learning errors, and assess the network capacity. Our findings show that synaptically-coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain.},
	pages = {e37124},
	journaltitle = {{eLife}},
	author = {Kim, Christopher M and Chow, Carson C},
	urldate = {2021-12-21},
	date = {2018-09-20},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {learning, recurrent dynamics, spiking network, universal dynamics},
	file = {Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\FK678ZHF\\Kim and Chow - 2018 - Learning recurrent dynamics in spiking networks.pdf:application/pdf},
}




@software{norse2021,
  author       = {Pehle, Christian and
                  Pedersen, Jens Egholm},
  title        = {{Norse -  A deep learning library for spiking
                   neural networks}},
  month        = jan,
  year         = 2021,
  note         = {Documentation: https://norse.ai/docs/},
  publisher    = {Zenodo},
  version      = {0.0.7},
  doi          = {10.5281/zenodo.4422025},
  url          = {https://doi.org/10.5281/zenodo.4422025}
}


@article{eshraghian2021training,
        title   =  {Training spiking neural networks using lessons from deep learning},
        author  =  {Eshraghian, Jason K and Ward, Max and Neftci, Emre and Wang, Xinxin
                    and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and
                   Jeong, Doo Seok and Lu, Wei D},
        journal = {arXiv preprint arXiv:2109.12894},
        year    = {2021}
}


@ARTICLE{Werbos_BPTT,  author={Werbos, P.J.},  journal={Proceedings of the IEEE},   title={Backpropagation through time: what it does and how to do it},   year={1990},  volume={78},  number={10},  pages={1550-1560},  doi={10.1109/5.58337}}

@misc{yao_heidelberg,
  doi = {10.48550/ARXIV.2107.11711},

  url = {https://arxiv.org/abs/2107.11711},

  author = {Yao, Man and Gao, Huanhuan and Zhao, Guangshe and Wang, Dingheng and Lin, Yihan and Yang, Zhaoxu and Li, Guoqi},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Temporal-wise Attention Spiking Neural Networks for Event Streams Classification},

  publisher = {arXiv},

  year = {2021},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{Cramer9311226,
  author={Cramer, Benjamin and Stradmann, Yannik and Schemmel, Johannes and Zenke, Friedemann},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks},
  year={2022},
  volume={33},
  number={7},
  pages={2744-2757},
  doi={10.1109/TNNLS.2020.3044364}
}


@article{wilson1972excitatory,
  title={Excitatory and inhibitory interactions in localized populations of model neurons},
  author={Wilson, Hugh R and Cowan, Jack D},
  journal={Biophysical journal},
  volume={12},
  number={1},
  pages={1--24},
  year={1972},
  publisher={Elsevier}
}


@misc{PainchaudDoyonDesrosiers2022,
  doi = {10.48550/ARXIV.2204.00583},

  url = {https://arxiv.org/abs/2204.00583},

  author = {Painchaud, Vincent and Doyon, Nicolas and Desrosiers, Patrick},

  keywords = {Neurons and Cognition (q-bio.NC), Biological Physics (physics.bio-ph), FOS: Biological sciences, FOS: Biological sciences, FOS: Physical sciences, FOS: Physical sciences},

  title = {Beyond Wilson-Cowan dynamics: oscillations and chaos without inhibition},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}

@article{VogelsTimRajanAbbott2005NeuralNetworkDynamics,
author = {Vogels, Tim P. and Rajan, Kanaka and Abbott, L.F.},
title = {NEURAL NETWORK DYNAMICS},
journal = {Annual Review of Neuroscience},
volume = {28},
number = {1},
pages = {357-376},
year = {2005},
doi = {10.1146/annurev.neuro.28.061604.135637},
    note ={PMID: 16022600},

URL = {
        https://doi.org/10.1146/annurev.neuro.28.061604.135637

},
eprint = {
        https://doi.org/10.1146/annurev.neuro.28.061604.135637

}
,
    abstract = { Neural network modeling is often concerned with stimulus-driven responses, but most of the activity in the brain is internally generated. Here, we review network models of internally generated activity, focusing on three types of network dynamics: (a) sustained responses to transient stimuli, which provide a model of working memory; (b) oscillatory network activity; and (c) chaotic activity, which models complex patterns of background spiking in cortical and other circuits. We also review propagation of stimulus-driven activity through spontaneously active networks. Exploring these aspects of neural network dynamics is critical for understanding how neural circuits produce cognitive function. }
}


@article{GROSSBERG198817,
title = {Nonlinear neural networks: Principles, mechanisms, and architectures},
journal = {Neural Networks},
volume = {1},
number = {1},
pages = {17-61},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90021-4},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900214},
author = {Stephen Grossberg},
abstract = {An historical discussion is provided of the intellectual trends that caused nineteenth century interdisciplinary studies of physics and psychobiology by leading scientists such as Helmholtz, Maxwell, and Mach to splinter into separate twentieth-century scientific movements. The nonlinear, nonstationary, and nonlocal nature of behavioral and brain data are emphasized. Three sources of contemporary neural network research—the binary, linear, and continuous-nonlinear models—are noted. The remainder of the article describes results about continuous-nonlinear models: Many models of content-addressable memory are shown to be special cases of the Cohen-Grossberg model and global Liapunov function, including the additive, brain-state-in-a-box, McCulloch-Pitts, Boltzmann machine, Hartline-Ratliff-Miller, shunting, masking field, bidirectional associative memory, Volterra-Lotka, Gilpin-Ayala, and Eigen-Schuster models. A Liapunov functional method is described for proving global limit or oscillation theorems for nonlinear competitive systems when their decision schemes are globally consistent or inconsistent, respectively. The former case is illustrated by a model of a globally stable economic market, and the latter case is illustrated by a model of the voting paradox. Key properties of shunting competitive feedback networks are summarized, including the role of sigmoid signalling, automatic gain control, competitive choice and quantization, tunable filtering, total activity normalization, and noise suppression in pattern transformation and memory storage applications. Connections to models of competitive learning, vector quantization, and categorical perception are noted. Adaptive resonance theory (ART) models for self-stabilizing adaptive pattern recognition in response to complex real-time nonstationary input environments are compared with off-line models such as autoassociators, the Boltzmann machine, and back propagation. Special attention is paid to the stability and capacity of these models, and to the role of top-down expectations and attentional processing in the active regulation of both learning and fast information processing. Models whose performance and learning are regulated by internal gating and matching signals, or by external environmentally generated error signals, are contrasted with models whose learning is regulated by external teacher signals that have no analog in natural real-time environments. Examples from sensory-motor control of adaptive vector encoders, adaptive coordinate transformations, adaptive gain control by visual error signals, and automatic generation of synchronous multijoint movement trajectories illustrate the former model types. Internal matching processes are shown capable of discovering several different types of invariant environmental properties. These include ART mechanisms which discover recognition invariants, adaptive vector encoder mechanisms which discover movement invariants, and autoreceptive associative mechanisms which discover invariants of self-regulating target position maps.}
}


@misc{perich_inferring_2021,
	title = {Inferring brain-wide interactions using data-constrained recurrent neural network models},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.18.423348v2},
	doi = {10.1101/2020.12.18.423348},
	abstract = {Behavior arises from the coordinated activity of numerous anatomically and functionally distinct brain regions. Modern experimental tools allow unprecedented access to large neural populations spanning many interacting regions brain-wide. Yet, understanding such large-scale datasets necessitates both scalable computational models to extract meaningful features of inter-region communication and principled theories to interpret those features. Here, we introduce Current-Based Decomposition ({CURBD}), an approach for inferring brain-wide interactions using data-constrained recurrent neural network models that directly reproduce experimentally-obtained neural data. {CURBD} leverages the functional interactions inferred by such models to reveal directional currents between multiple brain regions. We first show that {CURBD} accurately isolates inter-region currents in simulated networks with known dynamics. We then apply {CURBD} to multi-region neural recordings obtained from mice during running, macaques during Pavlovian conditioning, and humans during memory retrieval to demonstrate the widespread applicability of {CURBD} to untangle brain-wide interactions underlying behavior from a variety of neural datasets.},
	publisher = {{bioRxiv}},
	author = {Perich, Matthew G. and Arlt, Charlotte and Soares, Sofia and Young, Megan E. and Mosher, Clayton P. and Minxha, Juri and Carter, Eugene and Rutishauser, Ueli and Rudebeck, Peter H. and Harvey, Christopher D. and Rajan, Kanaka},
	urldate = {2022-11-06},
	date = {2021-03-11},
	langid = {english},
	note = {Pages: 2020.12.18.423348
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\JRWXEIV4\\Perich et al. - 2021 - Inferring brain-wide interactions using data-const.pdf:application/pdf;Snapshot:C\:\\Users\\gince\\Zotero\\storage\\NL7TS343\\2020.12.18.423348v2.html:text/html},
}


@misc{zhang_revisiting_2021,
	title = {Revisiting Recursive Least Squares for Training Deep Neural Networks},
	url = {http://arxiv.org/abs/2109.03220},
	abstract = {Recursive least squares ({RLS}) algorithms were once widely used for training small-scale neural networks, due to their fast convergence. However, previous {RLS} algorithms are unsuitable for training deep neural networks ({DNNs}), since they have high computational complexity and too many preconditions. In this paper, to overcome these drawbacks, we propose three novel {RLS} optimization algorithms for training feedforward neural networks, convolutional neural networks and recurrent neural networks (including long short-term memory networks), by using the error backpropagation and our average-approximation {RLS} method, together with the equivalent gradients of the linear least squares loss function with respect to the linear outputs of hidden layers. Compared with previous {RLS} optimization algorithms, our algorithms are simple and elegant. They can be viewed as an improved stochastic gradient descent ({SGD}) algorithm, which uses the inverse autocorrelation matrix of each layer as the adaptive learning rate. Their time and space complexities are only several times those of {SGD}. They only require the loss function to be the mean squared error and the activation function of the output layer to be invertible. In fact, our algorithms can be also used in combination with other first-order optimization algorithms without requiring these two preconditions. In addition, we present two improved methods for our algorithms. Finally, we demonstrate their effectiveness compared to the Adam algorithm on {MNIST}, {CIFAR}-10 and {IMDB} datasets, and investigate the influences of their hyperparameters experimentally.},
	number = {{arXiv}:2109.03220},
	publisher = {{arXiv}},
	author = {Zhang, Chunyuan and Song, Qi and Zhou, Hui and Ou, Yigui and Deng, Hongyao and Yang, Laurence Tianruo},
	urldate = {2022-11-06},
	date = {2021-09-07},
	eprinttype = {arxiv},
	eprint = {2109.03220 [cs]},
	keywords = {68T07, Computer Science - Machine Learning, K.3.2},
	annotation = {Comment: 12 pages,5 figures, {IEEE} Transactions on Neural Networks and Learning Systems under review},
	file = {arXiv Fulltext PDF:C\:\\Users\\gince\\Zotero\\storage\\3TTLBCD7\\Zhang et al. - 2021 - Revisiting Recursive Least Squares for Training De.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\gince\\Zotero\\storage\\BAPYNDG3\\2109.html:text/html},
}

@article{al-batah_modified_2010,
	title = {Modified Recursive Least Squares algorithm to train the Hybrid Multilayered Perceptron ({HMLP}) network},
	volume = {10},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494609000891},
	doi = {10.1016/j.asoc.2009.06.018},
	abstract = {In this paper, a new learning algorithm, called the Modified Recursive Least Square ({MRLS}), is introduced for the Hybrid Multilayered Perceptron ({HMLP}) network. Adopting the Recursive Least Square ({RLS}) algorithm as its basis, the {MRLS} algorithm differs from {RLS} in the way that the weight of the linear connections for the {HMLP} network is estimated. The convergence rate of the {MRLS} algorithm is further improved by varying the forgetting factor, optimizing the way the momentum and learning rate are assigned. To investigate its applicability, the {MRLS} algorithm is demonstrated on the {HMLP} network using six benchmark data sets obtained from the {UCI} repository. The classification performance of the {HMLP} network trained with the {MRLS} algorithm is compared with those of the {HMLP} network trained with the Modified Recursive Prediction Error ({MRPE}) algorithm and the {MLP} trained with the standard {RLS} algorithm as well as with other commonly adopted machine learning classifiers. The comparison results indicated that the proposed {MRLS} trained {HMLP} network provides significant improvement over {RLS} trained {MLP} network, {MRPE} trained {HMLP} network, and other machine learning classifiers in terms of accuracy, convergence rate and mean square error ({MSE}).},
	pages = {236--244},
	number = {1},
	journaltitle = {Applied Soft Computing},
	shortjournal = {Applied Soft Computing},
	author = {Al-Batah, Mohammad Subhi and Mat Isa, Nor Ashidi and Zamli, Kamal Zuhairi and Azizli, Khairun Azizi},
	urldate = {2022-10-18},
	date = {2010-01-01},
	langid = {english},
	keywords = {Artificial Neural Network, Hybrid Multilayered Perceptron, Modified Recursive Least Square, Multilayered Perceptron, Pattern recognition},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\gince\\Zotero\\storage\\CTQ6VNZX\\Al-Batah et al. - 2010 - Modified Recursive Least Squares algorithm to trai.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\gince\\Zotero\\storage\\SJ5I9P7U\\S1568494609000891.html:text/html},
}


