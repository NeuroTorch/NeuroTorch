{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wilson Cowan Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This tutorial was tested with the version `0.0.1-beta2` of NeuroTorch.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table class=\"nt-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://NeuroTorch.github.io/NeuroTorch/\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/neurotorch_logo_32px.png?raw=true\" width=32px height=32px  />Documentation</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/NeuroTorch/NeuroTorch/blob/main/tutorials/time_series_forecasting_wilson_cowan/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/colab_logo_32px.png?raw=true\" width=32px height=32px  />Run in Google Colab</a>\n",
    "</td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/tutorials/time_series_forecasting_wilson_cowan/tutorial.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32px height=32px />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/NeuroTorch/NeuroTorch/blob/main/tutorials/time_series_forecasting_wilson_cowan/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/download_logo_32px.png?raw=true\" width=32px height=32px />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quick setup for Google colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Install dependencies {display-mode: \"form\"}\n",
    "\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/NeuroTorch/NeuroTorch.git\n",
    "    %cd NeuroTorch/\n",
    "    !git checkout 0.0.1-beta2\n",
    "else:\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "!pip install git+https://github.com/NeuroTorch/NeuroTorch.git@0.0.1-beta2\n",
    "!pip install pythonbasictools\n",
    "!pip install unstable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, we will be learning how to use NeuroTorch to train a recurrent neural network that reproduce time series using the Wilson-Cowan dynamic.\n",
    "\n",
    "**Plan of this notebook**\n",
    "1. Brief introduction to the theory behind the Wilson-Cowan dynamic\n",
    "2. Setup\n",
    "3. How to create Dataset and dataloader\n",
    "4. Setting up our data\n",
    "5. How to run the training\n",
    "6. Tools to visualise the trained data\n",
    "7. Stacking multiple layers\n",
    "8. Callbacks\n",
    "9. Alternative way to enforce Dale's law\n",
    "\n",
    "---\n",
    "\n",
    "### 1 - Wilson-Cowan dynamic\n",
    "\n",
    "Introduce in 1972 by Wilson HR and Cowan JD, the Wilson-Cowan dynamic performs well when it comes to reproducing and predicting calcium neuronal activity. The equation used is the following :\n",
    "\n",
    "# <center> $\\tau \\dot{\\vec{x}} = -\\vec{x} + (1 - \\vec{r} \\odot \\vec{x}) \\odot \\sigma({\\mathbf{W}\\vec{x} - \\vec{\\mu}})$\n",
    "\n",
    "We can apply Euler's discretisation. By doing so, we obtain :\n",
    "\n",
    "# <center> $\\vec{x}_{(t+1)} = \\vec{x}_{(t)}\\left(1- \\frac{\\Delta t}{\\tau}\\right) + \\frac{\\Delta t}{\\tau}\\left(\\left(1 - \\vec{r}\\odot\\vec{x}_{(t)}\\right)\\odot\\sigma\\left(\\mathbf{W}\\vec{x}_{(t)} - \\vec{\\mu}\\right)\\right)$\n",
    "\n",
    "\n",
    "However, in NeuroTorch, we slightly modify this equation to allow to use of batches and to be coherent with format that what use in the rest of the module. In neuroscience, it is common to represent the neuronal activity with a matrix ($N$ x $T$) where $N$ is the number of neurons and $T$ is the number of time step. However, in NeuroTorch, we use the following convention : ($B$ x $T$ x $N$) where $B$ is the number of batch. In this notebook, our goal is to reproduce a specific time series, therefore, our batch size will be $B = 1$. We therefore apply a transpose on our weight matrix and inputs. With this in mind, the Wilson-Cowan becomes :\n",
    "\n",
    "# <center> $\\vec{x}_{(t+1)} = \\vec{x}_{(t)}\\left(1 - \\frac{\\Delta t}{\\tau}\\right) + \\frac{\\Delta t}{\\tau}\\left(\\left(1 - \\vec{r}\\odot\\vec{x}_{(t)}\\right)\\odot \\sigma\\left(\\vec{x}_{(t)}\\mathbf{W} - \\vec{\\mu}\\right)\\right)$\n",
    "\n",
    "By doing so, we obtain the same result but we are now using an equation that follow what have been done so far in NeuroTorch.\n",
    "\n",
    "Now, let us define the different parameters of this equation:\n",
    "\n",
    "- $\\vec{x}$ is the neuronal activity at a time $t$ (normalized between 0 and 1) of size (1 x $N$)\n",
    "- $\\Delta t$ is the time step of integration\n",
    "- $\\tau$ is the characteristic time. It represents the time needed for the neuronal activity to decay\n",
    "- $\\vec{r}$ is the refractory time. It represents the time needed for a neuron to fire again once she has already fired of size (1 x $N$)\n",
    "- $\\sigma$ is the sigmoid function\n",
    "- $W$ is the weight connexion (connectome) of size ($N$ x $N$). $W_{ij}$ tell us how neuron $i$ is connected to neuron $j$\n",
    "- $\\mu$ is the activation threshold of size (1 x $N$). It represents the input needed for a neuron to fire. A neuron with a small $\\mu$ will be sensitive to input and will easily fire.\n",
    "\n",
    "One may notice that hidden states were not discussed here. When working with one layer, this is not an issue. However, at the end of this tutorial, we will show how you can use multiple layers. Hidden states will then need to be discussed there.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 - Setup\n",
    "\n",
    "For those not familiar with Python and virtual environment, you can:\n",
    "\n",
    "- Follow those steps if you understand french : [venv and pycharm](https://github.com/JeremieGince/TutorielPython-Manuel/tree/master/Environments)\n",
    "\n",
    "- For english speaker : TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r time_series_forcasting_wilson_cowan/requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial, you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page. However, for this notebook, we recommend using CPU since we are not using batches.\n",
    "\n",
    "After setting up the virtual environment, we will need to import the necessary packages.\n",
    "\n",
    "These are the packages that will be needed for NeuroTorch:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import neurotorch as nt\n",
    "from neurotorch.visualisation.time_series_visualisation import VisualiseKMeans\n",
    "from neurotorch.regularization.connectome import DaleLawL2\n",
    "from neurotorch import WilsonCowanLayer\n",
    "from tutorials.util import GoogleDriveDownloader\n",
    "from copy import deepcopy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 3 - Dataset & Dataloader\n",
    "\n",
    "When working with PyTorch, it is important to define a dataset and a dataloader. Of course, you can always use the one that are provided in this module. However, you might be interested in knowing how those simple class are built. With NeuroTorch, multiple datasets and dataloaders are already built-in depending on what you need. Dataset are used to prepare your datas to be used in NeuroTorch. For instance, dataset are used to inform NeuroTorch on the transformation you want to apply to you data or the ratio of datas that will be used to training and to evaluate the quality of the training.\n",
    "\n",
    "1. First, you begin by creating the constructor\n",
    "2. You redefine the <code>\\_\\_len\\_\\_</code> operator. It returns the size of the dataset\n",
    "3. You redefine the <code>\\_\\_getitem__</code> operator. You want to be able to use indexing such as <code> dataset[i] </code> can be use to get the *i*th sample in your dataset. You return the value needed for training and testing.\n",
    "\n",
    "In our case, we have a dataset of size 1 (one sample) since we only use one time series. Also, when defining <code>\\_\\_getitem__</code>, we will return the initial condition and the rest of the timeseries since we only have one dataset. We unsqueeze the initiale condition so the tensor goes from size [$N$] to size [1 x $N$]. Remember that, in this dataset, your time series must have the size [$T$ x $N$]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WCDataset(Dataset):\n",
    "\tdef __init__(self, x):\n",
    "\t\tself.x = x\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn 1\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\treturn torch.unsqueeze(self.x[0], dim=0), self.x[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 4 - Setting up our data\n",
    "\n",
    "This section depend heavily on the way you have your data. For this tutorial, we will use a neuronal activity from a zebra fish. We will take 200 random neurons in our dataset. In this dataset, 406 time steps are present. We therefore need to import our data and select a sample size. From there, we want to applied a gaussian filter and set our array in a size that is compatible with NeuroTorch's convention. This part is left to the user since it heavily depend on the user's data. For this tutorial, we will prepare our data directly in our dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WSDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tGenerate a dataset of Wilson-Cowan time series.\n",
    "\tThis dataset is usefull to reproduce a time series using Wilson-Cowan layers.\n",
    "\t\"\"\"\n",
    "\tROOT_FOLDER = \"data/ts/\"\n",
    "\tFILE_ID_NAME = {\n",
    "\t\t\"SampleZebrafishData_PaulDeKoninckLab_2020-12-16.npy\": \"1-3jgAZiNU__NxxhXub7ezAJUqDMFpMCO\",\n",
    "\t}\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tfilename: Optional[str] = None,\n",
    "\t\t\tsample_size: int = 200,\n",
    "\t\t\tsmoothing_sigma: float = 10.0,\n",
    "\t\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\t\tdownload: bool = True,\n",
    "\t\t\t**kwargs\n",
    "\t):\n",
    "\t\t\"\"\"\n",
    "\t\t:param filename: filename of the dataset to load. If None, download the dataset from google drive.\n",
    "\t\t:param sample_size: number of neuron to use for training\n",
    "\t\t:param smoothing_sigma: sigma for the gaussian smoothing\n",
    "\t\t:param device: device to load the dataset on\n",
    "\t\t:param download: if True, download the dataset from google drive\n",
    "\t\t\"\"\"\n",
    "\t\tself.ROOT_FOLDER = kwargs.get(\"root_folder\", self.ROOT_FOLDER)\n",
    "\t\tif filename is None:\n",
    "\t\t\tfilename = list(self.FILE_ID_NAME.keys())[0]\n",
    "\t\t\tdownload = True\n",
    "\t\tpath = os.path.join(self.ROOT_FOLDER, filename)\n",
    "\t\tif download:\n",
    "\t\t\tassert filename in self.FILE_ID_NAME, \\\n",
    "\t\t\t\tf\"File {filename} not found in the list of available files: {list(self.FILE_ID_NAME.keys())}.\"\n",
    "\t\t\tGoogleDriveDownloader(self.FILE_ID_NAME[filename], path, skip_existing=True, verbose=False).download()\n",
    "\t\tts = np.load(path)\n",
    "\t\tn_neurons, n_shape = ts.shape\n",
    "\t\tsample = np.random.randint(n_neurons, size=sample_size)\n",
    "\t\tdata = ts[sample, :]\n",
    "\n",
    "\t\tfor neuron in range(data.shape[0]):\n",
    "\t\t\tdata[neuron, :] = gaussian_filter1d(data[neuron, :], sigma=smoothing_sigma)\n",
    "\t\t\tdata[neuron, :] = data[neuron, :] - np.min(data[neuron, :])\n",
    "\t\t\tdata[neuron, :] = data[neuron, :] / np.max(data[neuron, :])\n",
    "\t\tself.original_time_series = data\n",
    "\t\tself.x = torch.tensor(data.T, dtype=torch.float32, device=device)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\t__len__ is used to get the number of samples in the dataset. Since we are training on the entire time series,\n",
    "\t\twe only have one sample which is the entire time series hence the length is 1.\n",
    "\t\t\"\"\"\n",
    "\t\treturn 1\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\t\"\"\"\n",
    "\t\treturn the initial condition and the time series that will be use for training.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.unsqueeze(self.x[0], dim=0), self.x[1:]\n",
    "\n",
    "\t@property\n",
    "\tdef full_time_series(self):\n",
    "\t\treturn self.x[None, :, :]\n",
    "\n",
    "\t@property\n",
    "\tdef original_series(self):\n",
    "\t\treturn self.original_time_series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "### 5 - Training the data\n",
    "\n",
    "To reproduce a time series, you can follow those easy steps :\n",
    "\n",
    "1. Think about the parameters of your training. What kind of optimizer, learning rate, regularisation ...\n",
    "2. Define your function <code> train_with_params </code>\n",
    "3. Run the code!\n",
    "\n",
    "With NeuroTorch, you can easily use the optimizer, loss function, regularisation ... that you want! However, when it comes to reproducing time series, we have selected the one that performs the best. For this tutorial, we will use those.\n",
    "\n",
    "#### Defining <code> train_with_params </code>\n",
    "\n",
    "As you can see, until now, very few lines of codes were needed. <code> train_with_params </code> is the only function that must be definie by the user because this is where you tell NeuroTorch the parameters you wish to use. We will walk you through the process, so you can easily recreate this function or even modify to fit your needs. <code> train_with_params </code> takes 3 kind of entries :\n",
    "\n",
    "- Training related arguments\n",
    "- Wilson-Cowan dynamic arguments\n",
    "- device to compute\n",
    "\n",
    "Generally, you want your argument to be aspect of your training that might change from training to training. For instance, if you're planinng on always using the same learning rate, you might want to remove learning rate from <code> train_with_params </code> and directly add this parameters in the code rather than as an arguments. Of course, the opposite can also be done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def train_with_params(\n",
    "\t\tpath: str,\n",
    "\t\tsigma: float = 15.0,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tn_iterations: int = 100,\n",
    "\t\tforward_weights: Optional[torch.Tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1,\n",
    "\t\tdt: float = 1e-3,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu: Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\thh_init: str = \"inputs\",\n",
    "\t\tcheckpoint_folder=\"./checkpoints\",\n",
    "\t\tforce_dale_law: bool = True\n",
    "):\n",
    "\tpass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From there, we can build this function by following these steps :\n",
    "\n",
    "1. Initialise layer\n",
    "2. Initialise sequential\n",
    "3. Initialise trainer\n",
    "\n",
    "##### Step 1.\n",
    "\n",
    "When initialising the layer, you can specify the initial parameter of the Wilson-Cowan. If a parameter has not received an argument, it will be generated randomly. The forward weights is the main parameter that is trained. If you want, you can also train the parameter r, $\\mu$ and $\\tau$. Also, you might want to give an initial forward weight that follow some biological properties to accelerate the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_params(\n",
    "\t\tpath: str,\n",
    "\t\tsigma: float = 15.0,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tn_iterations: int = 100,\n",
    "\t\tforward_weights: Optional[torch.Tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1,\n",
    "\t\tdt: float = 1e-3,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu: Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\thh_init: str = \"inputs\",\n",
    "\t\tcheckpoint_folder=\"./checkpoints\",\n",
    "\t\tforce_dale_law: bool = True\n",
    "):\n",
    "\tdataset = WSDataset(path=path, sample_size=200, smoothing_sigma=sigma, device=device)\n",
    "\tx = dataset.full_time_series\n",
    "\tws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tforward_sign=0.5,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\thh_init=hh_init,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan_layer1\",\n",
    "\t\tforce_dale_law=force_dale_law\n",
    "\t).build()\n",
    "\tpass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Step 2.\n",
    "\n",
    "We will now add the sequential model. Here, you want to add the layer you have, the device on which you will compute and the foresight time step. Please note that the foresight time step is the number of time step that will be predicted from the initial condition. Since we are trying to reproduce the time series, we want as much time step as the data. Our foresight time step will therefore be the total time step number of our data minus one (to remove the initial condition). After that, you build the model so that the parameter that will be trained can be initialized. We now have :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_params(\n",
    "\t\tpath: str,\n",
    "\t\tsigma: float = 15.0,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tn_iterations: int = 100,\n",
    "\t\tforward_weights: Optional[torch.Tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1,\n",
    "\t\tdt: float = 1e-3,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu: Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\thh_init: str = \"inputs\",\n",
    "\t\tcheckpoint_folder=\"./checkpoints\",\n",
    "\t\tforce_dale_law: bool = True\n",
    "):\n",
    "\tdataset = WSDataset(path=path, sample_size=200, smoothing_sigma=sigma, device=device)\n",
    "\tx = dataset.full_time_series\n",
    "\tws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tforward_sign=0.5,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\thh_init=hh_init,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan_layer1\",\n",
    "\t\tforce_dale_law=force_dale_law\n",
    "\t).build()\n",
    "\n",
    "\tmodel = nt.SequentialRNN(\n",
    "\t\tlayers=[ws_layer],\n",
    "\t\tdevice=device,\n",
    "\t\tforesight_time_steps=x.shape[1] - 1,\n",
    "\t\tcheckpoint_folder=checkpoint_folder,\n",
    "\t)\n",
    "\tmodel.build()\n",
    "\tpass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Step 3.\n",
    "\n",
    "To add the trainer, we simply definie the optimizer for the parameter and the regularisation (if we wish to have regularisation). The following regularisation will be applied on the connectome :\n",
    "\n",
    "<center> $\\min_{W} = Tr(W^T(\\alpha * W - (1 - \\alpha)W_{EI}))$ </center>\n",
    "\n",
    "The parameter $\\alpha$ is a parameter between 0 and 1 that define whether the regularisation is the dale law ($\\alpha = 0$), a reduction of the connectome ($\\alpha = 1$) or a combination of both. Also, $W_{EI}$ is a reference matrix that must follow dale's law and will guide the optimizer. Please note that the regularisation of the connectome has a separate optimizer. The loss function that is used here is the proportion of variance :\n",
    "<center> $pVar = 1 - \\frac{MSE(x,y)}{var{(x)}}$ </center>\n",
    "\n",
    "This loss function is define between $-\\infty$ and $1$. We want to maximise it.\n",
    "\n",
    "To use the Dale's law L2 regularisation, a reference connectome that is already following Dale's law needs to be applied. Also, further down the road, it will be interesting to begin the training with a random matrix that follows Dale's law rather than \"just\" a random matrix. To do so, you can use the <code> init </code> option. With this, a connectome can be easily initialize. To do, <code> init.dale_ </code> needs a sample connectome with the correct size, the ratio between inhibitory and excitation neurons. Finally, the density of connection can be given."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nt.init.dale_(torch.zeros(200, 200), inh_ratio=0.5, rho=0.99)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once this step is complete. We will have a well functioning training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_params(\n",
    "\t\tsigma: float = 15.0,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tn_iterations: int = 100,\n",
    "\t\tforward_weights: Optional[torch.Tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1,\n",
    "\t\tdt: float = 1e-3,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu: Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\thh_init: str = \"inputs\",\n",
    "\t\tcheckpoint_folder=\"./checkpoints\",\n",
    "\t\tforce_dale_law: bool = True\n",
    "):\n",
    "\t# STEP 1\n",
    "\tdataset = WSDataset(sample_size=200, smoothing_sigma=sigma, device=device)\n",
    "\tx = dataset.full_time_series\n",
    "\tws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tforward_sign=0.5,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\thh_init=hh_init,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan_layer1\",\n",
    "\t\tforce_dale_law=force_dale_law\n",
    "\t).build()\n",
    "\n",
    "\t# STEP 2\n",
    "\tmodel = nt.SequentialRNN(\n",
    "\t\tlayers=[ws_layer],\n",
    "\t\tdevice=device,\n",
    "\t\tforesight_time_steps=x.shape[1] - 1,\n",
    "\t\tcheckpoint_folder=checkpoint_folder,\n",
    "\t)\n",
    "\tmodel.build()\n",
    "\n",
    "\t# REGULARISATION ON THE CONNECTOME -> Not mandatory\n",
    "\tregularisation = DaleLawL2([ws_layer.forward_weights], alpha=0.5,\n",
    "\t\t\t\t\t\t\t   reference_weights=[nt.init.dale_(torch.zeros(200, 200), inh_ratio=0.5, rho=0.99)])\n",
    "\n",
    "\t# OPTIMIZER FOR GRADIENT DESCENT -> Adam, AdamW, SGD\n",
    "\toptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, maximize=True, weight_decay=0.1)  # Weight decay can be applied to reduce the energy if connectome\n",
    "\n",
    "\t# OPTIMIZER FOR REGULARISATION -> Only needed if regularisation\n",
    "\toptimizer_regul = torch.optim.SGD(regularisation.parameters(), lr=5e-4)\n",
    "\n",
    "\t#STEP 3\n",
    "\ttrainer = nt.trainers.RegressionTrainer(\n",
    "\t\tmodel,\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tregularization_optimizer=optimizer_regul,\n",
    "\t\tcriterion=nt.losses.PVarianceLoss(),\n",
    "\t\tregularisation=regularisation,\n",
    "\t\tmetrics=[regularisation]\n",
    "\t)\n",
    "\ttrainer.train(\n",
    "\t\tDataLoader(dataset, shuffle=False, num_workers=0),\n",
    "\t\tn_iterations=n_iterations,\n",
    "\t\texec_metrics_on_train=True\n",
    "\t)\n",
    "\tx_pred = torch.concat([\n",
    "\t\ttorch.unsqueeze(x[:, 0].clone(), dim=1).to(model.device),\n",
    "\t\tmodel.get_prediction_trace(torch.unsqueeze(x[:, 0].clone(), dim=1))\n",
    "\t], dim=1)\n",
    "\tout = {\n",
    "\t\t\"W\": ws_layer.forward_weights.detach().cpu().numpy(),\n",
    "\t\t\"mu\": ws_layer.mu.detach().numpy(),\n",
    "\t\t\"r\": ws_layer.r.detach().numpy(),\n",
    "\t\t\"tau\": ws_layer.tau.detach().numpy(),\n",
    "\t\t\"x_pred\": torch.squeeze(x_pred).detach().numpy().T,\n",
    "\t\t\"original_time_series\": dataset.original_series,\n",
    "\t}\n",
    "\treturn out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is now time to call the function and train our data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_forward_weight = nt.init.dale_(torch.zeros(200, 200), inh_ratio=0.5, rho=0.2)\n",
    "\n",
    "result = train_with_params(\n",
    "\tsigma=25,\n",
    "\tlearning_rate=1e-2,\n",
    "\tn_iterations=500,\n",
    "\tforward_weights=initial_forward_weight,\n",
    "\tstd_weights=1,\n",
    "\tdt=0.02,\n",
    "\tmu=0.0,\n",
    "\tmean_mu=0,\n",
    "\tstd_mu=1,\n",
    "\tr=0.1,\n",
    "\tmean_r=0.2,\n",
    "\tstd_r=0,\n",
    "\ttau=0.1,\n",
    "\tlearn_mu=True,\n",
    "\tlearn_r=True,\n",
    "\tlearn_tau=True,\n",
    "\tdevice=torch.device(\"cpu\"),\n",
    "\thh_init=\"inputs\",\n",
    "\tforce_dale_law=False\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 6 - Visualise data\n",
    "\n",
    "Now, we might want to extract the trained parameters such as the connectome to visualise them. We will slightly modify our function train_with params to extract the trained value. We will use the class Visualise that allows us to plot our data in numerous way such as heatmap or ridgeplot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_viz = nt.Visualise(\n",
    "\tresult[\"x_pred\"].T,\n",
    "\tshape=nt.Size(\n",
    "\t\t[\n",
    "\t\t\tnt.Dimension(None, nt.DimensionProperty.TIME, \"Time [s]\"),\n",
    "\t\t\tnt.Dimension(None, nt.DimensionProperty.NONE, \"Activity [-]\"),\n",
    "\t\t]\n",
    "\t)\n",
    ")\n",
    "\n",
    "pred_viz.plot_timeseries_comparison_report(\n",
    "\tresult[\"original_time_series\"].T,\n",
    "\ttitle=f\"Prediction\",\n",
    "\tshow=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VisualiseKMeans(result[\"x_pred\"], nt.Size([nt.Dimension(200, nt.DimensionProperty.NONE, \"Neuron [-]\"), nt.Dimension(406, nt.DimensionProperty.TIME, \"time [s]\")])).heatmap(show=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(result[\"W\"].T, cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 7 - Stacking multiple layers\n",
    "\n",
    "If the goal of the training is to extract a realistic connectome, then it is important to use one layer as shown previously. However, we can stack multiple layers to reproduce/predict the time step. Hence, if the goal is to reproduce and/or predict data without extracting a connectome, then adding layers might be a good idea since it is possible to get more abstract representation of the data. Therefore, it is possible to better approximate complex functions that might explain the data.\n",
    "\n",
    "With **NeuroTorch**, it is rather easy to add *N* supplementary layers. To add more layers :\n",
    "\n",
    "1- Chose how the hidden states are initialized.\n",
    "2- Create *N* objects <code>WilsonCowanLayer</code> (If a deepcopy is made, you must change the name of the new one!)\n",
    "3- Apply regularisation on the different connectome (if desired)\n",
    "\n",
    "#### Step 1 : hidden states\n",
    "\n",
    "When dealing with multiple layers, you must use hidden states to ensure that your Euler integration is done properly. In section 1, we have shown the equation that was implemented in NeuroTorch. However, the equation that is *actually* implemented is slightly different to ensure that hidden states and multi-layers training is working correctly. The *actual* equation is :\n",
    "\n",
    "# <center> $\\vec{x}_{(t+1)} = \\vec{h}_{(t)}\\left(1 - \\frac{\\Delta t}{\\tau}\\right) + \\frac{\\Delta t}{\\tau}\\left(\\left(1 - \\vec{r}\\odot\\vec{h}_{(t)}\\right)\\odot \\sigma\\left(\\vec{h}_{(t)}\\mathbf{W}^{\\text{rec}}+\\vec{x}_{(t)}\\mathbf{W} - \\vec{\\mu}\\right)\\right)$\n",
    "\n",
    "The new terms can be defined in the following way :\n",
    "\n",
    "- $\\mathbf{W}^{\\text{rec}}$ is the recurrent matrix of size (output x output)\n",
    "- $\\vec{h}_{(t)}$ is the hidden state at a time $t$\n",
    "\n",
    "With **NeuroTorch**, you can initialise the hidden states with zeros, with random numbers or with the actual inputs. In facts, when working with one layer, the hidden states are initialise with the inputs. Therefore, $\\vec{h}_{(t)} = \\vec{x}_{(t)}$ and $\\mathbf{W}^{\\text{rec}}$ is a matrix fill with zeros. When working with multiple layers, the output of layer is the input of the next one and so on. In this tutorial, we will initialise the hidden states with the inputs. We pass it as an argument to each Wilson-Cowan layers.\n",
    "\n",
    "#### Step 2 : Create *N* layers\n",
    "\n",
    "You can stack up layers by passing them in the list in the Sequential model. In this tutorial, we use two layers\n",
    "\n",
    "#### Step 3 : Apply regularisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can apply the regularisation by applying the forward weight of the new model to the regularisation\n",
    "\n",
    "When combining those 3 steps, the code becomes :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_params(\n",
    "\t\tsigma: float = 15.0,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tn_iterations: int = 100,\n",
    "\t\tforward_weights: Optional[torch.Tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1,\n",
    "\t\tdt: float = 1e-3,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu: Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\"),\n",
    "\t\thh_init: str = \"inputs\",\n",
    "\t\tcheckpoint_folder=\"./checkpoints\",\n",
    "\t\tforce_dale_law: bool = True\n",
    "):\n",
    "\tdataset = WSDataset(sample_size=200, smoothing_sigma=sigma, device=device)\n",
    "\tx = dataset.full_time_series\n",
    "\tws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tforward_sign=0.5,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\thh_init=hh_init,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan_layer1\",\n",
    "\t\tforce_dale_law=force_dale_law\n",
    "\t).build()\n",
    "\n",
    "\tws_layer_2 = deepcopy(ws_layer)\n",
    "\tws_layer_2.name = \"WilsonCowan_layer2\"\n",
    "\n",
    "\tmodel = nt.SequentialRNN(\n",
    "\t\tlayers=[ws_layer, ws_layer_2],\n",
    "\t\tdevice=device,\n",
    "\t\tforesight_time_steps=x.shape[1] - 1,\n",
    "\t\tcheckpoint_folder=checkpoint_folder,\n",
    "\t)\n",
    "\tmodel.build()\n",
    "\n",
    "\tregularisation = DaleLawL2([ws_layer.forward_weights, ws_layer_2.forward_weights], alpha=0.5,\n",
    "\t\t\t\t\t\t\t   reference_weights=[nt.init.dale_(torch.zeros(200, 200), inh_ratio=0.5, rho=0.99)])\n",
    "\toptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, maximize=True, weight_decay=0.1)  # Weight decay can be applied to reduce the energy if connectome\n",
    "\n",
    "\toptimizer_regul = torch.optim.SGD(regularisation.parameters(), lr=5e-4)\n",
    "\ttrainer = nt.trainers.RegressionTrainer(\n",
    "\t\tmodel,\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tregularization_optimizer=optimizer_regul,\n",
    "\t\tcriterion=nt.losses.PVarianceLoss(),\n",
    "\t\tregularisation=regularisation,\n",
    "\t\tmetrics=[regularisation]\n",
    "\t)\n",
    "\ttrainer.train(\n",
    "\t\tDataLoader(dataset, shuffle=False, num_workers=0),\n",
    "\t\tn_iterations=n_iterations,\n",
    "\t\texec_metrics_on_train=True\n",
    "\t)\n",
    "\tx_pred = torch.concat([\n",
    "\t\ttorch.unsqueeze(x[:, 0].clone(), dim=1).to(model.device),\n",
    "\t\tmodel.get_prediction_trace(torch.unsqueeze(x[:, 0].clone(), dim=1))\n",
    "\t], dim=1)\n",
    "\tout = {\n",
    "\t\t\"W\": ws_layer.forward_weights.detach().cpu().numpy(),\n",
    "\t\t\"mu\": ws_layer.mu.detach().numpy(),\n",
    "\t\t\"r\": ws_layer.r.detach().numpy(),\n",
    "\t\t\"tau\": ws_layer.tau.detach().numpy(),\n",
    "\t\t\"x_pred\": torch.squeeze(x_pred).detach().numpy().T,\n",
    "\t\t\"original_time_series\": dataset.original_series,\n",
    "\t}\n",
    "\treturn out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_forward_weight = nt.init.dale_(torch.zeros(200, 200), inh_ratio=0.5, rho=0.2)\n",
    "\n",
    "result = train_with_params(\n",
    "\tsigma=25,\n",
    "\tlearning_rate=1e-2,\n",
    "\tn_iterations=500,\n",
    "\tforward_weights=initial_forward_weight,\n",
    "\tstd_weights=1,\n",
    "\tdt=0.02,\n",
    "\tmu=0.0,\n",
    "\tmean_mu=0,\n",
    "\tstd_mu=1,\n",
    "\tr=0.1,\n",
    "\tmean_r=0.2,\n",
    "\tstd_r=0,\n",
    "\ttau=0.1,\n",
    "\tlearn_mu=True,\n",
    "\tlearn_r=True,\n",
    "\tlearn_tau=True,\n",
    "\tdevice=torch.device(\"cpu\"),\n",
    "\thh_init=\"inputs\",\n",
    "\tforce_dale_law=False\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_viz = nt.Visualise(\n",
    "\tresult[\"x_pred\"].T,\n",
    "\tshape=nt.Size(\n",
    "\t\t[\n",
    "\t\t\tnt.Dimension(None, nt.DimensionProperty.TIME, \"Time [s]\"),\n",
    "\t\t\tnt.Dimension(None, nt.DimensionProperty.NONE, \"Activity [-]\"),\n",
    "\t\t]\n",
    "\t)\n",
    ")\n",
    "\n",
    "pred_viz.plot_timeseries_comparison_report(\n",
    "\tresult[\"original_time_series\"].T,\n",
    "\ttitle=f\"Prediction\",\n",
    "\tshow=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 8 - Callbacks\n",
    "\n",
    "Callbacks are useful to improve the training. For instance, here's two useful callbacks.\n",
    "\n",
    "##### Save the best training : checkpoint manager\n",
    "\n",
    "This first callback allows you to save the best training. To maximize the speed of training, you can specify that this callback only applied for the iterations near the end. For instance :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\tcheckpoint_manager = nt.CheckpointManager(\n",
    "\t\tcheckpoint_folder,\n",
    "\t\tmetric=\"train_loss\",\n",
    "\t\tminimise_metric=False,\n",
    "\t\tsave_freq=-1,\n",
    "\t\tsave_best_only=True,\n",
    "\t\tstart_save_at=int(0.98 * n_iterations),\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we specify that we want to apply this callback to the train loss (the pVar) that is being maximised. Also, with <code>save_freq=-1</code> we specify that we want to save the last iteration. Also, we start saving after 98% of the iterations are done.\n",
    "\n",
    "##### Learning rate scheduling\n",
    "\n",
    "This callback is to reduce the learning rate once we arrive close to a certain loss. The pVar tends to be unstable near 0.95 especially with a big learning rate. Hence, it is usefull to be able to reduce gradually the learning rate once we reach a high performance since the remaining adjustment are minimal in order to achieve a better loss. This callback can be combined with the previous one by adding them in a list. This can be done in the following way:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\tcallbacks = [\n",
    "\t\tLRSchedulerOnMetric(\n",
    "\t\t\t'train_loss',\n",
    "\t\t\tmetric_schedule=np.linspace(0.97, 1.0, 100),\n",
    "\t\t\tmin_lr=learning_rate/10,\n",
    "\t\t\tretain_progress=True,\n",
    "\t\t),\n",
    "\t\tcheckpoint_manager,\n",
    "\t\tconvergence_time_getter,\n",
    "\t\tEarlyStoppingThreshold(metric='train_loss', threshold=0.999, minimize_metric=False),\n",
    "\t]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We schedule the train loss to reduce once it is between 0.97 and 1. We reduce the learning rate to a minimum of <code> learning_rate/10 </code>. Also, you can apply an early stopping to automatically stop the training once the loss is at a certain value. Finally, <code> convergence_time_getter </code> allows you to know the optimal time needed to obtain a certain loss.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 9 - Alternative way to enforce Dale's law\n",
    "\n",
    "\n",
    "Previously, we have shown that we can use a regularisation on the connectome to enforce Dale's law. This method is great if you're looking for a specific ratio of excitatory and inhibatory neurons and if computation time is a concern. However, one of the issue with this method, is that the ratio has to be set manually. This alternative way to enforce Dale's law allows you to let the AI \"choose\" a ratio that optimise best the loss function. In NeuroTorch, this alternative method is applied in the following way:\n",
    "\n",
    "<center> $\\mathbf{W} = \\mathbf{W_{\\text{weight}}} \\odot \\tanh({\\vec{w}})$ </center>\n",
    "    \n",
    "\n",
    "Where :\n",
    "- $\\mathbf{W}$ is the full connectome\n",
    "- $\\mathbf{W_{\\text{weight}}}$ is the weight of the connexion in absolute value. The sign of those connexion is not consider in this matrix\n",
    "- $\\vec{w}$ Is a vector that contain the information about the sign of each neuron. A $\\tanh$ is applied so that this vector is define between -1 and 1. A $\\tanh$ is also use because, as a continuous function, it is easier to train since vanishing gradient is less of a problem. \n",
    "\n",
    "To use this method, one simply needs to add an argument in the <code>kwargs</code> of the layer. For instance, here's an example :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tforward_sign=0.5,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\thh_init=hh_init,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan_layer1\",\n",
    "\t\tforce_dale_law=True\n",
    "\t).build()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<code> forward_sign </code> can be a sign vector or a float (if a specified initial ratio is desired).\n",
    "\n",
    "**To test all of those features, please use :** <code> main.py </code>! Enjoy!\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
