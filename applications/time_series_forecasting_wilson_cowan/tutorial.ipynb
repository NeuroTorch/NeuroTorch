{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wilson Cowan Tutorial\n",
    "\n",
    "In this tutorial, we will be learning how to use NeuroTorch to train a recurrent neural network that reproduce time series using the Wilson-Cowan dynamic.\n",
    "\n",
    "**Plan of this notebook**\n",
    "1. Brief introduction to the theory behind the Wilson-Cowan dynamic\n",
    "2. Setup\n",
    "3. How to create Dataset and dataloader\n",
    "4. Setting up our data\n",
    "5. How to run the training\n",
    "6. Tools to visualise the trained data\n",
    "\n",
    "---\n",
    "\n",
    "### Wilson-Cowan dynamic\n",
    "\n",
    "Introduce in 1972 by Wilson HR and Cowan JD, the Wilson-Cowan dynamic performs well when it comes to reproducing and predicting calcium neuronal activity. The equation used is the following :\n",
    "\n",
    "# <center> $\\tau \\dot{\\vec{x}} = -\\vec{x} + (1 - \\vec{r} \\odot \\vec{x}) \\odot \\sigma({\\mathbf{W}\\vec{x} - \\vec{\\mu}})$\n",
    "\n",
    "We can apply Euler's discretisation. By doing so, we obtain :\n",
    "\n",
    "# <center> $\\vec{x}_{(t+1)} = \\vec{x}_{(t)}\\left(1- \\frac{\\Delta t}{\\tau}\\right) + \\frac{\\Delta t}{\\tau}\\left(\\left(1 - \\vec{r}\\odot\\vec{x}_{(t)}\\right)\\odot\\sigma\\left(\\mathbf{W}\\vec{x} - \\vec{\\mu}\\right)\\right)$\n",
    "\n",
    "\n",
    "However, in NeuroTorch, we slightly modify this equation to allow to use of batches and to be coherent with format that what use in the rest of the module. In neuroscience, it is common to represent the neuronal activity with a matrix ($N$ x $T$) where $N$ is the number of neurons and $T$ is the number of time step. However, in NeuroTorch, we use the following convention : ($B$ x $T$ x $N$) where $B$ is the number of batch. In this notebook, our goal is to reproduce a specific time series, therefore, our batch size will be $B = 1$. We therefore apply a transpose on our weight matrix and inputs. With this in mind, the Wilson-Cowan becomes :\n",
    "\n",
    "# <center> $\\vec{x}_{(t+1)} = \\vec{x}_{(t)}\\left(1 - \\frac{\\Delta t}{\\tau}\\right) + \\frac{\\Delta t}{\\tau}\\left(\\left(1 - \\vec{r}\\odot\\vec{x}_{(t)}\\right)\\odot \\sigma\\left(\\vec{x}\\mathbf{W} - \\mu\\right)\\right)$\n",
    "\n",
    "By doing so, we obtain the same result but we are now using an equation that follow what have been done so far in NeuroTorch.\n",
    "\n",
    "Now, let us define the different parameters of this equation:\n",
    "\n",
    "- $\\vec{x}$ is the neuronal activity at a time $t$ (normalized between 0 and 1) of size (1 x $N$)\n",
    "- $\\Delta t$ is the time step of integration\n",
    "- $\\tau$ is the characteristic time. It represents the time needed for the neuronal activity to decay\n",
    "- $\\vec{r}$ is the refractory time. It represents the time needed for a neuron to fire again once she has already fired of size (1 x $N$)\n",
    "- $\\sigma$ is the sigmoid function\n",
    "- $W$ is the weight connexion (connectome) of size ($N$ x $N$). $W_{ij}$ tell us how neuron $i$ is connected to neuron $j$\n",
    "- $\\mu$ is the activation threshold of size (1 x $N$). It represents the input needed for a neuron to fire. A neuron with a small $\\mu$ will be sensitive to input and will easily fire.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup\n",
    "\n",
    "For those not familiar with Python and virtual environment, you can:\n",
    "\n",
    "- Follow those steps if you understand french : [venv and pycharm](https://github.com/JeremieGince/TutorielPython-Manuel/tree/master/Environments)\n",
    "\n",
    "- For english speaker : TODO\n",
    "\n",
    "You can now install the dependencies by running the following commands :\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r time_series_forcasting_wilson_cowan_requirements.txt\n",
    "!pip install git+https://github.com/JeremieGince/NeuroTorch.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial, you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page. However, for this notebook, we recommend using CPU since we are not using batches.\n",
    "\n",
    "After setting up the virtual environment, we will need to import the necessary packages.\n",
    "\n",
    "These are the packages that will be needed for NeuroTorch:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import neurotorch as nt\n",
    "from neurotorch.metrics import RegressionMetrics\n",
    "from neurotorch.visualisation.time_series_visualisation import Visualise, VisualiseKMeans\n",
    "from neurotorch.regularization.connectome import DaleLawL2\n",
    "from neurotorch import WilsonCowanLayer\n",
    "from src.neurotorch.transforms.base import to_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Dataset & Dataloader\n",
    "\n",
    "When working with PyTorch, it is important to define a dataset and a dataloader. Of course, you can always use the one that are provided in this module. However, you might be interested in knowing how those simple class are built. With NeuroTorch, multiple datasets and dataloader are already built-in depending on what you need. Dataset are used to prepare your datas to be used in NeuroTorch. For instance, dataset are used to inform NeuroTorch on the transformation you want to apply to you data or the ratio of datas that will be used to training and to evaluate the quality of the training.\n",
    "\n",
    "1. First, you begin by creating the constructor\n",
    "2. You redefine the <code>\\_\\_len\\_\\_</code> operator. It returns the size of the dataset\n",
    "3. You redefine the <code>\\_\\_getitem__</code> operator. You want to be able to use indexing such as <code> dataset[i] </code> can be use to get the *i*th sample in your dataset. You return the value needed for training and testing.\n",
    "\n",
    "In our case, we have a dataset of size 1 (one sample) since we only use one time series. Also, when defining <code>\\_\\_getitem__</code>, we will return the initial condition and the rest of the timeseries since we only have one dataset. We unsqueeze the initiale condition so the tensor goes from size [$N$] to size [1 x $N$]. Remember that, in this dataset, your time series must have the size [$T$ x $N$]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WCDataset(Dataset):\n",
    "\tdef __init__(self, x):\n",
    "\t\tself.x = x\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn 1\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\treturn torch.unsqueeze(self.x[0], dim=0), self.x[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the dataloader, we can simply use the dataloader built in PyTorch. Sometime, it is interesting to build our own. If this case applies to you, feel free look at a few dataloaders and datasets that were built for WilsonCowan in application/time_series_forecasting_wilson_cowan/dataset\n",
    "\n",
    "---\n",
    "### Setting up our data\n",
    "\n",
    "This section depend heavily on the way you have your data. For this tutorial, we will use a neuronal activity from a zebra fish. We will take 200 random neurons in our dataset. In this dataset, 406 time steps are present. We begin by importing the data :\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time_series = np.load('timeSeries_2020_12_16_cr3_df.npy')\n",
    "n_neurons, n_shape = time_series.shape\n",
    "sample_size = 200\n",
    "sample = np.random.randint(n_neurons, size=sample_size)\n",
    "data = time_series[sample, :]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To improve the training, one might also want to smooth the data :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "sigma = 20\n",
    "\n",
    "for neuron in range(data.shape[0]):\n",
    "\tdata[neuron, :] = gaussian_filter1d(data[neuron, :], sigma=sigma)\n",
    "\tdata[neuron, :] = data[neuron, :] - np.min(data[neuron, :])\n",
    "\tdata[neuron, :] = data[neuron, :] / np.max(data[neuron, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Training the data\n",
    "\n",
    "To reproduce a time series, you can follow those easy steps :\n",
    "\n",
    "1. Think about the parameters of your training. What kind of optimizer, learning rate, regularisation ...\n",
    "2. Define your function <code> train_with_params </code>\n",
    "3. Run the code!\n",
    "\n",
    "With NeuroTorch, you can easily use the optimizer, loss function, regularisation ... that you want! However, when it comes to reproducing time series, we have selected the one that performs the best. For this tutorial, we will use those.\n",
    "\n",
    "#### Defining <code> train_with_params </code>\n",
    "\n",
    "As you can see, until now, very few lines of codes were needed. <code> train_with_params </code> is the only function that must be definie by the user because this is where you tell NeuroTorch the parameters you wish to use. We will walk you through the process, so you can easily recreate this function or even modify to fit your needs. <code> train_with_params </code> takes 3 kind of entries :\n",
    "\n",
    "- Training related arguments\n",
    "- Wilson-Cowan dynamic arguments\n",
    "- device to compute\n",
    "\n",
    "Generally, you want your argument to be aspect of your training that might change from training to training. For instance, if you're planinng on always using the same learning rate, you might want to remove learning rate from <code> train_with_params </code> and directly add this parameters in the code rather than as an arguments. Of course, the opposite can also be done."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "def train_with_params(\n",
    "\t\ttrue_time_series: np.ndarray or torch.Tensor,\n",
    "\t\tlearning_rate: float = 1e-2,\n",
    "\t\tepochs: int = 100,\n",
    "\t\tforward_weights: Optional[torch.tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1.0,\n",
    "\t\tdt: float = 0.02,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu : Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\")\n",
    "):\n",
    "\tpass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From there, we can build this function by following these steps :\n",
    "\n",
    "1. Initialise layer\n",
    "2. Initialise sequential\n",
    "3. Initialise trainer\n",
    "\n",
    "##### Step 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_params(\n",
    "\t\ttrue_time_series: np.ndarray or torch.Tensor,\n",
    "\t\tlearning_rate: float = 1e-2,\n",
    "\t\tepochs: int = 100,\n",
    "\t\tforward_weights: Optional[torch.tensor or np.ndarray] = None,\n",
    "\t\tstd_weights: float = 1.0,\n",
    "\t\tdt: float = 0.02,\n",
    "\t\tmu: Optional[float or torch.Tensor or np.ndarray] = 0.0,\n",
    "\t\tmean_mu : Optional[float] = 0.0,\n",
    "\t\tstd_mu: Optional[float] = 1.0,\n",
    "\t\tr: Optional[float or torch.Tensor or np.ndarray] = 1.0,\n",
    "\t\tmean_r: Optional[float] = 1.0,\n",
    "\t\tstd_r: Optional[float] = 1.0,\n",
    "\t\ttau: float = 1.0,\n",
    "\t\tlearn_mu: bool = False,\n",
    "\t\tlearn_r: bool = False,\n",
    "\t\tlearn_tau: bool = False,\n",
    "\t\tdevice: torch.device = torch.device(\"cpu\")\n",
    "):\n",
    "\tif not torch.is_tensor(true_time_series):  # convert time series to torch tensor\n",
    "\t\ttrue_time_series = torch.tensor(true_time_series, dtype=torch.float32, device=device)\n",
    "\tx = true_time_series.T[np.newaxis, :]  # ensure that time series has format (1 x T x N)\n",
    "\tws_layer = WilsonCowanLayer(\n",
    "\t\tx.shape[-1], x.shape[-1],\n",
    "\t\tforward_weights=forward_weights,\n",
    "\t\tstd_weights=std_weights,\n",
    "\t\tdt=dt,\n",
    "\t\tr=r,\n",
    "\t\tmean_r=mean_r,\n",
    "\t\tstd_r=std_r,\n",
    "\t\tmu=mu,\n",
    "\t\tmean_mu=mean_mu,\n",
    "\t\tstd_mu=std_mu,\n",
    "\t\ttau=tau,\n",
    "\t\tlearn_r=learn_r,\n",
    "\t\tlearn_mu=learn_mu,\n",
    "\t\tlearn_tau=learn_tau,\n",
    "\t\tdevice=device,\n",
    "\t\tname=\"WilsonCowan\"\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}