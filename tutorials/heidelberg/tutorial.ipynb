{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Heidelberg Tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train a neural network to recognize audio recordings of spoken digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can install the dependencies by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r heidelberg_requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "from dataset import get_dataloaders\n",
    "import neurotorch as nt\n",
    "from neurotorch import Dimension, DimensionProperty\n",
    "from neurotorch.modules import HeavisideSigmoidApprox\n",
    "from neurotorch.callbacks import CheckpointManager, LoadCheckpointMode\n",
    "from neurotorch.metrics import ClassificationMetrics\n",
    "from neurotorch.modules import SequentialRNN\n",
    "from neurotorch.modules.layers import SpyLIFLayer, SpyLILayer\n",
    "from neurotorch.trainers import ClassificationTrainer\n",
    "from neurotorch.regularization import RegularizationList, L2, L1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs_file_setup(\"heidelberg_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's the time to defined our training and networks parameters. In the following,\n",
    "\n",
    "- the number of iteration is the number of time the trainer will pass through the entire training dataset;\n",
    "- the batch size is the number of samples that will be loaded at the same time for each forward and backward pass;\n",
    "- the learning rate is the learning rate used by the optimizer;\n",
    "- the random seed is the seed used by the random number generator;\n",
    "- the number of steps is the number of euler integration steps performed by the model during each forward and backward pass;\n",
    "- the number of hidden neurons is the number of neurons that will be used in the hidden layer of the model;\n",
    "- dt is the time step of the euler integration;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_iterations = 50\n",
    "batch_size = 256\n",
    "learning_rate = 2e-4\n",
    "seed = 42\n",
    "\n",
    "# Network parameters\n",
    "dynamic_type = nt.SpyLIFLayer\n",
    "n_steps = 100\n",
    "n_hidden_neurons = 256\n",
    "dt = 1e-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_folder = f\"./checkpoints/heidelberg_{dynamic_type.__name__}\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_folder, metric=\"val_accuracy\", minimise_metric=False, save_best_only=True, start_save_at=int(n_iterations * 0.9))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Here is an exemple of the Heidelberg dataset:\n",
    "![Heidelberg_exemples](images/heidelberg_exemples.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(\n",
    "\tbatch_size=batch_size,\n",
    "\ttrain_val_split_ratio=0.95,\n",
    "\tpin_memory=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can finally define our model. The sequential model is a neural network that is composed of a list of layers. The list of layers is ordered and the first layer is the input layer. The last layer is the output layer. The sequential model can be found in the subpackage `neurotorch.modules`. Note that we can specify the input transform that will be applied to the input data before the forward pass through the list of layers. If the transformation is computationally expensive, it is recommended to put it in the dataloaders instead of the model for the training. Usually the input transform in the model is used when the model is used for inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = SequentialRNN(\n",
    "\tlayers=[\n",
    "\t\tdynamic_type(\n",
    "\t\t\tinput_size=nt.Size([\n",
    "\t\t\t\tDimension(None, DimensionProperty.TIME),\n",
    "\t\t\t\tDimension(dataloaders[\"test\"].dataset.n_units, DimensionProperty.NONE)\n",
    "\t\t\t]),\n",
    "\t\t\toutput_size=n_hidden_neurons,\n",
    "\t\t\tuse_recurrent_connection=True,\n",
    "\t\t\tspike_func=HeavisideSigmoidApprox,\n",
    "\t\t\tdt=dt,\n",
    "\t\t),\n",
    "\t\tSpyLILayer(dt=dt, output_size=dataloaders[\"test\"].dataset.n_classes),\n",
    "\t],\n",
    "\tname=f\"heidelberg_network\",\n",
    "\thh_memory_size=1,\n",
    "\tcheckpoint_folder=checkpoint_folder,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the instantiation of the model, we have to build it, so it can infer the missing sizes of the layers in the model and create the tensors that will be used during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.build()\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regularization = RegularizationList([\n",
    "\tL2(network.parameters(), Lambda=1e-5),\n",
    "\tL1(network.parameters(), Lambda=1e-6),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we create the trainer that will be used to trained our network. They are several types of trainers that can be found in the subpackage `neurotorch.trainers` like the `ClassificationTrainer`, the `RegressionTrainer` or the `PPOTrainer`. In this tutorial, we will use the `ClassificationTrainer` because we are trying to classify some images. Note that the callbacks are some object that will be called during the training. They are found in the subpackage `neurotorch.callbacks` like the `CheckpointManager`.\n",
    "\n",
    "In addition we will optimize our network with BPTT (backpropagation through time). The object BPTT from neurotorch is a callback, then we just have to add it to ou callbacks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "\tnt.BPTT(optimizer=torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0.0)),\n",
    "\tcheckpoint_manager,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = ClassificationTrainer(\n",
    "\tmodel=network,\n",
    "\t# regularization=regularization,\n",
    "\t# regularization_optimizer=torch.optim.SGD(regularization.parameters(), lr=1e-2, weight_decay=0.0),\n",
    "\tcallbacks=callbacks,\n",
    "\tverbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training will start! Let's see how our simple network will perform on this task.\n",
    "\n",
    "Note: the argument `force_overwrite` is used to overwrite the existing checkpoint if it exists and the argument `load_checkpoint_mode` if the model will be loaded from the last checkpoint (`LoadCheckpointMode.LAST_ITR`) or from the best checkpoint (`LoadCheckpointMode.BEST_ITR`). The loading mode is really useful when you want to stop the training and restart it later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_history = trainer.train(\n",
    "\tdataloaders[\"train\"],\n",
    "\tdataloaders[\"val\"],\n",
    "\tn_iterations=n_iterations,\n",
    "\tload_checkpoint_mode=LoadCheckpointMode.LAST_ITR,\n",
    "\t# force_overwrite=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training is finished! Let's see how our network learned over the iterations.\n",
    "\n",
    "Note: the next graphs are really useful to see if the model overfit or underfit the data over the iterations. It's why you can use the callback `TrainingHistoryVisualizationCallback` to see those graphs in real time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_history.plot(show=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can load the model at the best or the last iteration for the testing phase."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = network.load_checkpoint(checkpoint_manager.checkpoints_meta_path, LoadCheckpointMode.BEST_ITR, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how our network perform on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = {\n",
    "\tk: ClassificationMetrics.compute_y_true_y_pred(network, dataloader, verbose=True, desc=f\"{k} predictions\")\n",
    "\tfor k, dataloader in dataloaders.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "\tk: ClassificationMetrics.accuracy(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "pprint.pprint(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the results on other popular classification metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precisions = {\n",
    "\tk: ClassificationMetrics.precision(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "recalls = {\n",
    "\tk: ClassificationMetrics.recall(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "f1s = {\n",
    "\tk: ClassificationMetrics.f1(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = OrderedDict(dict(\n",
    "\tnetwork=network,\n",
    "\taccuracies=accuracies,\n",
    "\tprecisions=precisions,\n",
    "\trecalls=recalls,\n",
    "\tf1s=f1s,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pprint.pprint(results, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the end of the tutorial!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "NeuroTorch is a library that allows you to easily build neural networks and train them. Moreover, it allows you to do audio signal classification using dynamics from neurosciences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
