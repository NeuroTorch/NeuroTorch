{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Checkpoint Manager Tutorial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This tutorial was tested with the version `0.0.1-beta0` of NeuroTorch.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The checkpoint manager is very useful in the NeuroTorch's training pipeline since it is a callback. Please note that it is possible to use the traditional pytorch save and load method, see [Pytorch save and load tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for more information.\n",
    "\n",
    "As usual, the first thing is to import NeuroTorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import neurotorch as nt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, simply create an object <code>CheckpointManager</code> with the name of your desired folder as an argument"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckpointManager<0>: (priority=100, save_state=False, load_state=False, )\n"
     ]
    }
   ],
   "source": [
    "checkpoint_folder = f\"./checkpoints/network\"\n",
    "checkpoint_manager = nt.CheckpointManager(checkpoint_folder)\n",
    "print(checkpoint_manager)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The name of the checkpoint_folder must also be given to the <code>Sequential</code> in order to save the parameters of the network during training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (_to_device_transform): ToDevice(cuda, async=True)\n",
      "  (input_layers): ModuleDict()\n",
      "  (hidden_layers): ModuleList()\n",
      "  (output_layers): ModuleDict(\n",
      "    (output_0): Linear<output_0>(10->10)@cuda\n",
      "  )\n",
      "  (input_transform): ModuleDict(\n",
      "    (output_0): Sequential(\n",
      "      (0): CallableToModuleWrapper(Compose(\n",
      "          ToTensor()\n",
      "      ))\n",
      "      (1): ToDevice(cuda, async=True)\n",
      "    )\n",
      "  )\n",
      "  (output_transform): ModuleDict(\n",
      "    (output_0): IdentityTransform()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = nt.Sequential(layers=[nt.Linear(10, 10)], checkpoint_folder=checkpoint_folder).build()\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is in the checkpoint folder ?\n",
    "\n",
    "After a training, you will obtain three different types of file.\n",
    "\n",
    "### Training's parameters\n",
    "\n",
    "These are your <code> .pth </code> files. Those files are the one that contain the parameter of your model at a certain time (at a certain iteration for instance). These are the files you might want to give to a colleague in order to reproduce your data.\n",
    "\n",
    "### Network-checkpoint $\\Rightarrow$ .Json summary\n",
    "\n",
    "A json file will be generated which contain the name of your different training parameters that are saved. The best one is label in a way that you can easily get access to it later. This json is the bridge between your code and the <code>.pth</code>\n",
    "\n",
    "### Training history figure\n",
    "\n",
    "A training history is also generated to summarize the performance of your training. It can bring insight on how the loss evolve relative to the iteration or the learning rate. It is a great tool to compare results obtained with different hyperparameters.\n",
    "\n",
    "Here's an example of the json file and the <code>.pth</code> :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'./checkpoints/network/network-checkpoints.json'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_manager.checkpoints_meta_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'./checkpoints/network\\\\network-itr0.pth'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_manager.save_checkpoint(itr=0, itr_metrics={}, state_dict=network.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_layer().forward_weights.data.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once your parameters are saved, it is now time to load them up in ordre to \"play\" with your data. Simply call <code>load_checkpoint</code> and give it a <code>load_checkpoint_mode</code>. This last step will determine which of your multiple <code>.pth</code> will be loaded. For instance, one might want to load the last checkpoint (use <code>nt.LoadCheckpointMode.LAST_ITR</code>) or one might want to use the best one (nt.LoadCheckpointMode.BEST_ITR). Here is an example :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'itr': 0,\n 'model_state_dict': OrderedDict([('output_layers.output_0.bias_weights',\n               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n              ('output_layers.output_0._forward_weights',\n               tensor([[-7.3540e-01,  4.3947e-01,  4.3454e-04, -1.5365e-01,  2.7009e-01,\n                         1.1282e-01,  4.2071e-02, -2.2711e-01,  3.6099e-01,  1.5909e-01],\n                       [-4.6935e-02, -1.7355e-01,  2.8171e-01,  3.0744e-01, -3.5100e-01,\n                         1.7603e-02,  6.6100e-02,  1.3925e-01, -7.1727e-02, -9.7251e-03],\n                       [-7.4916e-02, -3.1412e-02,  1.8288e-01,  4.4510e-01,  9.3525e-02,\n                         2.9784e-02,  1.2521e-01, -7.2491e-02, -4.5314e-01,  3.9042e-01],\n                       [-1.9225e-01,  1.7739e-01,  2.8031e-01,  2.1685e-01,  1.9586e-01,\n                        -1.3653e-01,  7.9019e-02,  1.3299e-01,  3.2456e-01, -4.5222e-01],\n                       [-8.3174e-02, -3.9672e-01,  2.9031e-01, -1.6875e-01,  4.6306e-02,\n                         4.1986e-01, -1.4235e-01,  2.3946e-02,  2.3502e-01, -4.7609e-01],\n                       [ 1.9356e-02, -2.1739e-02,  1.7779e-01, -3.4789e-02,  4.4785e-02,\n                         8.8605e-02,  2.9104e-01,  1.5570e-01, -7.0589e-01,  2.3611e-01],\n                       [-3.1387e-01, -4.6288e-01,  3.2183e-02,  5.0033e-02,  1.3759e-02,\n                        -3.2131e-01, -4.4193e-01,  2.6762e-01, -2.3519e-01,  4.1373e-01],\n                       [ 3.5554e-01, -2.5519e-01,  1.3032e-01,  1.3117e-01, -1.0465e-01,\n                         1.3751e-01,  8.2889e-02,  2.9038e-02, -3.0066e-01, -2.5166e-02],\n                       [-1.0801e-02, -2.3561e-01,  8.0063e-02,  2.4825e-01,  3.6940e-01,\n                        -5.7543e-01, -2.2124e-02,  2.3179e-01,  4.3978e-02, -1.3337e-01],\n                       [-2.6167e-02,  3.4522e-01, -6.3368e-02,  6.5590e-02, -1.7582e-01,\n                        -7.7795e-02, -1.5273e-01,  1.7656e-01,  1.3137e-01, -2.3871e-01]],\n                      device='cuda:0'))]),\n 'optimizer_state_dict': None,\n 'metrics': {},\n 'training_history': None}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.load_checkpoint(checkpoints_meta_path=checkpoint_manager.checkpoints_meta_path, load_checkpoint_mode=nt.LoadCheckpointMode.LAST_ITR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-7.3540e-01,  4.3947e-01,  4.3454e-04, -1.5365e-01,  2.7009e-01,\n          1.1282e-01,  4.2071e-02, -2.2711e-01,  3.6099e-01,  1.5909e-01],\n        [-4.6935e-02, -1.7355e-01,  2.8171e-01,  3.0744e-01, -3.5100e-01,\n          1.7603e-02,  6.6100e-02,  1.3925e-01, -7.1727e-02, -9.7251e-03],\n        [-7.4916e-02, -3.1412e-02,  1.8288e-01,  4.4510e-01,  9.3525e-02,\n          2.9784e-02,  1.2521e-01, -7.2491e-02, -4.5314e-01,  3.9042e-01],\n        [-1.9225e-01,  1.7739e-01,  2.8031e-01,  2.1685e-01,  1.9586e-01,\n         -1.3653e-01,  7.9019e-02,  1.3299e-01,  3.2456e-01, -4.5222e-01],\n        [-8.3174e-02, -3.9672e-01,  2.9031e-01, -1.6875e-01,  4.6306e-02,\n          4.1986e-01, -1.4235e-01,  2.3946e-02,  2.3502e-01, -4.7609e-01],\n        [ 1.9356e-02, -2.1739e-02,  1.7779e-01, -3.4789e-02,  4.4785e-02,\n          8.8605e-02,  2.9104e-01,  1.5570e-01, -7.0589e-01,  2.3611e-01],\n        [-3.1387e-01, -4.6288e-01,  3.2183e-02,  5.0033e-02,  1.3759e-02,\n         -3.2131e-01, -4.4193e-01,  2.6762e-01, -2.3519e-01,  4.1373e-01],\n        [ 3.5554e-01, -2.5519e-01,  1.3032e-01,  1.3117e-01, -1.0465e-01,\n          1.3751e-01,  8.2889e-02,  2.9038e-02, -3.0066e-01, -2.5166e-02],\n        [-1.0801e-02, -2.3561e-01,  8.0063e-02,  2.4825e-01,  3.6940e-01,\n         -5.7543e-01, -2.2124e-02,  2.3179e-01,  4.3978e-02, -1.3337e-01],\n        [-2.6167e-02,  3.4522e-01, -6.3368e-02,  6.5590e-02, -1.7582e-01,\n         -7.7795e-02, -1.5273e-01,  1.7656e-01,  1.3137e-01, -2.3871e-01]],\n       device='cuda:0', requires_grad=True)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_layer().forward_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A few more words on the checkpoint manager\n",
    "\n",
    "The checkpoint manager built in NeuroTorch allows you to specify **when** you want to save the training's parameters. This is because saving the parameters can be a long process if it is done at each step. Also, it is generally not interesting to save the first iteration (the last one are generally the one you want)! With the checkpoint manger, you can save at a certain frequency or only saved the last iteration for example.\n",
    "\n",
    "### Example from our tutorial *time_series_forecasting_wilson_cowan*\n",
    "\n",
    "In this tutorial (that we highly recommend!), we use the checkpoint manager as a powerful tool during the training. If you inspect closely <code>main.py</code> of this tutorial, you will find the following <code>CheckpointManager</code> :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckpointManager<1>: (priority=100, save_state=False, load_state=False, )\n"
     ]
    }
   ],
   "source": [
    "checkpoint_folder = f\"./checkpoints/network\"\n",
    "n_iterations = 1000\n",
    "checkpoint_manager = nt.CheckpointManager(\n",
    "\tcheckpoint_folder,\n",
    "\tmetric=\"train_loss\",\n",
    "\tminimise_metric=False,\n",
    "\tsave_freq=-1,\n",
    "\tsave_best_only=True,\n",
    "\tstart_save_at=int(0.98 * n_iterations),\n",
    ")\n",
    "print(checkpoint_manager)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at every argument to make sure we truly understand what is happening here.\n",
    "- First, we give the name of our checkpoint folder!\n",
    "- <code>metric</code> : We give the name of the metric to collect the best checkpoint on\n",
    "- <code>minimise_metric</code> : In this example, we wanted to maximise the metric. It is therefore set to False\n",
    "- <code>save_freq</code> : Here, we absolutely want to save the last iteration. By specifying $-1$, we tell the checkpoint manager to save the last iteration no matter what.\n",
    "- <code>save_best_only</code> : Not only do we want to save the last, we also want to save the best! This argument is therefore **True**\n",
    "- <code>start_save_at</code> : We also want to save the iterations near the end of our training. Here, we start saving after 98% of our training is done\n",
    "\n",
    "**Feel free to explore the different tutorials since most of them use the checkpoint manager!**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}