{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial was tested with the version `0.0.1-beta4` of NeuroTorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"nt-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://NeuroTorch.github.io/NeuroTorch/\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/neurotorch_logo_32px.png?raw=true\" width=32px height=32px  />Documentation</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/colab_logo_32px.png?raw=true\" width=32px height=32px  />Run in Google Colab</a>\n",
    "</td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32px height=32px />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/NeuroTorch/NeuroTorch/blob/main/tutorials/reinforcement_learning/tutorial.ipynb\"><img src=\"https://github.com/NeuroTorch/NeuroTorch/blob/main/images/download_logo_32px.png?raw=true\" width=32px height=32px />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train an agent in a [gym](https://www.gymlibrary.dev/content/basic_usage/) environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now install the dependencies by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/NeuroTorch/NeuroTorch.git@119-rl-ppo\n",
      "  Cloning https://github.com/NeuroTorch/NeuroTorch.git (to revision 119-rl-ppo) to c:\\users\\gince\\appdata\\local\\temp\\pip-req-build-nl05nrjs\n",
      "  Resolved https://github.com/NeuroTorch/NeuroTorch.git to commit fc67d4af63e921165a135232a2db507cff88f95b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (1.13.0+cu117)\n",
      "Requirement already satisfied: matplotlib>=3.5.2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (3.6.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (1.1.3)\n",
      "Requirement already satisfied: setuptools>=57.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (60.2.0)\n",
      "Requirement already satisfied: torchvision>=0.12.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (0.14.0+cu117)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (4.64.1)\n",
      "Requirement already satisfied: six>=1.16.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (1.16.0)\n",
      "Requirement already satisfied: unstable in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (0.5.1)\n",
      "Requirement already satisfied: pytest>=7.1.2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (7.2.2)\n",
      "Requirement already satisfied: psutil>=5.9.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (5.9.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (1.9.3)\n",
      "Requirement already satisfied: docutils>=0.17.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (0.19)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (1.23.5)\n",
      "Requirement already satisfied: pythonbasictools in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from NeuroTorch==0.0.1b3) (0.0.1a1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (9.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from matplotlib>=3.5.2->NeuroTorch==0.0.1b3) (0.11.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (1.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (22.1.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (0.4.6)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->NeuroTorch==0.0.1b3) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from scikit-learn>=1.1.1->NeuroTorch==0.0.1b3) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from scikit-learn>=1.1.1->NeuroTorch==0.0.1b3) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from torch>=1.11.0->NeuroTorch==0.0.1b3) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from torchvision>=0.12.0->NeuroTorch==0.0.1b3) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from requests->torchvision>=0.12.0->NeuroTorch==0.0.1b3) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from requests->torchvision>=0.12.0->NeuroTorch==0.0.1b3) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from requests->torchvision>=0.12.0->NeuroTorch==0.0.1b3) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from requests->torchvision>=0.12.0->NeuroTorch==0.0.1b3) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none -q https://github.com/NeuroTorch/NeuroTorch.git 'C:\\Users\\gince\\AppData\\Local\\Temp\\pip-req-build-nl05nrjs'\n",
      "  Running command git checkout -b 119-rl-ppo --track origin/119-rl-ppo\n",
      "  branch '119-rl-ppo' set up to track 'origin/119-rl-ppo'.\n",
      "  Switched to a new branch '119-rl-ppo'\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\github\\NeuroTorch\\tutorials\\reinforcement_learning\\rl_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythonbasictools in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (0.0.1a1)\n",
      "Requirement already satisfied: pytest>=7.1.2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pythonbasictools) (7.2.2)\n",
      "Requirement already satisfied: setuptools>=57.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pythonbasictools) (60.2.0)\n",
      "Requirement already satisfied: docutils>=0.17.1 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pythonbasictools) (0.19)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pythonbasictools) (4.64.1)\n",
      "Requirement already satisfied: psutil>=5.9.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pythonbasictools) (5.9.4)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (22.1.0)\n",
      "Requirement already satisfied: packaging in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (1.0.4)\n",
      "Requirement already satisfied: iniconfig in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from pytest>=7.1.2->pythonbasictools) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from packaging->pytest>=7.1.2->pythonbasictools) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\github\\NeuroTorch\\tutorials\\reinforcement_learning\\rl_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d]==0.26.2 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (5.1.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (1.23.5)\n",
      "Requirement already satisfied: swig==4.* in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (4.1.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (2.3.5)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from gym[box2d]==0.26.2) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\github\\neurotorch\\tutorials\\reinforcement_learning\\rl_venv\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[box2d]==0.26.2) (3.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\github\\NeuroTorch\\tutorials\\reinforcement_learning\\rl_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "#@title Install dependencies {display-mode: \"form\"}\n",
    "\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "\n",
    "!pip install git+https://github.com/NeuroTorch/NeuroTorch.git@119-rl-ppo\n",
    "!pip install pythonbasictools\n",
    "!pip install gym[box2d]==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "import neurotorch as nt\n",
    "from neurotorch.callbacks.early_stopping import EarlyStoppingThreshold\n",
    "from neurotorch.rl.agent import Agent\n",
    "from neurotorch.rl.rl_academy import RLAcademy\n",
    "from neurotorch.rl.utils import TrajectoryRenderer, space_to_continuous_shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Logs file at: .//logs/logs-06-03-2023/rl_tutorial-16781266451481016.log\n",
      "\n",
      "INFO:root:__Python VERSION: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "INFO:root:Number of available cores: 8.\n",
      "INFO:root:Number of available logical processors: 16.\n",
      "INFO:root:__pyTorch VERSION:1.13.0+cu117\n",
      "INFO:root:__CUDA VERSION:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:59:34_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n",
      "\n",
      "INFO:root:__nvidia-smi: Not Found\n",
      "INFO:root:__CUDNN VERSION:8500\n",
      "INFO:root:__Number CUDA Devices:1\n",
      "INFO:root:\n",
      "-------------------------\n",
      "DEVICE: cuda\n",
      "-------------------------\n",
      "\n",
      "INFO:root:NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "INFO:root:Memory Usage:\n",
      "INFO:root:Allocated: 0.0 GB\n",
      "INFO:root:Cached:   0.0 GB\n",
      "INFO:root:Memory summary: \n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_file_setup(\"rl_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env parameters\n",
    "env_id = \"LunarLander-v2\"\n",
    "continuous_action = True\n",
    "\n",
    "# Network parameters\n",
    "use_spiking_policy = True  # Type of the policy\n",
    "n_hidden_units = 128\n",
    "n_critic_hidden_units = 128\n",
    "n_encoder_steps = 8\n",
    "\n",
    "# Trainer parameters\n",
    "n_iterations = 100\n",
    "n_epochs = 30\n",
    "n_new_trajectories = 1\n",
    "last_k_rewards = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id, render_mode=\"rgb_array\", continuous=continuous_action)\n",
    "continuous_obs_shape = space_to_continuous_shape(getattr(env, \"single_observation_space\", env.observation_space))\n",
    "continuous_action_shape = space_to_continuous_shape(getattr(env, \"single_action_space\", env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_spiking_policy:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_snn-policy\"\n",
    "else:\n",
    "    checkpoint_folder = f\"data/tr_data/checkpoints_{env_id}_classical-policy\"\n",
    "checkpoint_manager = nt.CheckpointManager(\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    save_freq=int(0.1*n_iterations),\n",
    "    metric=RLAcademy.CUM_REWARDS_METRIC_KEY,\n",
    "    minimise_metric=False,\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are initializing the learning algorithm that will be used to train the agent. For now, this learning algorithm it's the popular [Proximal Policy Optimisation](https://arxiv.org/pdf/1707.06347.pdf) from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_la = nt.rl.PPO(\n",
    "    critic_criterion=torch.nn.SmoothL1Loss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now the time to define our policy. For short, the policy is the model that will be used to take the actions in the environment. The critic is the model used to estimate the rewards-to-go of the states that the agent will encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'neurotorch.transforms' has no attribute 'ReduceFuncTanh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_spiking_policy:\n\u001b[0;32m      2\u001b[0m     policy \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mSequentialRNN(\n\u001b[0;32m      3\u001b[0m         input_transform\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m             nt\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mConstantValuesTransform(n_steps\u001b[38;5;241m=\u001b[39mn_encoder_steps)\n\u001b[0;32m      5\u001b[0m         ],\n\u001b[0;32m      6\u001b[0m         layers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      7\u001b[0m             nt\u001b[38;5;241m.\u001b[39mSpyLIFLayerLPF(\n\u001b[0;32m      8\u001b[0m                 continuous_obs_shape[\u001b[38;5;241m0\u001b[39m], n_hidden_units, use_recurrent_connection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             ),\n\u001b[0;32m     10\u001b[0m             nt\u001b[38;5;241m.\u001b[39mSpyLILayer(n_hidden_units, continuous_action_shape[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m     11\u001b[0m         ],\n\u001b[0;32m     12\u001b[0m         output_transform\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     13\u001b[0m             (\n\u001b[1;32m---> 14\u001b[0m                 \u001b[43mnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceFuncTanh\u001b[49m(nt\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mReduceMean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m continuous_action \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m                 nt\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mReduceMax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m             )\n\u001b[0;32m     18\u001b[0m         ],\n\u001b[0;32m     19\u001b[0m     )\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     policy \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     22\u001b[0m         layers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     23\u001b[0m             torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(continuous_obs_shape[\u001b[38;5;241m0\u001b[39m], n_hidden_units),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m         ]\n\u001b[0;32m     32\u001b[0m     )\u001b[38;5;241m.\u001b[39mbuild()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'neurotorch.transforms' has no attribute 'ReduceFuncTanh'"
     ]
    }
   ],
   "source": [
    "if use_spiking_policy:\n",
    "    policy = nt.SequentialRNN(\n",
    "        input_transform=[\n",
    "            nt.transforms.ConstantValuesTransform(n_steps=n_encoder_steps)\n",
    "        ],\n",
    "        layers=[\n",
    "            nt.SpyLIFLayerLPF(\n",
    "                continuous_obs_shape[0], n_hidden_units, use_recurrent_connection=False\n",
    "            ),\n",
    "            nt.SpyLILayer(n_hidden_units, continuous_action_shape[0]),\n",
    "        ],\n",
    "        output_transform=[\n",
    "            (\n",
    "                nt.transforms.ReduceFuncTanh(nt.transforms.ReduceMean(dim=1))\n",
    "                if continuous_action else\n",
    "                nt.transforms.ReduceMax(dim=1)\n",
    "            )\n",
    "        ],\n",
    "    ).build()\n",
    "else:\n",
    "    policy = nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(continuous_obs_shape[0], n_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_hidden_units, n_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_hidden_units, continuous_action_shape[0]),\n",
    "            (torch.nn.Tanh() if continuous_action else torch.nn.Identity())\n",
    "        ]\n",
    "    ).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're defining the agent using the policy and the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env=env,\n",
    "    behavior_name=env_id,\n",
    "    policy=policy,\n",
    "    critic=nt.Sequential(\n",
    "        layers=[\n",
    "            torch.nn.Linear(continuous_obs_shape[0], n_critic_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_critic_hidden_units, n_critic_hidden_units),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(n_critic_hidden_units, 1),\n",
    "        ]\n",
    "    ).build(),\n",
    "    checkpoint_folder=checkpoint_manager.checkpoint_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an early stopping callback that will stop the training if the mean of the last k cumulative rewards is better or equal than 230 (at 200 the environnement is considered as solved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingThreshold(\n",
    "    metric=f\"mean_last_{last_k_rewards}_rewards\",\n",
    "    threshold=230.0,\n",
    "    minimize_metric=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the RLAcademy. This is a special type of Trainer used to train the agent in a reinforcement learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "academy = RLAcademy(\n",
    "    agent=agent,\n",
    "    callbacks=[checkpoint_manager, ppo_la, early_stopping],\n",
    ")\n",
    "print(f\"Academy:\\n{academy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will start the actual training with the following parameter:\n",
    "\n",
    "    - `n_iterations`: The number of time the trainer will generate trajectories and will do an optimisation pass.\n",
    "    - `n_epochs`: The number of time the trainer will pass through the buffer of episodes for an optimisation pass.\n",
    "    - `n_batches`: The number of batch to do at each epoch.\n",
    "    - `n_new_trajectories`: The number of new trajectories to generate at each iteration.\n",
    "    - `batch_size`: The number of episodes for a single batch.\n",
    "    - `buffer_size`: The size of the buffer.\n",
    "    - `clear_buffer`: Wheater to clear or the the buffer before each iteration.\n",
    "    - `last_k_rewards`: The number of k previous rewards to show in the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = academy.train(\n",
    "    env,\n",
    "    n_iterations=n_iterations,\n",
    "    n_epochs=n_epochs,\n",
    "    n_batches=-1,\n",
    "    n_new_trajectories=n_new_trajectories,\n",
    "    batch_size=4096,\n",
    "    buffer_size=np.inf,\n",
    "    clear_buffer=True,\n",
    "    randomize_buffer=True,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.LAST_ITR,\n",
    "    force_overwrite=False,\n",
    "    verbose=True,\n",
    "    render=False,\n",
    "    last_k_rewards=last_k_rewards,\n",
    ")\n",
    "if not getattr(env, \"closed\", False):\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will generate new trajectories of the agent just to see how it will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_checkpoint(\n",
    "    checkpoints_meta_path=checkpoint_manager.checkpoints_meta_path,\n",
    "    load_checkpoint_mode=nt.LoadCheckpointMode.BEST_ITR\n",
    ")\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\", continuous=continuous_action)\n",
    "agent.eval()\n",
    "gen_trajectories_out = academy.generate_trajectories(\n",
    "    n_trajectories=10, epsilon=0.0, verbose=True, env=env, render=True, re_trajectories=True,\n",
    ")\n",
    "best_trajectory_idx = np.argmax([t.cumulative_reward for t in gen_trajectories_out.trajectories])\n",
    "trajectory_renderer = TrajectoryRenderer(trajectory=gen_trajectories_out.trajectories[best_trajectory_idx], env=env)\n",
    "\n",
    "cumulative_rewards = gen_trajectories_out.cumulative_rewards\n",
    "print(f\"Buffer: {gen_trajectories_out.buffer}\")\n",
    "print(f\"Cumulative rewards: {np.nanmean(cumulative_rewards):.3f} +/- {np.nanstd(cumulative_rewards):.3f}\")\n",
    "best_cum_reward_fmt = f\"{cumulative_rewards[best_trajectory_idx]:.3f}\"\n",
    "print(f\"Best trajectory: {best_trajectory_idx}, cumulative reward: {best_cum_reward_fmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the best trajectory and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_renderer.render(\n",
    "    filename=(\n",
    "        f\"{agent.checkpoint_folder}/figures/trajectory_{best_trajectory_idx}-\"\n",
    "        f\"CR{best_cum_reward_fmt.replace('.', '_')}\"\n",
    "    ),\n",
    "    file_extension=\"gif\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
