

<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>neurotorch.modules.layers package &mdash; NeuroTorch  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="static/css/theme.min.css" type="text/css" />
  <link rel="stylesheet" href="static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="static/css/theme.min.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="neurotorch.regularization package" href="neurotorch.regularization.html" />
    <link rel="prev" title="neurotorch.modules package" href="neurotorch.modules.html" /> 

</head>

<body>
    <header>
        <div class="container">
            <a class="site-nav-toggle hidden-lg-up"><i class="icon-menu"></i></a>
            <a class="site-title" href="index.html">
                NeuroTorch
            </a>
        </div>
    </header>


<div class="breadcrumbs-outer hidden-xs-down">
    <div class="container">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="breadcrumbs">
    
      <li><a href="index.html">Docs</a></li>
        
          <li><a href="neurotorch.html">neurotorch package</a></li>
        
          <li><a href="neurotorch.modules.html">neurotorch.modules package</a></li>
        
      <li>neurotorch.modules.layers package</li>
    
    
      <li class="breadcrumbs-aside">
        
            
            <a href="sources/neurotorch.modules.layers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
</div>
    </div>
</div>
    <div class="main-outer">
        <div class="container">
            <div class="row">
                <div class="col-12 col-lg-3 site-nav">
                    
<div role="search">
    <form class="search" action="search.html" method="get">
        <div class="icon-input">
            <input type="text" name="q" placeholder="Search" />
            <span class="icon-search"></span>
        </div>
        <input type="submit" value="Go" class="d-hidden" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div>
                    <div class="site-nav-tree">
                        
                            
                            
                                <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">1. Description</a><ul>
<li class="toctree-l2"><a class="reference internal" href="readme.html#current-version">Current Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="readme.html#next-versions">Next Versions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#installation">2. Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="readme.html#last-unstable-version">2.1 Last unstable version</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#tutorials-applications">3. Tutorials / Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#quick-usage-preview">4. Quick usage preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#why-neurotorch">5. Why NeuroTorch?</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#similar-work">6. Similar work</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#about">7. About</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#important-links">8. Important Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#found-a-bug-or-have-a-feature-request">9. Found a bug or have a feature request?</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#thanks">10. Thanks</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#license">11. License</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html#citation">12. Citation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modules:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="neurotorch.html">neurotorch package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="neurotorch.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#module-neurotorch.dimension">neurotorch.dimension module</a></li>
<li class="toctree-l2"><a class="reference internal" href="neurotorch.html#module-neurotorch">Module contents</a></li>
</ul>
</li>
</ul>

                            
                        
                    </div>
                </div>
                <div class="col-12 col-lg-9">
                    <div class="document">
                        
                            
  <section id="neurotorch-modules-layers-package">
<h1>neurotorch.modules.layers package<a class="headerlink" href="#neurotorch-modules-layers-package" title="Permalink to this heading">¶</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-neurotorch.modules.layers.base">
<span id="neurotorch-modules-layers-base-module"></span><h2>neurotorch.modules.layers.base module<a class="headerlink" href="#module-neurotorch.modules.layers.base" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.base.</span></span><span class="sig-name descname"><span class="pre">BaseLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="neurotorch.modules.html#neurotorch.modules.base.SizedModule" title="neurotorch.modules.base.SizedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SizedModule</span></code></a></p>
<p>Base class for all layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_size</span></code> (Optional[Dimension]): The input size of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> (Optional[Dimension]): The output size of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code> (str): The name of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">kwargs</span></code> (dict): Additional keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the forward method of the layer. If the layer is not built, it will be built automatically.
In addition, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">kwargs['regularize']</span></code> is set to True, the :meth: <cite>update_regularization_loss</cite> method
will be called.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The inputs to the layer.</p></li>
<li><p><strong>args</strong> – The positional arguments to the forward method.</p></li>
<li><p><strong>kwargs</strong> – The keyword arguments to the forward method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor of the BaseLayer class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer.</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device of the layer. Defaults to the current available device.</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite>
will be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>freeze_weights</strong> (<em>bool</em>) – Whether to freeze the weights of the layer. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><span class="pre">BaseLayer</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.freeze_weights">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">freeze_weights</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.freeze_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.get_and_reset_regularization_loss">
<span class="sig-name descname"><span class="pre">get_and_reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.get_and_reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Get and reset the regularization loss for this layer. The regularization loss will be reset by the
reset_regularization_loss method after it is returned.</p>
<p>WARNING: If this method is not called after an integration, the update of the regularization loss can cause a
memory leak. TODO: fix this.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The regularization loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.get_regularization_loss">
<span class="sig-name descname"><span class="pre">get_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.get_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the regularization loss for this layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The regularization loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.infer_sizes_from_inputs">
<span class="sig-name descname"><span class="pre">infer_sizes_from_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.infer_sizes_from_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Try to infer the input and output size of the layer from the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>torch.Tensor</em>) – The inputs to infer the size from.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.is_built">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_built</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.is_built" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.is_ready_to_build">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_ready_to_build</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.is_ready_to_build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.requires_grad">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.requires_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.reset_regularization_loss">
<span class="sig-name descname"><span class="pre">reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the regularization loss to zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Move all the parameters of the layer to the specified device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to move the parameters to.</p></li>
<li><p><strong>non_blocking</strong> (<em>bool</em>) – Whether to move the parameters in a non-blocking way.</p></li>
<li><p><strong>args</strong> – Additional positional arguments.</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseLayer.update_regularization_loss">
<span class="sig-name descname"><span class="pre">update_regularization_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseLayer.update_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the regularization loss for this layer. Each update call increments the regularization loss so at the end
the regularization loss will be the sum of all calls to this function. This method is called at the end of each
forward call automatically by the BaseLayer class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>Optional</em><em>[</em><em>Any</em><em>]</em>) – The current state of the layer.</p></li>
<li><p><strong>args</strong> – Other positional arguments.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated regularization loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.base.</span></span><span class="sig-name descname"><span class="pre">BaseNeuronsLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></a></p>
<p>A base class for layers that have neurons. This class provides two importants Parameters: the
<a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code></a> and the <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code></a>. Child classes must implement the <a href="#id1"><span class="problematic" id="id2">:method:`forward`</span></a>
method and the <a href="#id3"><span class="problematic" id="id4">:mth:`create_empty_state`</span></a> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code></a> (torch.nn.Parameter): The weights used to compute the output of the layer.</p></li>
<li><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code></a> (torch.nn.Parameter): The weights used to compute the hidden state of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dt</span></code> (float): The time step of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">use_rec_eye_mask</span></code> (torch.Tensor): Whether to use the recurrent eye mask.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rec_mask</span></code> (torch.Tensor): The recurrent eye mask.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></a> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><span class="pre">BaseNeuronsLayer</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<p>In this method the <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code></a>, <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights" title="neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code></a> and :attr: <cite>rec_mask</cite> are created and
finally the method <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.initialize_weights_" title="neurotorch.modules.layers.base.BaseNeuronsLayer.initialize_weights_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize_weights_()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.connectivity_convention">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">connectivity_convention</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.utils.html#neurotorch.utils.ConnectivityConvention" title="neurotorch.utils.ConnectivityConvention"><span class="pre">ConnectivityConvention</span></a></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.connectivity_convention" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the connectivity convention.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The connectivity convention.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.force_dale_law">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">force_dale_law</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.force_dale_law" title="Permalink to this definition">¶</a></dt>
<dd><p>Get whether to force the Dale’s law.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether to force the Dale’s law.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.forward_sign">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward_sign</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the forward sign.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The forward sign.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward_weights</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Parameter</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.forward_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the forward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The forward weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_sign_parameter">
<span class="sig-name descname"><span class="pre">get_forward_sign_parameter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Parameter</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_sign_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the forward sign parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The forward sign parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_data">
<span class="sig-name descname"><span class="pre">get_forward_weights_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the forward weights data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The forward weights data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_parameter">
<span class="sig-name descname"><span class="pre">get_forward_weights_parameter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Parameter</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_forward_weights_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the forward weights parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The forward weights parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_sign_parameter">
<span class="sig-name descname"><span class="pre">get_recurrent_sign_parameter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Parameter</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_sign_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the recurrent sign parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recurrent sign parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_data">
<span class="sig-name descname"><span class="pre">get_recurrent_weights_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the recurrent weights data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recurrent weights data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_parameter">
<span class="sig-name descname"><span class="pre">get_recurrent_weights_parameter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Parameter</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_recurrent_weights_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the recurrent weights parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recurrent weights parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_sign_parameters">
<span class="sig-name descname"><span class="pre">get_sign_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Parameter</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_sign_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the sign parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The sign parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.get_weights_parameters">
<span class="sig-name descname"><span class="pre">get_weights_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Parameter</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.get_weights_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the weights parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The weights parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_sign">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">recurrent_sign</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the recurrent sign.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recurrent sign.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">recurrent_weights</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Parameter</span></em><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.recurrent_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the recurrent weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recurrent weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_sign_parameter">
<span class="sig-name descname"><span class="pre">set_forward_sign_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_sign_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the forward sign parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter</strong> – The forward sign parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_data">
<span class="sig-name descname"><span class="pre">set_forward_weights_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the forward weights data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> – The forward weights data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_parameter">
<span class="sig-name descname"><span class="pre">set_forward_weights_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_forward_weights_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the forward weights parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter</strong> – The forward weights parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_sign_parameter">
<span class="sig-name descname"><span class="pre">set_recurrent_sign_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_sign_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the recurrent sign parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter</strong> – The recurrent sign parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_data">
<span class="sig-name descname"><span class="pre">set_recurrent_weights_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the recurrent weights data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> – The recurrent weights data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_parameter">
<span class="sig-name descname"><span class="pre">set_recurrent_weights_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.base.BaseNeuronsLayer.set_recurrent_weights_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the recurrent weights parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter</strong> – The recurrent weights parameter.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers.classical">
<span id="neurotorch-modules-layers-classical-module"></span><h2>neurotorch.modules.layers.classical module<a class="headerlink" href="#module-neurotorch.modules.layers.classical" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.classical.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.classical.Linear" title="neurotorch.modules.layers.classical.Linear"><span class="pre">Linear</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<p>In this method the <code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> and :attr: <cite>rec_mask</cite> are created and
finally the method <a class="reference internal" href="#neurotorch.modules.layers.classical.Linear.initialize_weights_" title="neurotorch.modules.layers.classical.Linear.initialize_weights_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize_weights_()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.Linear.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.Linear.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.classical.</span></span><span class="sig-name descname"><span class="pre">LinearRNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.classical.LinearRNN" title="neurotorch.modules.layers.classical.LinearRNN"><span class="pre">LinearRNN</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<p>In this method the <code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> and :attr: <cite>rec_mask</cite> are created and
finally the method <a class="reference internal" href="#neurotorch.modules.layers.classical.LinearRNN.initialize_weights_" title="neurotorch.modules.layers.classical.LinearRNN.initialize_weights_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize_weights_()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.classical.LinearRNN.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.classical.LinearRNN.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers.leaky_integrate">
<span id="neurotorch-modules-layers-leaky-integrate-module"></span><h2>neurotorch.modules.layers.leaky_integrate module<a class="headerlink" href="#module-neurotorch.modules.layers.leaky_integrate" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.leaky_integrate.</span></span><span class="sig-name descname"><span class="pre">LILayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>The integration in time of these dynamics is done using the equation
<a class="reference internal" href="#equation-li-v">(1)</a> inspired by Bellec and al. <span id="id5">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-li-v">
<span class="eqno">(1)<a class="headerlink" href="#equation-li-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \kappa V_j^{t} + \sum_{i}^N W_{ij}x_i^{t+\Delta t} + b_j
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-li-kappa">
<span class="eqno">(2)<a class="headerlink" href="#equation-li-kappa" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \kappa = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>The parameters of the equation <a class="reference internal" href="#equation-li-v">(1)</a> are:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-li-kappa">(2)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias_weights</span></code> (torch.nn.Parameter): Bias weights of the layer.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">kappa</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time see equation <a class="reference internal" href="#equation-li-kappa">(2)</a>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.leaky_integrate.LILayer" title="neurotorch.modules.layers.leaky_integrate.LILayer"><span class="pre">LILayer</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<p>In this method the <code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> and :attr: <cite>rec_mask</cite> are created and
finally the method <a class="reference internal" href="#neurotorch.modules.layers.leaky_integrate.LILayer.initialize_weights_" title="neurotorch.modules.layers.leaky_integrate.LILayer.initialize_weights_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize_weights_()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>[membrane potential of shape (batch_size, self.output_size)]</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.LILayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.LILayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.leaky_integrate.</span></span><span class="sig-name descname"><span class="pre">SpyLILayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>The SpyLI dynamics is a more complex variant of the LI dynamics (class <a class="reference internal" href="#neurotorch.modules.layers.leaky_integrate.LILayer" title="neurotorch.modules.layers.leaky_integrate.LILayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LILayer</span></code></a>) allowing it to have a
greater power of expression. This variant is also inspired by Neftci <span id="id6">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span> and also
contains  two differential equations like the SpyLIF dynamics <code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code>. The equation <a class="reference internal" href="#equation-spyli-i">(3)</a>
presents the synaptic current update equation with euler integration while the equation <a class="reference internal" href="#equation-spyli-v">(4)</a> presents the
synaptic potential update.</p>
<div class="math notranslate nohighlight" id="equation-spyli-i">
<span class="eqno">(3)<a class="headerlink" href="#equation-spyli-i" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    I_{\text{syn}, j}^{t+\Delta t} = \alpha I_{ ext{syn}, j}^{t} +
    \sum_{i}^{N} W_{ij}^{\text{rec}} I_{\text{syn}, j}^{t}
    + \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyli-v">
<span class="eqno">(4)<a class="headerlink" href="#equation-spyli-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \beta V_j^t + I_{\text{syn}, j}^{t+\Delta t} + b_j
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyli-alpha">
<span class="eqno">(5)<a class="headerlink" href="#equation-spyli-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_{\text{syn}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<div class="math notranslate nohighlight" id="equation-spyli-beta">
<span class="eqno">(6)<a class="headerlink" href="#equation-spyli-beta" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \beta = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<p>SpyTorch library: <a class="reference external" href="https://github.com/surrogate-gradient-learning/spytorch">https://github.com/surrogate-gradient-learning/spytorch</a>.</p>
<p>The variables of the equations <a class="reference internal" href="#equation-spyli-i">(3)</a> and <a class="reference internal" href="#equation-spyli-v">(4)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(I_{\text{syn}, j}^{t}\)</span> is the synaptic current of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spyli-alpha">(5)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spyli-beta">(6)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spyli-alpha">(5)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> (torch.nn.Parameter): Decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spyli-beta">(6)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): Slope of the Heaviside function (<span class="math notranslate nohighlight">\(\gamma\)</span>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer" title="neurotorch.modules.layers.leaky_integrate.SpyLILayer"><span class="pre">SpyLILayer</span></a></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the layer. This method must be call after the layer is initialized to make sure that the layer is ready
to be used e.g. the input and output size is set, the weights are initialized, etc.</p>
<p>In this method the <code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> and :attr: <cite>rec_mask</cite> are created and
finally the method <a class="reference internal" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.initialize_weights_" title="neurotorch.modules.layers.leaky_integrate.SpyLILayer.initialize_weights_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize_weights_()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The layer itself.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#neurotorch.modules.layers.base.BaseLayer" title="neurotorch.modules.layers.base.BaseLayer">BaseLayer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>[membrane potential of shape (batch_size, self.output_size),
synaptic current of shape (batch_size, self.output_size)]</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.leaky_integrate.SpyLILayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.leaky_integrate.SpyLILayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers.spiking">
<span id="neurotorch-modules-layers-spiking-module"></span><h2>neurotorch.modules.layers.spiking module<a class="headerlink" href="#module-neurotorch.modules.layers.spiking" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.ALIFLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">ALIFLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.ALIFLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a></p>
<p>The ALIF dynamic, inspired by Bellec and textit{al.} <span id="id7">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>, is very
similar to the LIF dynamics (class <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a>). In fact, ALIF has exactly the same potential
update equation as LIF. The difference comes
from the fact that the threshold potential varies with time and neuron input. Indeed, the threshold
is increased at each output pulse and is then decreased with a certain rate in order to come back to
its starting threshold <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. The threshold equation from <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a> is thus slightly
modified by changing <span class="math notranslate nohighlight">\(V_{\text{th}} \to A_j^t\)</span>. Thus, the output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>
denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is redefined by the equation <a class="reference internal" href="#equation-alif-z">(27)</a>.</p>
<div class="math notranslate nohighlight" id="equation-alif-z">
<span class="eqno">(7)<a class="headerlink" href="#equation-alif-z" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    z_j^t = H(V_j^t - A_j^t)
\end{equation}\]</div>
<p>The update of the activation threshold is then described by <a class="reference internal" href="#equation-alif-a">(42)</a>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(8)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    A_j^t = V_{\text{th}} + \beta a_j^t
\end{equation}\]</div>
<p>with the adaptation variable <span class="math notranslate nohighlight">\(a_j^t\)</span> described by <a class="reference internal" href="#equation-alif-a">(43)</a> and <span class="math notranslate nohighlight">\(\beta\)</span> an amplification
factor greater than 1 and typically equivalent to <span class="math notranslate nohighlight">\(\beta\approx 1.6\)</span> <span id="id8">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(9)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    a_j^{t+1} = \rho a_j + z_j^t
\end{equation}\]</div>
<p>With the decay factor <span class="math notranslate nohighlight">\(\rho\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-alif-rho">
<span class="eqno">(10)<a class="headerlink" href="#equation-alif-rho" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \rho = e^{-\frac{\Delta t}{\tau_a}}
\end{equation}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code>: The amplification factor of the threshold potential <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rho</span></code>: The decay factor of the adaptation variable <span class="math notranslate nohighlight">\(\rho\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.ALIFLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.ALIFLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau_m</strong> (<em>float</em>) – The decay time constant of the membrane potential which is generally 20 ms. See equation
<a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The activation threshold of the neuron.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor of the spikes.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.ALIFLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.ALIFLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>[[membrane potential of shape (batch_size, self.output_size)]
[current threshold of shape (batch_size, self.output_size)]
[spikes of shape (batch_size, self.output_size)]]</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.ALIFLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.ALIFLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.ALIFLayer.update_regularization_loss">
<span class="sig-name descname"><span class="pre">update_regularization_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.ALIFLayer.update_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the regularization loss for this layer. Each update call increments the regularization loss so at the end
the regularization loss will be the sum of all calls to this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> (<em>Optional</em><em>[</em><em>Any</em><em>]</em>) – The current state of the layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated regularization loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.BellecLIFLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">BellecLIFLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisidePhiApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.BellecLIFLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a></p>
<dl class="simple">
<dt>Layer implementing the LIF neuron model from the paper:</dt><dd><p>“A solution to the learning dilemma for recurrent networks of spiking neurons”
by Bellec et al. (2020) <span id="id9">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.BellecLIFLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisidePhiApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.BellecLIFLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau_m</strong> (<em>float</em>) – The decay time constant of the membrane potential which is generally 20 ms. See equation
<a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The activation threshold of the neuron.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor of the spikes.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.BellecLIFLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.BellecLIFLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.IzhikevichLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">IzhikevichLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt=0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.IzhikevichLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>Izhikevich p.274</p>
<p>Not usable for now, stay tuned.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.IzhikevichLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt=0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.IzhikevichLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the layer.; See the <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code> class for more details.;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The input size of the layer;</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The output size of the layer.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use a recurrent connection. Default is True.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use a recurrent eye mask. Default is False. This mask will be used to
mask to zero the diagonal of the recurrent connection matrix.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – The time step of the layer. Default is 1e-3.</p></li>
<li><p><strong>kwargs</strong> – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>regularize</strong> (<em>bool</em>) – Whether to regularize the layer. If True, the method <cite>update_regularization_loss</cite> will
be called after each forward pass. Defaults to False.</p></li>
<li><p><strong>hh_init</strong> (<em>str</em>) – The initialization method for the hidden state. Defaults to “zeros”.</p></li>
<li><p><strong>hh_init_mu</strong> (<em>float</em>) – The mean of the hidden state initialization when hh_init is random . Defaults to 0.0.</p></li>
<li><p><strong>hh_init_std</strong> (<em>float</em>) – The standard deviation of the hidden state initialization when hh_init is random. Defaults to 1.0.</p></li>
<li><p><strong>hh_init_seed</strong> (<em>int</em>) – The seed of the hidden state initialization when hh_init is random. Defaults to 0.</p></li>
<li><p><strong>force_dale_law</strong> (<em>bool</em>) – Whether to force the Dale’s law in the layer’s weights. Defaults to False.</p></li>
<li><p><strong>forward_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the forward_sign vector. If it is a float, the forward_sign vector will be initialized with this
value as the ration of inhibitory neurons. If it is a tensor, it will be used as the forward_sign vector.</p></li>
<li><p><strong>recurrent_sign</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>float</em><em>]</em>) – If force_dale_law is True, this parameter will be used to
initialize the recurrent_sign vector. If it is a float, the recurrent_sign vector will be initialized with
this value as the ration of inhibitory neurons. If it is a tensor, it will be used as the recurrent_sign vector.</p></li>
<li><p><strong>sign_activation</strong> (<em>Callable</em>) – The activation function used to compute the sign of the weights i.e. the
forward_sign and recurrent_sign vectors. Defaults to torch.nn.Tanh.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.IzhikevichLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.IzhikevichLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[membrane potential of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.IzhikevichLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.IzhikevichLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.IzhikevichLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.IzhikevichLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">LIFLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>LIF dynamics, inspired by <span id="id10">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span> , <span id="id11">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span> , models the synaptic
potential and impulses of a neuron over time. The shape of this potential is not considered realistic
<span id="id12">Izhikevich [<a class="reference internal" href="neurotorch.modules.html#id10" title="Eugene M. Izhikevich. Dynamical Systems in Neuroscience. MIT Press, 2007. ISBN 978-0-262-09043-8.">Izh07</a>]</span> , but the time at which the potential exceeds the threshold is.
This potential is found by the recurrent equation <a class="reference internal" href="#equation-lif-v">(32)</a> .</p>
<div class="math notranslate nohighlight" id="equation-lif-v">
<span class="eqno">(11)<a class="headerlink" href="#equation-lif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\alpha V_j^t + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t +
    \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<p>The variables of the equation <a class="reference internal" href="#equation-lif-v">(32)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the potential over time (equation <a class="reference internal" href="#equation-lif-alpha">(33)</a> ).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-lif-alpha">
<span class="eqno">(12)<a class="headerlink" href="#equation-lif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_m}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_m\)</span> being the decay time constant of the membrane potential which is generally 20 ms.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-lif-z">(34)</a> .</p>
<div class="math notranslate nohighlight" id="equation-lif-z">
<span class="eqno">(13)<a class="headerlink" href="#equation-lif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - V_{\text{th}})\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code> (torch.nn.Parameter): The weights used to compute the output of the layer <span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> (torch.nn.Parameter): The weights used to compute the hidden state of the layer <span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dt</span></code> (float): The time step of the layer <span class="math notranslate nohighlight">\(\Delta t\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">use_rec_eye_mask</span></code> (bool): Whether to use the recurrent eye mask.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rec_mask</span></code> (torch.Tensor): The recurrent eye mask.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): The decay constant of the potential over time. See equation <a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): The activation threshold of the neuron.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension</span> <span class="pre">|</span> <span class="pre">~typing.Iterable[int</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Dimension]</span> <span class="pre">|</span> <span class="pre">~neurotorch.dimension.Size</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spike_func:</span> <span class="pre">~typing.Type[~neurotorch.modules.spike_funcs.SpikeFunction]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'neurotorch.modules.spike_funcs.HeavisideSigmoidApprox'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device:</span> <span class="pre">~torch.device</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau_m</strong> (<em>float</em>) – The decay time constant of the membrane potential which is generally 20 ms. See equation
<a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The activation threshold of the neuron.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor of the spikes.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.LIFLayer.update_regularization_loss">
<span class="sig-name descname"><span class="pre">update_regularization_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.LIFLayer.update_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the regularization loss for this layer. Each update call increments the regularization loss so at the end
the regularization loss will be the sum of all calls to this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> – The current state of the layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated regularization loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">SpyALIFLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a></p>
<p>The SpyALIF dynamic, inspired by Bellec and textit{al.} <span id="id13">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span> and bye the
<a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a> from the work of Neftci <span id="id14">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span>, is very
similar to the SpyLIF dynamics (class <a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a>). In fact, SpyALIF has exactly the same potential
update equation as SpyLIF. The difference comes
from the fact that the threshold potential varies with time and neuron input. Indeed, the threshold
is increased at each output spike and is then decreased with a certain rate in order to come back to
its starting threshold <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. The threshold equation from <a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a> is thus slightly
modified by changing <span class="math notranslate nohighlight">\(V_{\text{th}} \to A_j^t\)</span>. Thus, the output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>
denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is redefined by the equation <a class="reference internal" href="#equation-alif-z">(27)</a>.</p>
<div class="math notranslate nohighlight" id="equation-spyalif-i">
<span class="eqno">(14)<a class="headerlink" href="#equation-spyalif-i" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    I_{\text{syn}, j}^{t+\Delta t} = \alpha I_{ ext{syn}, j}^{t} + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t
    + \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyalif-v">
<span class="eqno">(15)<a class="headerlink" href="#equation-spyalif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\beta V_j^t + I_{\text{syn}, j}^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyalif-alpha">
<span class="eqno">(16)<a class="headerlink" href="#equation-spyalif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_{\text{syn}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<div class="math notranslate nohighlight" id="equation-spyalif-beta">
<span class="eqno">(17)<a class="headerlink" href="#equation-spyalif-beta" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \beta = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-spyalif-z">(40)</a> .</p>
<div class="math notranslate nohighlight" id="equation-spyalif-z">
<span class="eqno">(18)<a class="headerlink" href="#equation-spyalif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - A_j^t)\]</div>
<p>where <span class="math notranslate nohighlight">\(A_j^t\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.
The update of the activation threshold is then described by <a class="reference internal" href="#equation-alif-a">(42)</a>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(19)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    A_j^t = V_{\text{th}} + \kappa a_j^t
\end{equation}\]</div>
<p>with the adaptation variable <span class="math notranslate nohighlight">\(a_j^t\)</span> described by <a class="reference internal" href="#equation-alif-a">(43)</a> and <span class="math notranslate nohighlight">\(\kappa\)</span> an amplification
factor greater than 1 and typically equivalent to <span class="math notranslate nohighlight">\(\kappa\approx 1.6\)</span> <span id="id15">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(20)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    a_j^{t+1} = \rho a_j + z_j^t
\end{equation}\]</div>
<p>With the decay factor <span class="math notranslate nohighlight">\(\rho\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-alif-rho">
<span class="eqno">(21)<a class="headerlink" href="#equation-alif-rho" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \rho = e^{-\frac{\Delta t}{\tau_a}}
\end{equation}\]</div>
<p>SpyTorch library: <a class="reference external" href="https://github.com/surrogate-gradient-learning/spytorch">https://github.com/surrogate-gradient-learning/spytorch</a>.</p>
<p>The variables of the equations <a class="reference internal" href="#equation-spyalif-i">(36)</a> and <a class="reference internal" href="#equation-spyalif-v">(37)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(I_{\text{syn}, j}^{t}\)</span> is the synaptic current of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spyalif-alpha">(38)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> (torch.nn.Parameter): Decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spyalif-beta">(39)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): Activation threshold of the neuron (<span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): Slope of the Heaviside function (<span class="math notranslate nohighlight">\(\gamma\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">kappa</span></code>: The amplification factor of the threshold potential (<span class="math notranslate nohighlight">\(\kappa\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rho</span></code>: The decay factor of the adaptation variable (<span class="math notranslate nohighlight">\(\rho\)</span>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the SpyLIF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the input.</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the output.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use the recurrent connection.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use the recurrent eye mask.</p></li>
<li><p><strong>spike_func</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The spike function to use.</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – The learning type to use.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation).</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device to use.</p></li>
<li><p><strong>kwargs</strong> – The keyword arguments for the layer.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>tau_syn</strong> (<em>float</em>) – The synaptic time constant <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span>. Default: 5.0 * dt.</p></li>
<li><p><strong>tau_mem</strong> (<em>float</em>) – The membrane time constant <span class="math notranslate nohighlight">\(\tau_{\text{mem}}\)</span>. Default: 10.0 * dt.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold potential <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. Default: 1.0.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The multiplier of the derivative of the spike function <span class="math notranslate nohighlight">\(\gamma\)</span>. Default: 100.0.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor for the spikes. Higher this factor is,
the more the network will tend to spike less. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[synaptic current of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.reset_regularization_loss">
<span class="sig-name descname"><span class="pre">reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the regularization loss to zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyALIFLayer.update_regularization_loss">
<span class="sig-name descname"><span class="pre">update_regularization_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyALIFLayer.update_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the regularization loss for this layer. Each update call increments the regularization loss so at the end
the regularization loss will be the sum of all calls to this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> – The current state of the layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated regularization loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking.</span></span><span class="sig-name descname"><span class="pre">SpyLIFLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>The SpyLIF dynamics is a more complex variant of the LIF dynamics (class <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a>) allowing it to have a
greater power of expression. This variant is also inspired by Neftci <span id="id16">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span> and also
contains  two differential equations like the SpyLI dynamics <code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLI</span></code>. The equation <a class="reference internal" href="#equation-spylif-i">(45)</a> presents
the synaptic current update equation with euler integration while the equation <a class="reference internal" href="#equation-spylif-v">(46)</a> presents the
synaptic potential update.</p>
<div class="math notranslate nohighlight" id="equation-spylif-i">
<span class="eqno">(22)<a class="headerlink" href="#equation-spylif-i" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    I_{\text{syn}, j}^{t+\Delta t} = \alpha I_{ ext{syn}, j}^{t} + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t
    + \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spylif-v">
<span class="eqno">(23)<a class="headerlink" href="#equation-spylif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\beta V_j^t + I_{\text{syn}, j}^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spylif-alpha">
<span class="eqno">(24)<a class="headerlink" href="#equation-spylif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_{\text{syn}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<div class="math notranslate nohighlight" id="equation-spylif-beta">
<span class="eqno">(25)<a class="headerlink" href="#equation-spylif-beta" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \beta = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-spylif-z">(49)</a> .</p>
<div class="math notranslate nohighlight" id="equation-spylif-z">
<span class="eqno">(26)<a class="headerlink" href="#equation-spylif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - V_{\text{th}})\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.</p>
<p>SpyTorch library: <a class="reference external" href="https://github.com/surrogate-gradient-learning/spytorch">https://github.com/surrogate-gradient-learning/spytorch</a>.</p>
<p>The variables of the equations <a class="reference internal" href="#equation-spylif-i">(45)</a> and <a class="reference internal" href="#equation-spylif-v">(46)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(I_{\text{syn}, j}^{t}\)</span> is the synaptic current of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> (torch.nn.Parameter): Decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): Activation threshold of the neuron (<span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): Slope of the Heaviside function (<span class="math notranslate nohighlight">\(\gamma\)</span>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the SpyLIF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the input.</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the output.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use the recurrent connection.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use the recurrent eye mask.</p></li>
<li><p><strong>spike_func</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The spike function to use.</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – The learning type to use.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation).</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device to use.</p></li>
<li><p><strong>kwargs</strong> – The keyword arguments for the layer.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>tau_syn</strong> (<em>float</em>) – The synaptic time constant <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span>. Default: 5.0 * dt.</p></li>
<li><p><strong>tau_mem</strong> (<em>float</em>) – The membrane time constant <span class="math notranslate nohighlight">\(\tau_{\text{mem}}\)</span>. Default: 10.0 * dt.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold potential <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. Default: 1.0.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The multiplier of the derivative of the spike function <span class="math notranslate nohighlight">\(\gamma\)</span>. Default: 100.0.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor for the spikes. Higher this factor is,
the more the network will tend to spike less. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[synaptic current of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.reset_regularization_loss">
<span class="sig-name descname"><span class="pre">reset_regularization_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.reset_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the regularization loss to zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking.SpyLIFLayer.update_regularization_loss">
<span class="sig-name descname"><span class="pre">update_regularization_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking.SpyLIFLayer.update_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the regularization loss for this layer. Each update call increments the regularization loss so at the end
the regularization loss will be the sum of all calls to this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> – The current state of the layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated regularization loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers.spiking_lpf">
<span id="neurotorch-modules-layers-spiking-lpf-module"></span><h2>neurotorch.modules.layers.spiking_lpf module<a class="headerlink" href="#module-neurotorch.modules.layers.spiking_lpf" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking_lpf.</span></span><span class="sig-name descname"><span class="pre">ALIFLayerLPF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.ALIFLayer" title="neurotorch.modules.layers.spiking.ALIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ALIFLayer</span></code></a></p>
<p>The ALIF dynamic, inspired by Bellec and textit{al.} <span id="id17">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>, is very
similar to the LIF dynamics (class <code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code>). In fact, ALIF has exactly the same potential
update equation as LIF. The difference comes
from the fact that the threshold potential varies with time and neuron input. Indeed, the threshold
is increased at each output pulse and is then decreased with a certain rate in order to come back to
its starting threshold <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. The threshold equation from <code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code> is thus slightly
modified by changing <span class="math notranslate nohighlight">\(V_{\text{th}} \to A_j^t\)</span>. Thus, the output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>
denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is redefined by the equation <a class="reference internal" href="#equation-alif-z">(27)</a>.</p>
<p>In this version (LPF), the spikes are filtered with a low pass filter (LPF) described by the equation
<a class="reference internal" href="#equation-lpf">(50)</a>.</p>
<div class="math notranslate nohighlight" id="equation-alif-z">
<span class="eqno">(27)<a class="headerlink" href="#equation-alif-z" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    z_j^t = H(V_j^t - A_j^t)
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-lpf">
<span class="eqno">(28)<a class="headerlink" href="#equation-lpf" title="Permalink to this equation">¶</a></span>\[\mathcal{F}_{\text{lpf}-\alpha}(z_j^t) = {\text{lpf}-\alpha} \mathcal{F}_\alpha(z_j^{t-1}) + z_j^t\]</div>
<p>The update of the activation threshold is then described by <a class="reference internal" href="#equation-alif-a">(42)</a>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(29)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    A_j^t = V_{\text{th}} + \beta a_j^t
\end{equation}\]</div>
<p>with the adaptation variable <span class="math notranslate nohighlight">\(a_j^t\)</span> described by <a class="reference internal" href="#equation-alif-a">(43)</a> and <span class="math notranslate nohighlight">\(\beta\)</span> an amplification
factor greater than 1 and typically equivalent to <span class="math notranslate nohighlight">\(\beta\approx 1.6\)</span> <span id="id18">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(30)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    a_j^{t+1} = \rho a_j + z_j^t
\end{equation}\]</div>
<p>With the decay factor <span class="math notranslate nohighlight">\(\rho\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-alif-rho">
<span class="eqno">(31)<a class="headerlink" href="#equation-alif-rho" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \rho = e^{-\frac{\Delta t}{\tau_a}}
\end{equation}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code>: The amplification factor of the threshold potential <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rho</span></code>: The decay factor of the adaptation variable <span class="math notranslate nohighlight">\(\rho\)</span>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">lpf_alpha</span></code> (float): Decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the ALIFLayerLPF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>lpf_alpha</strong> (<em>float</em>) – The decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).
Default: np.exp(-dt / tau_mem).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[current threshold of shape (batch_size, self.output_size)],
[low pass filtered spikes of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.ALIFLayerLPF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.LIFLayerLPF">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking_lpf.</span></span><span class="sig-name descname"><span class="pre">LIFLayerLPF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.LIFLayer" title="neurotorch.modules.layers.spiking.LIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code></a></p>
<p>LIF dynamics, inspired by <span id="id19">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span> , <span id="id20">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span> , models the synaptic
potential and impulses of a neuron over time. The shape of this potential is not considered realistic
<span id="id21">Izhikevich [<a class="reference internal" href="neurotorch.modules.html#id10" title="Eugene M. Izhikevich. Dynamical Systems in Neuroscience. MIT Press, 2007. ISBN 978-0-262-09043-8.">Izh07</a>]</span> , but the time at which the potential exceeds the threshold is.
This potential is found by the recurrent equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p>
<p>In this version (LPF), the spikes are filtered with a low pass filter (LPF) described by the equation
<a class="reference internal" href="#equation-lpf">(50)</a>.</p>
<div class="math notranslate nohighlight" id="equation-lif-v">
<span class="eqno">(32)<a class="headerlink" href="#equation-lif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\alpha V_j^t + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t +
    \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<p>The variables of the equation <a class="reference internal" href="#equation-lif-v">(32)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the potential over time (equation <a class="reference internal" href="#equation-lif-alpha">(33)</a> ).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-lif-alpha">
<span class="eqno">(33)<a class="headerlink" href="#equation-lif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_m}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_m\)</span> being the decay time constant of the membrane potential which is generally 20 ms.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-lif-z">(34)</a> .</p>
<div class="math notranslate nohighlight" id="equation-lif-z">
<span class="eqno">(34)<a class="headerlink" href="#equation-lif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - V_{\text{th}})\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.</p>
<div class="math notranslate nohighlight" id="equation-lpf">
<span class="eqno">(35)<a class="headerlink" href="#equation-lpf" title="Permalink to this equation">¶</a></span>\[\mathcal{F}_{\text{lpf}-\alpha}(z_j^t) = {\text{lpf}-\alpha} \mathcal{F}_\alpha(z_j^{t-1}) + z_j^t\]</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_weights</span></code> (torch.nn.Parameter): The weights used to compute the output of the layer <span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">recurrent_weights</span></code> (torch.nn.Parameter): The weights used to compute the hidden state of the layer <span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dt</span></code> (float): The time step of the layer <span class="math notranslate nohighlight">\(\Delta t\)</span> in equation <a class="reference internal" href="#equation-lif-v">(32)</a>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">use_rec_eye_mask</span></code> (bool): Whether to use the recurrent eye mask.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rec_mask</span></code> (torch.Tensor): The recurrent eye mask.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): The decay constant of the potential over time. See equation <a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): The activation threshold of the neuron.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">lpf_alpha</span></code> (float): Decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the LIFLayerLPF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau_m</strong> (<em>float</em>) – The decay time constant of the membrane potential which is generally 20 ms. See equation
<a class="reference internal" href="#equation-lif-alpha">(33)</a> .</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The activation threshold of the neuron.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The gain of the neuron. The gain will increase the gradient of the neuron’s output.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor of the spikes.</p></li>
<li><p><strong>lpf_alpha</strong> (<em>float</em>) – The decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).
Default: np.exp(-dt / tau_mem).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[low pass filtered spikes of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.LIFLayerLPF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking_lpf.</span></span><span class="sig-name descname"><span class="pre">SpyALIFLayerLPF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyALIFLayer" title="neurotorch.modules.layers.spiking.SpyALIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyALIFLayer</span></code></a></p>
<p>The SpyALIF dynamic, inspired by Bellec and textit{al.} <span id="id22">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span> and bye the
<code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code> from the work of Neftci <span id="id23">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span>, is very
similar to the SpyLIF dynamics (class <code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code>). In fact, SpyALIF has exactly the same potential
update equation as SpyLIF. The difference comes
from the fact that the threshold potential varies with time and neuron input. Indeed, the threshold
is increased at each output spike and is then decreased with a certain rate in order to come back to
its starting threshold <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. The threshold equation from <code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code> is thus slightly
modified by changing <span class="math notranslate nohighlight">\(V_{\text{th}} \to A_j^t\)</span>. Thus, the output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>
denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is redefined by the equation <a class="reference internal" href="#equation-alif-z">(27)</a>.</p>
<p>In this version (LPF), the spikes are filtered with a low pass filter (LPF) described by the equation
<a class="reference internal" href="#equation-lpf">(50)</a>.</p>
<div class="math notranslate nohighlight" id="equation-spyalif-i">
<span class="eqno">(36)<a class="headerlink" href="#equation-spyalif-i" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    I_{\text{syn}, j}^{t+\Delta t} = \alpha I_{ ext{syn}, j}^{t} + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t
    + \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyalif-v">
<span class="eqno">(37)<a class="headerlink" href="#equation-spyalif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\beta V_j^t + I_{\text{syn}, j}^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spyalif-alpha">
<span class="eqno">(38)<a class="headerlink" href="#equation-spyalif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_{\text{syn}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<div class="math notranslate nohighlight" id="equation-spyalif-beta">
<span class="eqno">(39)<a class="headerlink" href="#equation-spyalif-beta" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \beta = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-spyalif-z">(40)</a> .</p>
<div class="math notranslate nohighlight" id="equation-spyalif-z">
<span class="eqno">(40)<a class="headerlink" href="#equation-spyalif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - A_j^t)\]</div>
<p>where <span class="math notranslate nohighlight">\(A_j^t\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.
The update of the activation threshold is then described by <a class="reference internal" href="#equation-alif-a">(42)</a>.</p>
<div class="math notranslate nohighlight" id="equation-lpf">
<span class="eqno">(41)<a class="headerlink" href="#equation-lpf" title="Permalink to this equation">¶</a></span>\[\mathcal{F}_{\text{lpf}-\alpha}(z_j^t) = {\text{lpf}-\alpha} \mathcal{F}_\alpha(z_j^{t-1}) + z_j^t\]</div>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(42)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    A_j^t = V_{\text{th}} + \kappa a_j^t
\end{equation}\]</div>
<p>with the adaptation variable <span class="math notranslate nohighlight">\(a_j^t\)</span> described by <a class="reference internal" href="#equation-alif-a">(43)</a> and <span class="math notranslate nohighlight">\(\kappa\)</span> an amplification
factor greater than 1 and typically equivalent to <span class="math notranslate nohighlight">\(\kappa\approx 1.6\)</span> <span id="id24">Bellec <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id11" title="Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11(1):3625, 2020. URL: https://www.nature.com/articles/s41467-020-17236-y (visited on 2021-12-18), doi:10.1038/s41467-020-17236-y.">BSS+20</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-alif-a">
<span class="eqno">(43)<a class="headerlink" href="#equation-alif-a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    a_j^{t+1} = \rho a_j + z_j^t
\end{equation}\]</div>
<p>With the decay factor <span class="math notranslate nohighlight">\(\rho\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-alif-rho">
<span class="eqno">(44)<a class="headerlink" href="#equation-alif-rho" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \rho = e^{-\frac{\Delta t}{\tau_a}}
\end{equation}\]</div>
<p>SpyTorch library: <a class="reference external" href="https://github.com/surrogate-gradient-learning/spytorch">https://github.com/surrogate-gradient-learning/spytorch</a>.</p>
<p>The variables of the equations <a class="reference internal" href="#equation-spyalif-i">(36)</a> and <a class="reference internal" href="#equation-spyalif-v">(37)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(I_{\text{syn}, j}^{t}\)</span> is the synaptic current of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spyalif-alpha">(38)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> (torch.nn.Parameter): Decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spyalif-beta">(39)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): Activation threshold of the neuron (<span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): Slope of the Heaviside function (<span class="math notranslate nohighlight">\(\gamma\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">kappa</span></code>: The amplification factor of the threshold potential (<span class="math notranslate nohighlight">\(\kappa\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">rho</span></code>: The decay factor of the adaptation variable (<span class="math notranslate nohighlight">\(\rho\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">lpf_alpha</span></code> (float): Decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the SpyALIFLayerLPF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the input.</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the output.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use the recurrent connection.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use the recurrent eye mask.</p></li>
<li><p><strong>spike_func</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The spike function to use.</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – The learning type to use.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation).</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device to use.</p></li>
<li><p><strong>kwargs</strong> – The keyword arguments for the layer.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>tau_syn</strong> (<em>float</em>) – The synaptic time constant <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span>. Default: 5.0 * dt.</p></li>
<li><p><strong>tau_mem</strong> (<em>float</em>) – The membrane time constant <span class="math notranslate nohighlight">\(\tau_{\text{mem}}\)</span>. Default: 10.0 * dt.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold potential <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. Default: 1.0.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The multiplier of the derivative of the spike function <span class="math notranslate nohighlight">\(\gamma\)</span>. Default: 100.0.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor for the spikes. Higher this factor is,
the more the network will tend to spike less. Default: 0.0.</p></li>
<li><p><strong>lpf_alpha</strong> (<em>float</em>) – The decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).
Default: np.exp(-dt / tau_mem).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[synaptic current of shape (batch_size, self.output_size)],
[current threshold of shape (batch_size, self.output_size)],
[low pass filtered spikes of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyALIFLayerLPF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.spiking_lpf.</span></span><span class="sig-name descname"><span class="pre">SpyLIFLayerLPF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.spiking.SpyLIFLayer" title="neurotorch.modules.layers.spiking.SpyLIFLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLIFLayer</span></code></a></p>
<p>The SpyLIF dynamics is a more complex variant of the LIF dynamics (class <code class="xref py py-class docutils literal notranslate"><span class="pre">LIFLayer</span></code>) allowing it to have a
greater power of expression. This variant is also inspired by Neftci <span id="id25">Neftci <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id7" title="Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. Conference Name: IEEE Signal Processing Magazine. doi:10.1109/MSP.2019.2931595.">NMZ19</a>]</span> and also
contains  two differential equations like the SpyLI dynamics <code class="xref py py-class docutils literal notranslate"><span class="pre">SpyLI</span></code>. The equation <a class="reference internal" href="#equation-spylif-i">(45)</a> presents
the synaptic current update equation with euler integration while the equation <a class="reference internal" href="#equation-spylif-v">(46)</a> presents the
synaptic potential update.</p>
<p>In this version (LPF), the spikes are filtered with a low pass filter (LPF) described by the equation
<a class="reference internal" href="#equation-lpf">(50)</a>.</p>
<div class="math notranslate nohighlight" id="equation-spylif-i">
<span class="eqno">(45)<a class="headerlink" href="#equation-spylif-i" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    I_{\text{syn}, j}^{t+\Delta t} = \alpha I_{ ext{syn}, j}^{t} + \sum_{i}^{N} W_{ij}^{\text{rec}} z_i^t
    + \sum_i^{N} W_{ij}^{\text{in}} x_i^{t+\Delta t}
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spylif-v">
<span class="eqno">(46)<a class="headerlink" href="#equation-spylif-v" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    V_j^{t+\Delta t} = \left(\beta V_j^t + I_{\text{syn}, j}^{t+\Delta t}\right) \left(1 - z_j^t\right)
\end{equation}\]</div>
<div class="math notranslate nohighlight" id="equation-spylif-alpha">
<span class="eqno">(47)<a class="headerlink" href="#equation-spylif-alpha" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \alpha = e^{-\frac{\Delta t}{\tau_{\text{syn}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<div class="math notranslate nohighlight" id="equation-spylif-beta">
<span class="eqno">(48)<a class="headerlink" href="#equation-spylif-beta" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \beta = e^{-\frac{\Delta t}{\tau_{\text{mem}}}}
\end{equation}\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span> being the decay time constant of the synaptic current.</p>
<p>The output of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> denoted <span class="math notranslate nohighlight">\(z_j^t\)</span> is defined by the equation <a class="reference internal" href="#equation-spylif-z">(49)</a> .</p>
<div class="math notranslate nohighlight" id="equation-spylif-z">
<span class="eqno">(49)<a class="headerlink" href="#equation-spylif-z" title="Permalink to this equation">¶</a></span>\[z_j^t = H(V_j^t - V_{\text{th}})\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span> denotes the activation threshold of the neuron and the function <span class="math notranslate nohighlight">\(H(\cdot)\)</span>
is the Heaviside function defined as <span class="math notranslate nohighlight">\(H(x) = 1\)</span> if <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(H(x) = 0\)</span> otherwise.</p>
<div class="math notranslate nohighlight" id="equation-lpf">
<span class="eqno">(50)<a class="headerlink" href="#equation-lpf" title="Permalink to this equation">¶</a></span>\[\mathcal{F}_{\text{lpf}-\alpha}(z_j^t) = {\text{lpf}-\alpha} \mathcal{F}_\alpha(z_j^{t-1}) + z_j^t\]</div>
<p>SpyTorch library: <a class="reference external" href="https://github.com/surrogate-gradient-learning/spytorch">https://github.com/surrogate-gradient-learning/spytorch</a>.</p>
<p>The variables of the equations <a class="reference internal" href="#equation-spylif-i">(45)</a> and <a class="reference internal" href="#equation-spylif-v">(46)</a> are described by the following definitions:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of neurons in the layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(I_{\text{syn}, j}^{t}\)</span> is the synaptic current of neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V_j^t\)</span> is the synaptic potential of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta t\)</span> is the integration time step.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_j^t\)</span> is the spike of the neuron <span class="math notranslate nohighlight">\(j\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{rec}}\)</span> is the recurrent weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}^{\text{in}}\)</span> is the input weight of the neuron <span class="math notranslate nohighlight">\(i\)</span> to the neuron <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i^{t}\)</span> is the input of the neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (torch.nn.Parameter): Decay constant of the synaptic current over time (equation <a class="reference internal" href="#equation-spylif-alpha">(47)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> (torch.nn.Parameter): Decay constant of the membrane potential over time (equation <a class="reference internal" href="#equation-spylif-beta">(48)</a>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">threshold</span></code> (torch.nn.Parameter): Activation threshold of the neuron (<span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">gamma</span></code> (torch.nn.Parameter): Slope of the Heaviside function (<span class="math notranslate nohighlight">\(\gamma\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">lpf_alpha</span></code> (float): Decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_rec_eye_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor for the SpyLIFLayerLPF layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the input.</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – The size of the output.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the layer.</p></li>
<li><p><strong>use_recurrent_connection</strong> (<em>bool</em>) – Whether to use the recurrent connection.</p></li>
<li><p><strong>use_rec_eye_mask</strong> (<em>bool</em>) – Whether to use the recurrent eye mask.</p></li>
<li><p><strong>spike_func</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The spike function to use.</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – The learning type to use.</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation).</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) – The device to use.</p></li>
<li><p><strong>kwargs</strong> – The keyword arguments for the layer.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>tau_syn</strong> (<em>float</em>) – The synaptic time constant <span class="math notranslate nohighlight">\(\tau_{\text{syn}}\)</span>. Default: 5.0 * dt.</p></li>
<li><p><strong>tau_mem</strong> (<em>float</em>) – The membrane time constant <span class="math notranslate nohighlight">\(\tau_{\text{mem}}\)</span>. Default: 10.0 * dt.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold potential <span class="math notranslate nohighlight">\(V_{\text{th}}\)</span>. Default: 1.0.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The multiplier of the derivative of the spike function <span class="math notranslate nohighlight">\(\gamma\)</span>. Default: 100.0.</p></li>
<li><p><strong>spikes_regularization_factor</strong> (<em>float</em>) – The regularization factor for the spikes. Higher this factor is,
the more the network will tend to spike less. Default: 0.0.</p></li>
<li><p><strong>lpf_alpha</strong> (<em>float</em>) – The decay constant of the low pass filter over time (equation <a class="reference internal" href="#equation-lpf">(50)</a>).
Default: np.exp(-dt / tau_mem).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Create an empty state in the following form:</dt><dd><p>([membrane potential of shape (batch_size, self.output_size)],
[synaptic current of shape (batch_size, self.output_size)],
[low pass filtered spikes of shape (batch_size, self.output_size)],
[spikes of shape (batch_size, self.output_size)])</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> – The size of the current batch.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.spiking_lpf.SpyLIFLayerLPF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers.wilson_cowan">
<span id="neurotorch-modules-layers-wilson-cowan-module"></span><h2>neurotorch.modules.layers.wilson_cowan module<a class="headerlink" href="#module-neurotorch.modules.layers.wilson_cowan" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.wilson_cowan.</span></span><span class="sig-name descname"><span class="pre">WilsonCowanCURBDLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer" title="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">WilsonCowanLayer</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – size of the input</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – size of the output
If we are predicting time series -&gt; input_size = output_size</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – Type of learning for the gradient descent</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation)</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – device for computation</p></li>
<li><p><strong>kwargs</strong> – Additional parameters for the Wilson-Cowan dynamic.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>forward_weights</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>np.ndarray</em><em>]</em>) – Forward weights of the layer.</p></li>
<li><p><strong>std_weight</strong> (<em>float</em>) – Instability of the initial random matrix.</p></li>
<li><p><strong>mu</strong> (<em>Union</em><em>[</em><em>float</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Activation threshold. If torch.Tensor -&gt; shape (1, number of neurons).</p></li>
<li><p><strong>mean_mu</strong> (<em>float</em>) – Mean of the activation threshold (if learn_mu is True).</p></li>
<li><p><strong>std_mu</strong> (<em>float</em>) – Standard deviation of the activation threshold (if learn_mu is True).</p></li>
<li><p><strong>learn_mu</strong> (<em>bool</em>) – Whether to train the activation threshold.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – Decay constant of RNN unit.</p></li>
<li><p><strong>learn_tau</strong> (<em>bool</em>) – Whether to train the decay constant.</p></li>
<li><p><strong>r</strong> (<em>float</em>) – Transition rate of the RNN unit. If torch.Tensor -&gt; shape (1, number of neurons).</p></li>
<li><p><strong>mean_r</strong> (<em>float</em>) – Mean of the transition rate (if learn_r is True).</p></li>
<li><p><strong>std_r</strong> (<em>float</em>) – Standard deviation of the transition rate (if learn_r is True).</p></li>
<li><p><strong>learn_r</strong> (<em>bool</em>) – Whether to train the transition rate.</p></li>
</ul>
</dd>
</dl>
<p>Remarks: Parameter mu and r can only be a parameter as a vector.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanCURBDLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.
With Euler discretisation, Wilson-Cowan equation becomes:</p>
<p>output = input * (1 - dt/tau) + dt/tau * (1 - input &#64; r) * sigmoid(input &#64; forward_weight - mu)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – time series at a time t of shape (batch_size, number of neurons)
Remark: if you use to compute a time series, use batch_size = 1.</p></li>
<li><p><strong>state</strong> (<em>Optional</em><em>[</em><em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>...</em><em>]</em><em>]</em>) – State of the layer (only for SNN -&gt; not use for RNN)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(time series at a time t+1, State of the layer -&gt; None)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, Tuple[torch.Tensor, …]]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.wilson_cowan.</span></span><span class="sig-name descname"><span class="pre">WilsonCowanLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#neurotorch.modules.layers.base.BaseNeuronsLayer" title="neurotorch.modules.layers.base.BaseNeuronsLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseNeuronsLayer</span></code></a></p>
<p>This layer is use for Wilson-Cowan neuronal dynamics.
This dynamic is also referred to as firing rate model.
Wilson-Cowan dynamic is great for neuronal calcium activity.
This layer use recurrent neural network (RNN).
The number of parameters that are trained is N^2 (+2N if mu and r is train)
where N is the number of neurons.</p>
<p>For references, please read:</p>
<blockquote>
<div><ul class="simple">
<li><p>Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons <span id="id26">Wilson and Cowan [<a class="reference internal" href="neurotorch.modules.html#id44" title="Hugh R Wilson and Jack D Cowan. Excitatory and inhibitory interactions in localized populations of model neurons. Biophysical journal, 12(1):1–24, 1972.">WC72</a>]</span></p></li>
<li><p>Beyond Wilson-Cowan dynamics: oscillations and chaos without inhibitions <span id="id27">Painchaud <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id45" title="Vincent Painchaud, Nicolas Doyon, and Patrick Desrosiers. Beyond wilson-cowan dynamics: oscillations and chaos without inhibition. 2022. URL: https://arxiv.org/abs/2204.00583, doi:10.48550/ARXIV.2204.00583.">PDD22</a>]</span></p></li>
<li><p>Neural Network dynamic <span id="id28">Vogels <em>et al.</em> [<a class="reference internal" href="neurotorch.modules.html#id46" title="Tim P. Vogels, Kanaka Rajan, and L.F. Abbott. Neural network dynamics. Annual Review of Neuroscience, 28(1):357-376, 2005. PMID: 16022600. URL: https://doi.org/10.1146/annurev.neuro.28.061604.135637, arXiv:https://doi.org/10.1146/annurev.neuro.28.061604.135637, doi:10.1146/annurev.neuro.28.061604.135637.">VRA05</a>]</span>.</p></li>
</ul>
</div></blockquote>
<p>The Wilson-Cowan dynamic is one of many dynamical models that can be used
to model neuronal activity. To explore more continuous and Non-linear dynamics,
please read Nonlinear Neural Network: Principles, Mechanisms, and Architecture <span id="id29">Grossberg [<a class="reference internal" href="neurotorch.modules.html#id47" title="Stephen Grossberg. Nonlinear neural networks: principles, mechanisms, and architectures. Neural Networks, 1(1):17-61, 1988. URL: https://www.sciencedirect.com/science/article/pii/0893608088900214, doi:https://doi.org/10.1016/0893-6080(88)90021-4.">Gro88</a>]</span>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Dimension" title="neurotorch.dimension.Dimension"><span class="pre">Dimension</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="neurotorch.html#neurotorch.dimension.Size" title="neurotorch.dimension.Size"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_recurrent_connection</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – size of the input</p></li>
<li><p><strong>output_size</strong> (<em>Optional</em><em>[</em><em>SizeTypes</em><em>]</em>) – size of the output
If we are predicting time series -&gt; input_size = output_size</p></li>
<li><p><strong>learning_type</strong> (<em>LearningType</em>) – Type of learning for the gradient descent</p></li>
<li><p><strong>dt</strong> (<em>float</em>) – Time step (Euler’s discretisation)</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – device for computation</p></li>
<li><p><strong>kwargs</strong> – Additional parameters for the Wilson-Cowan dynamic.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>forward_weights</strong> (<em>Union</em><em>[</em><em>torch.Tensor</em><em>, </em><em>np.ndarray</em><em>]</em>) – Forward weights of the layer.</p></li>
<li><p><strong>std_weight</strong> (<em>float</em>) – Instability of the initial random matrix.</p></li>
<li><p><strong>mu</strong> (<em>Union</em><em>[</em><em>float</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Activation threshold. If torch.Tensor -&gt; shape (1, number of neurons).</p></li>
<li><p><strong>mean_mu</strong> (<em>float</em>) – Mean of the activation threshold (if learn_mu is True).</p></li>
<li><p><strong>std_mu</strong> (<em>float</em>) – Standard deviation of the activation threshold (if learn_mu is True).</p></li>
<li><p><strong>learn_mu</strong> (<em>bool</em>) – Whether to train the activation threshold.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – Decay constant of RNN unit.</p></li>
<li><p><strong>learn_tau</strong> (<em>bool</em>) – Whether to train the decay constant.</p></li>
<li><p><strong>r</strong> (<em>float</em>) – Transition rate of the RNN unit. If torch.Tensor -&gt; shape (1, number of neurons).</p></li>
<li><p><strong>mean_r</strong> (<em>float</em>) – Mean of the transition rate (if learn_r is True).</p></li>
<li><p><strong>std_r</strong> (<em>float</em>) – Standard deviation of the transition rate (if learn_r is True).</p></li>
<li><p><strong>learn_r</strong> (<em>bool</em>) – Whether to train the transition rate.</p></li>
</ul>
</dd>
</dl>
<p>Remarks: Parameter mu and r can only be a parameter as a vector.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.create_empty_state">
<span class="sig-name descname"><span class="pre">create_empty_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.create_empty_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an empty state for the layer. This method must be implemented by the child class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>batch_size</strong> (<em>int</em>) – The batch size of the state.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The empty state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.
With Euler discretisation, Wilson-Cowan equation becomes:</p>
<p>output = input * (1 - dt/tau) + dt/tau * (1 - input &#64; r) * sigmoid(input &#64; forward_weight - mu)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>torch.Tensor</em>) – time series at a time t of shape (batch_size, number of neurons)
Remark: if you use to compute a time series, use batch_size = 1.</p></li>
<li><p><strong>state</strong> (<em>Optional</em><em>[</em><em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>...</em><em>]</em><em>]</em>) – State of the layer (only for SNN -&gt; not use for RNN)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(time series at a time t+1, State of the layer -&gt; None)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, Tuple[torch.Tensor, …]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.initialize_weights_">
<span class="sig-name descname"><span class="pre">initialize_weights_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.initialize_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the parameters (weights) that will be trained.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.r">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">r</span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.r" title="Permalink to this definition">¶</a></dt>
<dd><p>This property is used to ensure that the transition rate will never be negative if trained.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.tau">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tau</span></span><a class="headerlink" href="#neurotorch.modules.layers.wilson_cowan.WilsonCowanLayer.tau" title="Permalink to this definition">¶</a></dt>
<dd><p>This property is used to ensure that the decay constant will never be negative if trained.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-neurotorch.modules.layers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-neurotorch.modules.layers" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neurotorch.modules.layers.</span></span><span class="sig-name descname"><span class="pre">LayerType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neurotorch.modules.layers.LayerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.ALIF">
<span class="sig-name descname"><span class="pre">ALIF</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.ALIF" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.Izhikevich">
<span class="sig-name descname"><span class="pre">Izhikevich</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.Izhikevich" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.LI">
<span class="sig-name descname"><span class="pre">LI</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">3</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.LI" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.LIF">
<span class="sig-name descname"><span class="pre">LIF</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.LIF" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.SpyALIF">
<span class="sig-name descname"><span class="pre">SpyALIF</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">6</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.SpyALIF" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.SpyLI">
<span class="sig-name descname"><span class="pre">SpyLI</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">5</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.SpyLI" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.SpyLIF">
<span class="sig-name descname"><span class="pre">SpyLIF</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">4</span></em><a class="headerlink" href="#neurotorch.modules.layers.LayerType.SpyLIF" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neurotorch.modules.layers.LayerType.from_str">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#neurotorch.modules.layers.LayerType" title="neurotorch.modules.layers.LayerType"><span class="pre">LayerType</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#neurotorch.modules.layers.LayerType.from_str" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the LayerType from a string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em>) – The name of the LayerType.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The LayerType.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[<a class="reference internal" href="#neurotorch.modules.layers.LayerType" title="neurotorch.modules.layers.LayerType">LayerType</a>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


                        
                    </div>
                </div>
            </div>
        </div>
    </div>    


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
    <script type="text/javascript" src="static/documentation_options.js"></script>
    <script type="text/javascript" src="static/doctools.js"></script>
    <script type="text/javascript" src="static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="static/js/theme.js"></script>
  
    <div class="footer" role="contentinfo">
        <div class="container">
            &#169; Copyright 2022, Jérémie Gince.
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1.
        </div>
    </div>  

</body>
</html>