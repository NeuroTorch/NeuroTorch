{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Heidelberg Tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will be learning how to use NeuroTorch to train a neural network to recognize audio recordings of spoken digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can install the dependencies by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r heidelberg_requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a cuda device and want to use it for this tutorial (it is recommended to do so), you can uninstall pytorch with `pip uninstall torch` and re-install it with the right cuda version by generating a command with [PyTorch GetStarted](https://pytorch.org/get-started/locally/) web page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the virtual environment, we will need to import the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from pythonbasictools.device import log_device_setup, DeepLib\n",
    "from pythonbasictools.logging import logs_file_setup\n",
    "\n",
    "from dataset import get_dataloaders\n",
    "import neurotorch as nt\n",
    "from neurotorch import Dimension, DimensionProperty\n",
    "from neurotorch.modules import HeavisideSigmoidApprox\n",
    "from neurotorch.callbacks import CheckpointManager, LoadCheckpointMode\n",
    "from neurotorch.metrics import ClassificationMetrics\n",
    "from neurotorch.modules import SequentialRNN\n",
    "from neurotorch.modules.layers import SpyLIFLayer, SpyLILayer\n",
    "from neurotorch.trainers import ClassificationTrainer\n",
    "from neurotorch.regularization import RegularizationList, L2, L1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Logs file at: .//logs/logs-08-11-2022/heidelberg_tutorial-1667921342172184.log\n",
      "\n",
      "INFO:root:__Python VERSION: 3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]\n",
      "INFO:root:Number of available cores: 12.\n",
      "INFO:root:Number of available logical processors: 24.\n",
      "INFO:root:__pyTorch VERSION:1.12.1+cu116\n",
      "INFO:root:__CUDA VERSION:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\r\n",
      "Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\r\n",
      "Cuda compilation tools, release 11.2, V11.2.67\r\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\r\n",
      "\n",
      "INFO:root:__nvidia-smi: Not Found\n",
      "INFO:root:__CUDNN VERSION:8302\n",
      "INFO:root:__Number CUDA Devices:1\n",
      "INFO:root:\n",
      "-------------------------\n",
      "DEVICE: cuda\n",
      "-------------------------\n",
      "\n",
      "INFO:root:NVIDIA GeForce RTX 3070\n",
      "INFO:root:Memory Usage:\n",
      "INFO:root:Allocated: 0.0 GB\n",
      "INFO:root:Cached:   0.0 GB\n",
      "INFO:root:Memory summary: \n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_file_setup(\"heidelberg_tutorial\", add_stdout=False)\n",
    "log_device_setup(deepLib=DeepLib.Pytorch)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.set_per_process_memory_fraction(0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's the time to defined our training and networks parameters. In the following,\n",
    "\n",
    "- the number of iteration is the number of time the trainer will pass through the entire training dataset;\n",
    "- the batch size is the number of samples that will be loaded at the same time for each forward and backward pass;\n",
    "- the learning rate is the learning rate used by the optimizer;\n",
    "- the random seed is the seed used by the random number generator;\n",
    "- the number of steps is the number of euler integration steps performed by the model during each forward and backward pass;\n",
    "- the number of hidden neurons is the number of neurons that will be used in the hidden layer of the model;\n",
    "- dt is the time step of the euler integration;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_iterations = 50\n",
    "batch_size = 256\n",
    "learning_rate = 2e-4\n",
    "seed = 42\n",
    "\n",
    "# Network parameters\n",
    "dynamic_type = nt.SpyLIFLayer\n",
    "n_steps = 100\n",
    "n_hidden_neurons = 256\n",
    "dt = 1e-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x2345937b330>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we're initializing a callback of the trainer used to save the network during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "checkpoint_folder = f\"./checkpoints/heidelberg_{dynamic_type.__name__}\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_folder, metric=\"val_accuracy\", minimise_metric=False, save_best_only=True, start_save_at=int(n_iterations * 0.9))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Here is an exemple of the Heidelberg dataset:\n",
    "![Heidelberg_exemples](images/heidelberg_exemples.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(\n",
    "\tbatch_size=batch_size,\n",
    "\ttrain_val_split_ratio=0.95,\n",
    "\tpin_memory=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can finally define our model. The sequential model is a neural network that is composed of a list of layers. The list of layers is ordered and the first layer is the input layer. The last layer is the output layer. The sequential model can be found in the subpackage `neurotorch.modules`. Note that we can specify the input transform that will be applied to the input data before the forward pass through the list of layers. If the transformation is computationally expensive, it is recommended to put it in the dataloaders instead of the model for the training. Usually the input transform in the model is used when the model is used for inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "network = SequentialRNN(\n",
    "\tlayers=[\n",
    "\t\tdynamic_type(\n",
    "\t\t\tinput_size=nt.Size([\n",
    "\t\t\t\tDimension(None, DimensionProperty.TIME),\n",
    "\t\t\t\tDimension(dataloaders[\"test\"].dataset.n_units, DimensionProperty.NONE)\n",
    "\t\t\t]),\n",
    "\t\t\toutput_size=n_hidden_neurons,\n",
    "\t\t\tuse_recurrent_connection=True,\n",
    "\t\t\tspike_func=HeavisideSigmoidApprox,\n",
    "\t\t\tdt=dt,\n",
    "\t\t),\n",
    "\t\tSpyLILayer(dt=dt, output_size=dataloaders[\"test\"].dataset.n_classes),\n",
    "\t],\n",
    "\tname=f\"heidelberg_network\",\n",
    "\thh_memory_size=1,\n",
    "\tcheckpoint_folder=checkpoint_folder,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the instantiation of the model, we have to build it, so it can infer the missing sizes of the layers in the model and create the tensors that will be used during the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialRNN(\n",
      "  (_to_device_transform): ToDevice(cuda, async=True)\n",
      "  (input_layers): ModuleDict(\n",
      "    (input_0): SpyLIFLayer<input_0>(700<->256)@cuda\n",
      "  )\n",
      "  (hidden_layers): ModuleList()\n",
      "  (output_layers): ModuleDict(\n",
      "    (output_0): SpyLILayer<output_0>(256->20)@cuda\n",
      "  )\n",
      "  (_memory_device_transform): ToDevice(cuda, async=True)\n",
      "  (input_transform): ModuleDict(\n",
      "    (input_0): Sequential(\n",
      "      (0): CallableToModuleWrapper(Compose(\n",
      "          ToTensor()\n",
      "      ))\n",
      "      (1): ToDevice(cuda, async=True)\n",
      "    )\n",
      "  )\n",
      "  (output_transform): ModuleDict(\n",
      "    (output_0): IdentityTransform()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network.build()\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "regularization = RegularizationList([\n",
    "\tL2(network.parameters(), Lambda=1e-5),\n",
    "\tL1(network.parameters(), Lambda=1e-6),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we create the trainer that will be used to trained our network. They are several types of trainers that can be found in the subpackage `neurotorch.trainers` like the `ClassificationTrainer`, the `RegressionTrainer` or the `PPOTrainer`. In this tutorial, we will use the `ClassificationTrainer` because we are trying to classify some images. Note that the callbacks are some object that will be called during the training. They are found in the subpackage `neurotorch.callbacks` like the `CheckpointManager`.\n",
    "\n",
    "In addition we will optimize our network with BPTT (backpropagation through time). The object BPTT from neurotorch is a callback, then we just have to add it to ou callbacks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "\tnt.BPTT(optimizer=torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0.0), criterion=torch.nn.NLLLoss()),\n",
    "\tcheckpoint_manager,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "trainer = ClassificationTrainer(\n",
    "\tpredict_method=\"get_prediction_log_proba\",\n",
    "\tmodel=network,\n",
    "\t# regularization=regularization,\n",
    "\t# regularization_optimizer=torch.optim.SGD(regularization.parameters(), lr=1e-2, weight_decay=0.0),\n",
    "\tcallbacks=callbacks,\n",
    "\tverbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training will start! Let's see how our simple network will perform on this task.\n",
    "\n",
    "Note: the argument `force_overwrite` is used to overwrite the existing checkpoint if it exists and the argument `load_checkpoint_mode` if the model will be loaded from the last checkpoint (`LoadCheckpointMode.LAST_ITR`) or from the best checkpoint (`LoadCheckpointMode.BEST_ITR`). The loading mode is really useful when you want to stop the training and restart it later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/50 [00:00<?, ?itr/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad9408ddd4ff4a5fb3a7572c0318e2ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m training_history \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m\t\u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m\t\u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m\t\u001B[49m\u001B[43mn_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m\t\u001B[49m\u001B[43mload_checkpoint_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mLoadCheckpointMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLAST_ITR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;66;43;03m# force_overwrite=True,\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\trainers\\trainer.py:419\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, train_dataloader, val_dataloader, n_iterations, n_epochs, load_checkpoint_mode, force_overwrite, p_bar_position, p_bar_leave, **kwargs)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state_(itr_metrics\u001B[38;5;241m=\u001B[39m{})\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mon_iteration_begin(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 419\u001B[0m itr_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_exec_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexec_metrics_on_train\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    421\u001B[0m \titr_train_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exec_metrics(train_dataloader, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\trainers\\trainer.py:478\u001B[0m, in \u001B[0;36mTrainer._exec_iteration\u001B[1;34m(self, train_dataloader, val_dataloader)\u001B[0m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_training_state\u001B[38;5;241m.\u001B[39mn_epochs):\n\u001B[0;32m    477\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state_(epoch\u001B[38;5;241m=\u001B[39mepoch_idx)\n\u001B[1;32m--> 478\u001B[0m \ttrain_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_exec_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    479\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(train_losses)\n\u001B[0;32m    480\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state_(train_loss\u001B[38;5;241m=\u001B[39mtrain_loss)\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\trainers\\trainer.py:521\u001B[0m, in \u001B[0;36mTrainer._exec_epoch\u001B[1;34m(self, dataloader)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (x_batch, y_batch) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m    520\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state_(batch\u001B[38;5;241m=\u001B[39mi)\n\u001B[1;32m--> 521\u001B[0m \tbatch_losses\u001B[38;5;241m.\u001B[39mappend(to_numpy(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_exec_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m    522\u001B[0m mean_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(batch_losses)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mon_epoch_end(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\trainers\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer._exec_batch\u001B[1;34m(self, x_batch, y_batch)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state_(pred_batch\u001B[38;5;241m=\u001B[39mpred_batch)\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m--> 538\u001B[0m \t\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_optimization_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpred_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mon_optimization_end(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\callbacks\\base_callback.py:649\u001B[0m, in \u001B[0;36mCallbacksList.on_optimization_begin\u001B[1;34m(self, trainer, **kwargs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;124;03mCalled when the optimization phase of an iteration starts. The optimization phase is defined as\u001B[39;00m\n\u001B[0;32m    640\u001B[0m \u001B[38;5;124;03mthe moment where the model weights are updated.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;124;03m:return: None\u001B[39;00m\n\u001B[0;32m    647\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m--> 649\u001B[0m \tcallback\u001B[38;5;241m.\u001B[39mon_optimization_begin(trainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\learning_algorithms\\bptt.py:119\u001B[0m, in \u001B[0;36mBPTT.on_optimization_begin\u001B[1;34m(self, trainer, **kwargs)\u001B[0m\n\u001B[0;32m    117\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mcurrent_training_state\u001B[38;5;241m.\u001B[39my_batch\n\u001B[0;32m    118\u001B[0m pred_batch \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mformat_pred_batch(trainer\u001B[38;5;241m.\u001B[39mcurrent_training_state\u001B[38;5;241m.\u001B[39mpred_batch, y_batch)\n\u001B[1;32m--> 119\u001B[0m batch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_optim_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m trainer\u001B[38;5;241m.\u001B[39mupdate_state_(batch_loss\u001B[38;5;241m=\u001B[39mbatch_loss)\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\learning_algorithms\\bptt.py:111\u001B[0m, in \u001B[0;36mBPTT._make_optim_step\u001B[1;34m(self, pred_batch, y_batch, retain_graph)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_make_optim_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, pred_batch, y_batch, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    110\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 111\u001B[0m \tbatch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_criterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m \tbatch_loss\u001B[38;5;241m.\u001B[39mbackward(retain_graph\u001B[38;5;241m=\u001B[39mretain_graph)\n\u001B[0;32m    113\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\learning_algorithms\\bptt.py:99\u001B[0m, in \u001B[0;36mBPTT.apply_criterion\u001B[1;34m(self, pred_batch, y_batch)\u001B[0m\n\u001B[0;32m     96\u001B[0m \t\tpred_batch \u001B[38;5;241m=\u001B[39m {k: pred_batch \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion}\n\u001B[0;32m     97\u001B[0m \t\u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pred_batch, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_batch, \u001B[38;5;28mdict\u001B[39m), \\\n\u001B[0;32m     98\u001B[0m \t\t\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf criterion is a dict, pred, y_batch and pred must be a dict too.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 99\u001B[0m \tbatch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([\n\u001B[0;32m    100\u001B[0m \t\t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion[k](pred_batch[k], y_batch[k]\u001B[38;5;241m.\u001B[39mto(pred_batch[k]\u001B[38;5;241m.\u001B[39mdevice))\n\u001B[0;32m    101\u001B[0m \t\t\u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion\n\u001B[0;32m    102\u001B[0m \t])\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    104\u001B[0m \t\u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pred_batch, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pred_batch) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\src\\neurotorch\\learning_algorithms\\bptt.py:100\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     96\u001B[0m \t\tpred_batch \u001B[38;5;241m=\u001B[39m {k: pred_batch \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion}\n\u001B[0;32m     97\u001B[0m \t\u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pred_batch, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_batch, \u001B[38;5;28mdict\u001B[39m), \\\n\u001B[0;32m     98\u001B[0m \t\t\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf criterion is a dict, pred, y_batch and pred must be a dict too.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     99\u001B[0m \tbatch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([\n\u001B[1;32m--> 100\u001B[0m \t\t\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m \t\t\u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion\n\u001B[0;32m    102\u001B[0m \t])\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    104\u001B[0m \t\u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pred_batch, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pred_batch) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\tutorials\\heidelberg\\heidelberg_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\tutorials\\heidelberg\\heidelberg_venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:211\u001B[0m, in \u001B[0;36mNLLLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Github\\NeuroTorch\\tutorials\\heidelberg\\heidelberg_venv\\lib\\site-packages\\torch\\nn\\functional.py:2689\u001B[0m, in \u001B[0;36mnll_loss\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[0;32m   2687\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2688\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 2689\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "training_history = trainer.train(\n",
    "\tdataloaders[\"train\"],\n",
    "\tdataloaders[\"val\"],\n",
    "\tn_iterations=n_iterations,\n",
    "\tload_checkpoint_mode=LoadCheckpointMode.LAST_ITR,\n",
    "\t# force_overwrite=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training is finished! Let's see how our network learned over the iterations.\n",
    "\n",
    "Note: the next graphs are really useful to see if the model overfit or underfit the data over the iterations. It's why you can use the callback `TrainingHistoryVisualizationCallback` to see those graphs in real time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_history.plot(show=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can load the model at the best or the last iteration for the testing phase."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = network.load_checkpoint(checkpoint_manager.checkpoints_meta_path, LoadCheckpointMode.BEST_ITR, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how our network perform on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = {\n",
    "\tk: ClassificationMetrics.compute_y_true_y_pred(network, dataloader, verbose=True, desc=f\"{k} predictions\")\n",
    "\tfor k, dataloader in dataloaders.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "\tk: ClassificationMetrics.accuracy(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "pprint.pprint(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the results on other popular classification metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precisions = {\n",
    "\tk: ClassificationMetrics.precision(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "recalls = {\n",
    "\tk: ClassificationMetrics.recall(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}\n",
    "f1s = {\n",
    "\tk: ClassificationMetrics.f1(network, y_true=y_true, y_pred=y_pred)\n",
    "\tfor k, (y_true, y_pred) in predictions.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = OrderedDict(dict(\n",
    "\tnetwork=network,\n",
    "\taccuracies=accuracies,\n",
    "\tprecisions=precisions,\n",
    "\trecalls=recalls,\n",
    "\tf1s=f1s,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pprint.pprint(results, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the end of the tutorial!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "NeuroTorch is a library that allows you to easily build neural networks and train them. Moreover, it allows you to do audio signal classification using dynamics from neurosciences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
